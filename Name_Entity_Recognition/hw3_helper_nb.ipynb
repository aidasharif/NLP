{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_info.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epib-mwZdQ0S"
      },
      "source": [
        "This is a tutorial for understanding some data structures in assignment 3 and clarifying some concepts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMC7m8lEdSE-",
        "outputId": "66f6fd12-955d-4378-fdb1-d91c561d3e40"
      },
      "source": [
        "!pip install --upgrade spacy==2.1.0 allennlp==0.9.0\n",
        "# AllenNLP is a library built on top of PyTorch designed to help build deep NLP models with ease.\n",
        "!pip install seqeval\n",
        "# Seqeval supports POS tagging notations, and thus can be used for evaluation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/0f/ca790def675011f25bce8775cf9002b5085cd2288f85e891f70b32c18752/spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 1.3MB/s \n",
            "\u001b[?25hCollecting allennlp==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 33.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 48.5MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.6.4)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 49.9MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/a8/ba/03b4c978708510c2ab52a75804530edfd96647f3de44abe1cf25d16150ad/responses-0.13.2-py2.py3-none-any.whl\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2.10.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/d0/8375e4dfa2c6c9cf27d0f76a858ec4519df786658daf2c11bd0d6bf1a419/boto3-1.17.40-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.4.1)\n",
            "Collecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 56.1MB/s \n",
            "\u001b[?25hCollecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 60.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.5)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 60.4MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.5.3)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.1.2)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.7MB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.8.0+cu101)\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9.0) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (8.7.0)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/57/8a68360d697cf9159cba5ee35f2d25bdcda33883e8b5a997714a191a0b11/zope.interface-5.3.0-cp37-cp37m-manylinux2010_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 57.6MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/25/f52f0dde4135833c2f85eae30a749d260231065b46942534df8366d7e1ec/greenlet-1.0.0-cp37-cp37m-manylinux2010_x86_64.whl (160kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 43.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.40\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/d7/f92f8eb20a4d54d2d2b34a40449346503a15733b9d63b7c03dd01f35c473/botocore-1.20.40-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 50.8MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==0.9.0) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp==0.9.0) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->allennlp==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp==0.9.0) (1.8.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9.0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9.0) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.7.12)\n",
            "Requirement already satisfied, skipping upgrade: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.9.0)\n",
            "Requirement already satisfied, skipping upgrade: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.16)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.1.4)\n",
            "Building wheels for collected packages: word2number, overrides, parsimonious, ftfy, jsonnet\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=7eb740d5d790a34cae0c1ff4981b84d0b321fa338b56efa44010341bed0e9ac0\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=8438f9257f7058d27342775ea0ffbbeff8f32f7e536fad9495bdc053f9f4f7c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp37-none-any.whl size=42711 sha256=9ca1b845c6f07724f6c927ddbac2eb7d42775de0ffc9bdc5c991dd24d23ac5ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=175b471e1e31dcf358b9c29561d99ddea489ef2682cd1292b508e7e713c240a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388781 sha256=b8aac5b262085a45d8c634c5f7d7949b36a739e8df5ccc87de46098c72514a2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "Successfully built word2number overrides parsimonious ftfy jsonnet\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.40 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: responses 0.13.2 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: plac, blis, preshed, thinc, spacy, jmespath, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers, flask-cors, zope.interface, zope.event, greenlet, gevent, responses, word2number, overrides, flaky, parsimonious, tensorboardX, conllu, unidecode, pytorch-pretrained-bert, jsonpickle, ftfy, jsonnet, numpydoc, allennlp\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 boto3-1.17.40 botocore-1.20.40 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.10 ftfy-5.9 gevent-21.1.2 greenlet-1.0.0 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.13.2 s3transfer-0.3.6 sentencepiece-0.1.95 spacy-2.1.0 tensorboardX-2.1 thinc-7.0.8 unidecode-1.2.0 word2number-1.1 zope.event-4.5.0 zope.interface-5.3.0\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=3e3805336ee08bdc1bd1082ad8bc0973b6d6cdd166f91a2f00c51aa57941f4c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD-TvDb6dajw",
        "outputId": "e4900f47-148c-4ffa-f73c-7fc8508b4642"
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "import seqeval\n",
        "import copy\n",
        "\n",
        "from typing import Iterator, List, Dict\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.dataset_readers import conll2003\n",
        "\n",
        "from seqeval.metrics import recall_score\n",
        "from seqeval.metrics import precision_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9a3b026b30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvrcjvogPWk"
      },
      "source": [
        "LSTM Tagger is a basic model that takes embedded representation of data and encodes them based on sequence to sequence encoding model. \n",
        "\n",
        "In the last assignment you represented a word as its count in the document. It is possible to map a word onto a continuous domain, generating the word embeddings to either capture its context or the interactions with other words in higher dimensions. For instance, consider a sentence 'The dog is running after a car'. Here, *running* can be represented by the words around it, namely *is* and *after*. Thus we have an understanding of *running* in context of the occurence in sentence. There are many possible ways to create embeddings, and this is a separate field of research in itself. \n",
        "\n",
        "Finally, the forward function implements the forward computation for model to tag scores with cross entropy loss, since our labels are categorical. 'tag_logits' are scores in log domain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUal3_YDdfa9"
      },
      "source": [
        "class LstmTagger(Model):\n",
        "  def __init__(self,\n",
        "               word_embeddings: TextFieldEmbedder,\n",
        "               encoder: Seq2SeqEncoder,\n",
        "               vocab: Vocabulary) -> None:\n",
        "    super().__init__(vocab)\n",
        "    self.word_embeddings = word_embeddings\n",
        "    self.encoder = encoder\n",
        "    self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(self,\n",
        "              tokens: Dict[str, torch.Tensor],\n",
        "              metadata,\n",
        "              tags: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "    mask = get_text_field_mask(tokens)\n",
        "    embeddings = self.word_embeddings(tokens)\n",
        "    encoder_out = self.encoder(embeddings, mask)\n",
        "    tag_logits = self.hidden2tag(encoder_out)\n",
        "    output = {\"tag_logits\": tag_logits}\n",
        "    if tags is not None:\n",
        "      self.accuracy(tag_logits, tags, mask)\n",
        "      output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, tags, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {\"accuracy\": self.accuracy.get_metric(reset)}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AykU-XSArn8t"
      },
      "source": [
        "CoNLL 2003 is a standardized NER and POS tagging dataset, which is avaialable through AllenNLP. DatasetReader used with AllenNLP helps convert the dataset file to instances, which are classes with multiple fields. For example, a single instance of train or validation data here contains three fields in a single instance: sentence tokens, metadata and associated tags. These classes are utilized to create a standard pipeline for the flow of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAZ34L9gdiF3",
        "outputId": "2fe637bc-db55-4ef3-ff00-f038c3898885"
      },
      "source": [
        "reader = conll2003.Conll2003DatasetReader()\n",
        "train_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.train.ner.sample'))\n",
        "validation_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.development.ner.sample'))\n",
        "print('\\nFirst sample instance from training dataset:\\n', train_dataset[0])\n",
        "\n",
        "from itertools import chain\n",
        "vocab = Vocabulary.from_instances(chain(train_dataset, validation_dataset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159377B [00:00, 1806026.94B/s]\n",
            "562it [00:00, 18005.70it/s]\n",
            "8366B [00:00, 4611584.61B/s]\n",
            "23it [00:00, 8101.87it/s]\n",
            "585it [00:00, 31610.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "First sample instance from training dataset:\n",
            " Instance with fields:\n",
            " \t tokens: TextField of length 5 with text: \n",
            " \t\t[What, kind, of, memory, ?]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            " \t metadata: MetadataField (print field.metadata to see specific information). \n",
            " \t tags: SequenceLabelField of length 5 with labels:\n",
            " \t\t['O', 'O', 'O', 'O', 'O']\n",
            " \t\tin namespace: 'labels'. \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGkBG4BEmBe5",
        "outputId": "b8904127-f9b9-4e77-a963-765ba4cdbf7a"
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=False, batch_first=True))\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=100,\n",
        "                  cuda_device=cuda_device)\n",
        "trainer.train()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8442, loss: 0.9096 ||: 100%|██████████| 281/281 [00:01<00:00, 226.88it/s]\n",
            "accuracy: 0.7878, loss: 1.1957 ||: 100%|██████████| 12/12 [00:00<00:00, 440.61it/s]\n",
            "accuracy: 0.8442, loss: 0.7290 ||: 100%|██████████| 281/281 [00:01<00:00, 278.56it/s]\n",
            "accuracy: 0.7878, loss: 1.1993 ||: 100%|██████████| 12/12 [00:00<00:00, 452.86it/s]\n",
            "accuracy: 0.8442, loss: 0.7162 ||: 100%|██████████| 281/281 [00:01<00:00, 275.62it/s]\n",
            "accuracy: 0.7878, loss: 1.1955 ||: 100%|██████████| 12/12 [00:00<00:00, 467.62it/s]\n",
            "accuracy: 0.8442, loss: 0.7065 ||: 100%|██████████| 281/281 [00:01<00:00, 278.34it/s]\n",
            "accuracy: 0.7878, loss: 1.1663 ||: 100%|██████████| 12/12 [00:00<00:00, 466.28it/s]\n",
            "accuracy: 0.8442, loss: 0.6982 ||: 100%|██████████| 281/281 [00:01<00:00, 270.51it/s]\n",
            "accuracy: 0.7878, loss: 1.1688 ||: 100%|██████████| 12/12 [00:00<00:00, 473.18it/s]\n",
            "accuracy: 0.8442, loss: 0.6904 ||: 100%|██████████| 281/281 [00:01<00:00, 279.65it/s]\n",
            "accuracy: 0.7878, loss: 1.1509 ||: 100%|██████████| 12/12 [00:00<00:00, 472.37it/s]\n",
            "accuracy: 0.8442, loss: 0.6809 ||: 100%|██████████| 281/281 [00:01<00:00, 277.41it/s]\n",
            "accuracy: 0.7878, loss: 1.1519 ||: 100%|██████████| 12/12 [00:00<00:00, 485.43it/s]\n",
            "accuracy: 0.8442, loss: 0.6698 ||: 100%|██████████| 281/281 [00:01<00:00, 280.46it/s]\n",
            "accuracy: 0.7878, loss: 1.1253 ||: 100%|██████████| 12/12 [00:00<00:00, 458.83it/s]\n",
            "accuracy: 0.8442, loss: 0.6555 ||: 100%|██████████| 281/281 [00:01<00:00, 280.34it/s]\n",
            "accuracy: 0.7878, loss: 1.1081 ||: 100%|██████████| 12/12 [00:00<00:00, 481.44it/s]\n",
            "accuracy: 0.8442, loss: 0.6391 ||: 100%|██████████| 281/281 [00:01<00:00, 275.88it/s]\n",
            "accuracy: 0.7878, loss: 1.0817 ||: 100%|██████████| 12/12 [00:00<00:00, 471.80it/s]\n",
            "accuracy: 0.8442, loss: 0.6130 ||: 100%|██████████| 281/281 [00:01<00:00, 280.72it/s]\n",
            "accuracy: 0.7878, loss: 1.0442 ||: 100%|██████████| 12/12 [00:00<00:00, 481.51it/s]\n",
            "accuracy: 0.8442, loss: 0.5806 ||: 100%|██████████| 281/281 [00:01<00:00, 279.91it/s]\n",
            "accuracy: 0.7878, loss: 0.9766 ||: 100%|██████████| 12/12 [00:00<00:00, 486.24it/s]\n",
            "accuracy: 0.8454, loss: 0.5401 ||: 100%|██████████| 281/281 [00:01<00:00, 280.41it/s]\n",
            "accuracy: 0.7878, loss: 0.9135 ||: 100%|██████████| 12/12 [00:00<00:00, 446.73it/s]\n",
            "accuracy: 0.8562, loss: 0.5032 ||: 100%|██████████| 281/281 [00:01<00:00, 278.35it/s]\n",
            "accuracy: 0.7878, loss: 0.8665 ||: 100%|██████████| 12/12 [00:00<00:00, 425.36it/s]\n",
            "accuracy: 0.8590, loss: 0.4737 ||: 100%|██████████| 281/281 [00:01<00:00, 280.79it/s]\n",
            "accuracy: 0.7878, loss: 0.8225 ||: 100%|██████████| 12/12 [00:00<00:00, 424.79it/s]\n",
            "accuracy: 0.8603, loss: 0.4489 ||: 100%|██████████| 281/281 [00:01<00:00, 277.38it/s]\n",
            "accuracy: 0.7918, loss: 0.8152 ||: 100%|██████████| 12/12 [00:00<00:00, 473.52it/s]\n",
            "accuracy: 0.8615, loss: 0.4321 ||: 100%|██████████| 281/281 [00:01<00:00, 276.99it/s]\n",
            "accuracy: 0.7939, loss: 0.7808 ||: 100%|██████████| 12/12 [00:00<00:00, 488.17it/s]\n",
            "accuracy: 0.8624, loss: 0.4183 ||: 100%|██████████| 281/281 [00:01<00:00, 280.67it/s]\n",
            "accuracy: 0.7939, loss: 0.7610 ||: 100%|██████████| 12/12 [00:00<00:00, 445.41it/s]\n",
            "accuracy: 0.8629, loss: 0.4047 ||: 100%|██████████| 281/281 [00:01<00:00, 280.63it/s]\n",
            "accuracy: 0.7939, loss: 0.7626 ||: 100%|██████████| 12/12 [00:00<00:00, 459.70it/s]\n",
            "accuracy: 0.8629, loss: 0.3968 ||: 100%|██████████| 281/281 [00:01<00:00, 279.75it/s]\n",
            "accuracy: 0.7939, loss: 0.7571 ||: 100%|██████████| 12/12 [00:00<00:00, 498.27it/s]\n",
            "accuracy: 0.8635, loss: 0.3917 ||: 100%|██████████| 281/281 [00:01<00:00, 279.13it/s]\n",
            "accuracy: 0.7939, loss: 0.7338 ||: 100%|██████████| 12/12 [00:00<00:00, 422.52it/s]\n",
            "accuracy: 0.8638, loss: 0.3862 ||: 100%|██████████| 281/281 [00:00<00:00, 281.44it/s]\n",
            "accuracy: 0.7939, loss: 0.7192 ||: 100%|██████████| 12/12 [00:00<00:00, 459.62it/s]\n",
            "accuracy: 0.8647, loss: 0.3811 ||: 100%|██████████| 281/281 [00:01<00:00, 274.04it/s]\n",
            "accuracy: 0.7939, loss: 0.7412 ||: 100%|██████████| 12/12 [00:00<00:00, 454.68it/s]\n",
            "accuracy: 0.8652, loss: 0.3750 ||: 100%|██████████| 281/281 [00:01<00:00, 278.31it/s]\n",
            "accuracy: 0.7959, loss: 0.7228 ||: 100%|██████████| 12/12 [00:00<00:00, 468.14it/s]\n",
            "accuracy: 0.8664, loss: 0.3728 ||: 100%|██████████| 281/281 [00:01<00:00, 279.59it/s]\n",
            "accuracy: 0.7939, loss: 0.7156 ||: 100%|██████████| 12/12 [00:00<00:00, 476.59it/s]\n",
            "accuracy: 0.8665, loss: 0.3694 ||: 100%|██████████| 281/281 [00:01<00:00, 275.00it/s]\n",
            "accuracy: 0.8000, loss: 0.6997 ||: 100%|██████████| 12/12 [00:00<00:00, 457.63it/s]\n",
            "accuracy: 0.8680, loss: 0.3642 ||: 100%|██████████| 281/281 [00:01<00:00, 278.62it/s]\n",
            "accuracy: 0.8000, loss: 0.6844 ||: 100%|██████████| 12/12 [00:00<00:00, 414.96it/s]\n",
            "accuracy: 0.8704, loss: 0.3623 ||: 100%|██████████| 281/281 [00:01<00:00, 267.19it/s]\n",
            "accuracy: 0.7959, loss: 0.7030 ||: 100%|██████████| 12/12 [00:00<00:00, 506.57it/s]\n",
            "accuracy: 0.8702, loss: 0.3591 ||: 100%|██████████| 281/281 [00:01<00:00, 277.01it/s]\n",
            "accuracy: 0.8000, loss: 0.7037 ||: 100%|██████████| 12/12 [00:00<00:00, 400.69it/s]\n",
            "accuracy: 0.8724, loss: 0.3544 ||: 100%|██████████| 281/281 [00:01<00:00, 279.76it/s]\n",
            "accuracy: 0.8041, loss: 0.6771 ||: 100%|██████████| 12/12 [00:00<00:00, 503.40it/s]\n",
            "accuracy: 0.8722, loss: 0.3515 ||: 100%|██████████| 281/281 [00:01<00:00, 278.25it/s]\n",
            "accuracy: 0.8020, loss: 0.6781 ||: 100%|██████████| 12/12 [00:00<00:00, 446.32it/s]\n",
            "accuracy: 0.8730, loss: 0.3480 ||: 100%|██████████| 281/281 [00:01<00:00, 274.89it/s]\n",
            "accuracy: 0.8020, loss: 0.6703 ||: 100%|██████████| 12/12 [00:00<00:00, 525.44it/s]\n",
            "accuracy: 0.8718, loss: 0.3469 ||: 100%|██████████| 281/281 [00:01<00:00, 278.20it/s]\n",
            "accuracy: 0.8061, loss: 0.6612 ||: 100%|██████████| 12/12 [00:00<00:00, 445.19it/s]\n",
            "accuracy: 0.8770, loss: 0.3399 ||: 100%|██████████| 281/281 [00:01<00:00, 278.37it/s]\n",
            "accuracy: 0.8020, loss: 0.6871 ||: 100%|██████████| 12/12 [00:00<00:00, 429.19it/s]\n",
            "accuracy: 0.8746, loss: 0.3399 ||: 100%|██████████| 281/281 [00:01<00:00, 268.83it/s]\n",
            "accuracy: 0.8102, loss: 0.6542 ||: 100%|██████████| 12/12 [00:00<00:00, 489.20it/s]\n",
            "accuracy: 0.8768, loss: 0.3401 ||: 100%|██████████| 281/281 [00:01<00:00, 276.35it/s]\n",
            "accuracy: 0.8163, loss: 0.6623 ||: 100%|██████████| 12/12 [00:00<00:00, 476.91it/s]\n",
            "accuracy: 0.8771, loss: 0.3338 ||: 100%|██████████| 281/281 [00:01<00:00, 279.63it/s]\n",
            "accuracy: 0.8163, loss: 0.6392 ||: 100%|██████████| 12/12 [00:00<00:00, 517.54it/s]\n",
            "accuracy: 0.8773, loss: 0.3338 ||: 100%|██████████| 281/281 [00:01<00:00, 274.15it/s]\n",
            "accuracy: 0.8184, loss: 0.6631 ||: 100%|██████████| 12/12 [00:00<00:00, 441.43it/s]\n",
            "accuracy: 0.8777, loss: 0.3313 ||: 100%|██████████| 281/281 [00:01<00:00, 277.62it/s]\n",
            "accuracy: 0.8265, loss: 0.6367 ||: 100%|██████████| 12/12 [00:00<00:00, 472.23it/s]\n",
            "accuracy: 0.8807, loss: 0.3263 ||: 100%|██████████| 281/281 [00:01<00:00, 273.46it/s]\n",
            "accuracy: 0.8184, loss: 0.6831 ||: 100%|██████████| 12/12 [00:00<00:00, 469.34it/s]\n",
            "accuracy: 0.8814, loss: 0.3271 ||: 100%|██████████| 281/281 [00:01<00:00, 274.47it/s]\n",
            "accuracy: 0.8204, loss: 0.6196 ||: 100%|██████████| 12/12 [00:00<00:00, 454.75it/s]\n",
            "accuracy: 0.8804, loss: 0.3247 ||: 100%|██████████| 281/281 [00:01<00:00, 274.05it/s]\n",
            "accuracy: 0.8265, loss: 0.6587 ||: 100%|██████████| 12/12 [00:00<00:00, 415.45it/s]\n",
            "accuracy: 0.8829, loss: 0.3210 ||: 100%|██████████| 281/281 [00:01<00:00, 274.58it/s]\n",
            "accuracy: 0.8265, loss: 0.7094 ||: 100%|██████████| 12/12 [00:00<00:00, 517.46it/s]\n",
            "accuracy: 0.8823, loss: 0.3195 ||: 100%|██████████| 281/281 [00:01<00:00, 280.89it/s]\n",
            "accuracy: 0.8347, loss: 0.6275 ||: 100%|██████████| 12/12 [00:00<00:00, 416.48it/s]\n",
            "accuracy: 0.8860, loss: 0.3186 ||: 100%|██████████| 281/281 [00:01<00:00, 275.64it/s]\n",
            "accuracy: 0.8347, loss: 0.6176 ||: 100%|██████████| 12/12 [00:00<00:00, 456.80it/s]\n",
            "accuracy: 0.8874, loss: 0.3133 ||: 100%|██████████| 281/281 [00:01<00:00, 280.46it/s]\n",
            "accuracy: 0.8306, loss: 0.6574 ||: 100%|██████████| 12/12 [00:00<00:00, 458.30it/s]\n",
            "accuracy: 0.8849, loss: 0.3130 ||: 100%|██████████| 281/281 [00:01<00:00, 276.67it/s]\n",
            "accuracy: 0.8327, loss: 0.6155 ||: 100%|██████████| 12/12 [00:00<00:00, 510.34it/s]\n",
            "accuracy: 0.8859, loss: 0.3105 ||: 100%|██████████| 281/281 [00:01<00:00, 277.97it/s]\n",
            "accuracy: 0.8347, loss: 0.6508 ||: 100%|██████████| 12/12 [00:00<00:00, 470.50it/s]\n",
            "accuracy: 0.8874, loss: 0.3081 ||: 100%|██████████| 281/281 [00:01<00:00, 277.85it/s]\n",
            "accuracy: 0.8388, loss: 0.6000 ||: 100%|██████████| 12/12 [00:00<00:00, 541.29it/s]\n",
            "accuracy: 0.8910, loss: 0.3065 ||: 100%|██████████| 281/281 [00:01<00:00, 278.81it/s]\n",
            "accuracy: 0.8429, loss: 0.6015 ||: 100%|██████████| 12/12 [00:00<00:00, 507.22it/s]\n",
            "accuracy: 0.8889, loss: 0.3058 ||: 100%|██████████| 281/281 [00:01<00:00, 279.88it/s]\n",
            "accuracy: 0.8327, loss: 0.6083 ||: 100%|██████████| 12/12 [00:00<00:00, 450.74it/s]\n",
            "accuracy: 0.8911, loss: 0.3005 ||: 100%|██████████| 281/281 [00:01<00:00, 277.08it/s]\n",
            "accuracy: 0.8408, loss: 0.5912 ||: 100%|██████████| 12/12 [00:00<00:00, 480.24it/s]\n",
            "accuracy: 0.8912, loss: 0.2994 ||: 100%|██████████| 281/281 [00:01<00:00, 280.68it/s]\n",
            "accuracy: 0.8429, loss: 0.5803 ||: 100%|██████████| 12/12 [00:00<00:00, 507.40it/s]\n",
            "accuracy: 0.8917, loss: 0.2971 ||: 100%|██████████| 281/281 [00:01<00:00, 270.74it/s]\n",
            "accuracy: 0.8449, loss: 0.5864 ||: 100%|██████████| 12/12 [00:00<00:00, 500.66it/s]\n",
            "accuracy: 0.8919, loss: 0.2956 ||: 100%|██████████| 281/281 [00:01<00:00, 280.65it/s]\n",
            "accuracy: 0.8388, loss: 0.5819 ||: 100%|██████████| 12/12 [00:00<00:00, 452.53it/s]\n",
            "accuracy: 0.8905, loss: 0.2918 ||: 100%|██████████| 281/281 [00:01<00:00, 278.26it/s]\n",
            "accuracy: 0.8449, loss: 0.6037 ||: 100%|██████████| 12/12 [00:00<00:00, 470.60it/s]\n",
            "accuracy: 0.8935, loss: 0.2925 ||: 100%|██████████| 281/281 [00:01<00:00, 271.37it/s]\n",
            "accuracy: 0.8367, loss: 0.6141 ||: 100%|██████████| 12/12 [00:00<00:00, 456.97it/s]\n",
            "accuracy: 0.8912, loss: 0.2885 ||: 100%|██████████| 281/281 [00:01<00:00, 278.90it/s]\n",
            "accuracy: 0.8449, loss: 0.5687 ||: 100%|██████████| 12/12 [00:00<00:00, 520.14it/s]\n",
            "accuracy: 0.8934, loss: 0.2856 ||: 100%|██████████| 281/281 [00:01<00:00, 272.61it/s]\n",
            "accuracy: 0.8429, loss: 0.5704 ||: 100%|██████████| 12/12 [00:00<00:00, 522.20it/s]\n",
            "accuracy: 0.8942, loss: 0.2839 ||: 100%|██████████| 281/281 [00:01<00:00, 273.93it/s]\n",
            "accuracy: 0.8490, loss: 0.5556 ||: 100%|██████████| 12/12 [00:00<00:00, 487.45it/s]\n",
            "accuracy: 0.8951, loss: 0.2825 ||: 100%|██████████| 281/281 [00:01<00:00, 271.38it/s]\n",
            "accuracy: 0.8449, loss: 0.5665 ||: 100%|██████████| 12/12 [00:00<00:00, 451.70it/s]\n",
            "accuracy: 0.8949, loss: 0.2792 ||: 100%|██████████| 281/281 [00:01<00:00, 278.39it/s]\n",
            "accuracy: 0.8429, loss: 0.5579 ||: 100%|██████████| 12/12 [00:00<00:00, 424.14it/s]\n",
            "accuracy: 0.8963, loss: 0.2773 ||: 100%|██████████| 281/281 [00:01<00:00, 280.16it/s]\n",
            "accuracy: 0.8551, loss: 0.5621 ||: 100%|██████████| 12/12 [00:00<00:00, 427.53it/s]\n",
            "accuracy: 0.8959, loss: 0.2775 ||: 100%|██████████| 281/281 [00:01<00:00, 274.37it/s]\n",
            "accuracy: 0.8490, loss: 0.5543 ||: 100%|██████████| 12/12 [00:00<00:00, 513.65it/s]\n",
            "accuracy: 0.8985, loss: 0.2737 ||: 100%|██████████| 281/281 [00:01<00:00, 275.94it/s]\n",
            "accuracy: 0.8510, loss: 0.5627 ||: 100%|██████████| 12/12 [00:00<00:00, 439.12it/s]\n",
            "accuracy: 0.8978, loss: 0.2722 ||: 100%|██████████| 281/281 [00:01<00:00, 276.38it/s]\n",
            "accuracy: 0.8490, loss: 0.5505 ||: 100%|██████████| 12/12 [00:00<00:00, 503.86it/s]\n",
            "accuracy: 0.9001, loss: 0.2711 ||: 100%|██████████| 281/281 [00:00<00:00, 281.77it/s]\n",
            "accuracy: 0.8551, loss: 0.5381 ||: 100%|██████████| 12/12 [00:00<00:00, 487.75it/s]\n",
            "accuracy: 0.8992, loss: 0.2677 ||: 100%|██████████| 281/281 [00:00<00:00, 281.39it/s]\n",
            "accuracy: 0.8551, loss: 0.5285 ||: 100%|██████████| 12/12 [00:00<00:00, 452.41it/s]\n",
            "accuracy: 0.9008, loss: 0.2662 ||: 100%|██████████| 281/281 [00:01<00:00, 272.78it/s]\n",
            "accuracy: 0.8571, loss: 0.5346 ||: 100%|██████████| 12/12 [00:00<00:00, 465.96it/s]\n",
            "accuracy: 0.9038, loss: 0.2634 ||: 100%|██████████| 281/281 [00:01<00:00, 275.81it/s]\n",
            "accuracy: 0.8510, loss: 0.5500 ||: 100%|██████████| 12/12 [00:00<00:00, 451.77it/s]\n",
            "accuracy: 0.9012, loss: 0.2635 ||: 100%|██████████| 281/281 [00:01<00:00, 271.53it/s]\n",
            "accuracy: 0.8612, loss: 0.5255 ||: 100%|██████████| 12/12 [00:00<00:00, 444.85it/s]\n",
            "accuracy: 0.9036, loss: 0.2609 ||: 100%|██████████| 281/281 [00:01<00:00, 267.76it/s]\n",
            "accuracy: 0.8571, loss: 0.5376 ||: 100%|██████████| 12/12 [00:00<00:00, 517.82it/s]\n",
            "accuracy: 0.9066, loss: 0.2595 ||: 100%|██████████| 281/281 [00:01<00:00, 272.80it/s]\n",
            "accuracy: 0.8571, loss: 0.5174 ||: 100%|██████████| 12/12 [00:00<00:00, 525.05it/s]\n",
            "accuracy: 0.9066, loss: 0.2572 ||: 100%|██████████| 281/281 [00:01<00:00, 273.15it/s]\n",
            "accuracy: 0.8612, loss: 0.5215 ||: 100%|██████████| 12/12 [00:00<00:00, 508.94it/s]\n",
            "accuracy: 0.9051, loss: 0.2559 ||: 100%|██████████| 281/281 [00:01<00:00, 276.49it/s]\n",
            "accuracy: 0.8592, loss: 0.5170 ||: 100%|██████████| 12/12 [00:00<00:00, 507.72it/s]\n",
            "accuracy: 0.9089, loss: 0.2509 ||: 100%|██████████| 281/281 [00:01<00:00, 276.29it/s]\n",
            "accuracy: 0.8531, loss: 0.5340 ||: 100%|██████████| 12/12 [00:00<00:00, 516.38it/s]\n",
            "accuracy: 0.9082, loss: 0.2529 ||: 100%|██████████| 281/281 [00:01<00:00, 270.82it/s]\n",
            "accuracy: 0.8633, loss: 0.5106 ||: 100%|██████████| 12/12 [00:00<00:00, 514.52it/s]\n",
            "accuracy: 0.9108, loss: 0.2506 ||: 100%|██████████| 281/281 [00:01<00:00, 272.94it/s]\n",
            "accuracy: 0.8653, loss: 0.5080 ||: 100%|██████████| 12/12 [00:00<00:00, 458.73it/s]\n",
            "accuracy: 0.9110, loss: 0.2489 ||: 100%|██████████| 281/281 [00:01<00:00, 273.38it/s]\n",
            "accuracy: 0.8612, loss: 0.5040 ||: 100%|██████████| 12/12 [00:00<00:00, 518.50it/s]\n",
            "accuracy: 0.9104, loss: 0.2473 ||: 100%|██████████| 281/281 [00:01<00:00, 272.26it/s]\n",
            "accuracy: 0.8612, loss: 0.5126 ||: 100%|██████████| 12/12 [00:00<00:00, 512.99it/s]\n",
            "accuracy: 0.9140, loss: 0.2462 ||: 100%|██████████| 281/281 [00:01<00:00, 270.66it/s]\n",
            "accuracy: 0.8653, loss: 0.4963 ||: 100%|██████████| 12/12 [00:00<00:00, 476.12it/s]\n",
            "accuracy: 0.9129, loss: 0.2437 ||: 100%|██████████| 281/281 [00:01<00:00, 274.20it/s]\n",
            "accuracy: 0.8633, loss: 0.5014 ||: 100%|██████████| 12/12 [00:00<00:00, 504.81it/s]\n",
            "accuracy: 0.9143, loss: 0.2429 ||: 100%|██████████| 281/281 [00:01<00:00, 273.65it/s]\n",
            "accuracy: 0.8653, loss: 0.4982 ||: 100%|██████████| 12/12 [00:00<00:00, 472.00it/s]\n",
            "accuracy: 0.9157, loss: 0.2398 ||: 100%|██████████| 281/281 [00:01<00:00, 276.16it/s]\n",
            "accuracy: 0.8653, loss: 0.4891 ||: 100%|██████████| 12/12 [00:00<00:00, 445.78it/s]\n",
            "accuracy: 0.9155, loss: 0.2376 ||: 100%|██████████| 281/281 [00:01<00:00, 279.55it/s]\n",
            "accuracy: 0.8633, loss: 0.4985 ||: 100%|██████████| 12/12 [00:00<00:00, 510.17it/s]\n",
            "accuracy: 0.9149, loss: 0.2381 ||: 100%|██████████| 281/281 [00:01<00:00, 276.71it/s]\n",
            "accuracy: 0.8612, loss: 0.4942 ||: 100%|██████████| 12/12 [00:00<00:00, 469.16it/s]\n",
            "accuracy: 0.9180, loss: 0.2370 ||: 100%|██████████| 281/281 [00:01<00:00, 268.82it/s]\n",
            "accuracy: 0.8633, loss: 0.4891 ||: 100%|██████████| 12/12 [00:00<00:00, 524.97it/s]\n",
            "accuracy: 0.9177, loss: 0.2335 ||: 100%|██████████| 281/281 [00:01<00:00, 276.85it/s]\n",
            "accuracy: 0.8694, loss: 0.4835 ||: 100%|██████████| 12/12 [00:00<00:00, 483.08it/s]\n",
            "accuracy: 0.9174, loss: 0.2320 ||: 100%|██████████| 281/281 [00:01<00:00, 264.86it/s]\n",
            "accuracy: 0.8653, loss: 0.4792 ||: 100%|██████████| 12/12 [00:00<00:00, 510.83it/s]\n",
            "accuracy: 0.9184, loss: 0.2300 ||: 100%|██████████| 281/281 [00:01<00:00, 272.35it/s]\n",
            "accuracy: 0.8694, loss: 0.4697 ||: 100%|██████████| 12/12 [00:00<00:00, 496.02it/s]\n",
            "accuracy: 0.9192, loss: 0.2300 ||: 100%|██████████| 281/281 [00:01<00:00, 271.90it/s]\n",
            "accuracy: 0.8694, loss: 0.4781 ||: 100%|██████████| 12/12 [00:00<00:00, 407.82it/s]\n",
            "accuracy: 0.9191, loss: 0.2276 ||: 100%|██████████| 281/281 [00:01<00:00, 275.49it/s]\n",
            "accuracy: 0.8735, loss: 0.4647 ||: 100%|██████████| 12/12 [00:00<00:00, 499.13it/s]\n",
            "accuracy: 0.9205, loss: 0.2253 ||: 100%|██████████| 281/281 [00:01<00:00, 273.25it/s]\n",
            "accuracy: 0.8673, loss: 0.4579 ||: 100%|██████████| 12/12 [00:00<00:00, 535.32it/s]\n",
            "accuracy: 0.9208, loss: 0.2246 ||: 100%|██████████| 281/281 [00:01<00:00, 273.03it/s]\n",
            "accuracy: 0.8633, loss: 0.4635 ||: 100%|██████████| 12/12 [00:00<00:00, 482.32it/s]\n",
            "accuracy: 0.9205, loss: 0.2229 ||: 100%|██████████| 281/281 [00:01<00:00, 273.88it/s]\n",
            "accuracy: 0.8837, loss: 0.4519 ||: 100%|██████████| 12/12 [00:00<00:00, 460.89it/s]\n",
            "accuracy: 0.9212, loss: 0.2214 ||: 100%|██████████| 281/281 [00:01<00:00, 270.05it/s]\n",
            "accuracy: 0.8816, loss: 0.4565 ||: 100%|██████████| 12/12 [00:00<00:00, 444.88it/s]\n",
            "accuracy: 0.9225, loss: 0.2196 ||: 100%|██████████| 281/281 [00:01<00:00, 274.51it/s]\n",
            "accuracy: 0.8735, loss: 0.4428 ||: 100%|██████████| 12/12 [00:00<00:00, 519.68it/s]\n",
            "accuracy: 0.9215, loss: 0.2205 ||: 100%|██████████| 281/281 [00:01<00:00, 271.69it/s]\n",
            "accuracy: 0.8735, loss: 0.4451 ||: 100%|██████████| 12/12 [00:00<00:00, 454.06it/s]\n",
            "accuracy: 0.9230, loss: 0.2158 ||: 100%|██████████| 281/281 [00:01<00:00, 276.60it/s]\n",
            "accuracy: 0.8755, loss: 0.4507 ||: 100%|██████████| 12/12 [00:00<00:00, 476.25it/s]\n",
            "accuracy: 0.9243, loss: 0.2147 ||: 100%|██████████| 281/281 [00:01<00:00, 273.88it/s]\n",
            "accuracy: 0.8755, loss: 0.4415 ||: 100%|██████████| 12/12 [00:00<00:00, 454.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 99,\n",
              " 'best_validation_accuracy': 0.8755102040816326,\n",
              " 'best_validation_loss': 0.44149550279447186,\n",
              " 'epoch': 99,\n",
              " 'peak_cpu_memory_MB': 2575.904,\n",
              " 'peak_gpu_0_memory_MB': 903,\n",
              " 'training_accuracy': 0.924266497193094,\n",
              " 'training_cpu_memory_MB': 2575.904,\n",
              " 'training_duration': '0:01:50.754801',\n",
              " 'training_epochs': 99,\n",
              " 'training_gpu_0_memory_MB': 903,\n",
              " 'training_loss': 0.21472710906341505,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.8755102040816326,\n",
              " 'validation_loss': 0.44149550279447186}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpl9KVIExn_Z"
      },
      "source": [
        "```tag_sentence``` gives you the token, its correct tag and the predicted tag from model for each word in the sentence s. `model.forward_on_instance(s)['tag_logits']` is a 2D matrix of shape n_words x n_tags, and the argmax is taken to get the tag with maximum value for each word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLZjSfHQxjDh",
        "outputId": "e94ce881-ffd8-4c25-91ec-7d0ebeb0e13d"
      },
      "source": [
        "def tag_sentence(s):\n",
        "  tag_ids = np.argmax(model.forward_on_instance(s)['tag_logits'], axis=-1)\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)\n",
        "\n",
        "baseline_output = [tag_sentence(i) for i in validation_dataset]\n",
        "\n",
        "## Show the first example of an instance passed to tag_sentence\n",
        "print(validation_dataset[0])\n",
        "\n",
        "## Show the first example of output\n",
        "print('\\n Corresponding output:\\n')\n",
        "baseline_output[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Instance with fields:\n",
            " \t tokens: TextField of length 27 with text: \n",
            " \t\t[With, a, wave, of, his, hand, ,, Peng, Dehuai, said, that, despite, being, over, 100, regiments, ,,\n",
            "\t\tlet, 's, call, this, campaign, the, Hundred, Regiments, Offensive, .]\n",
            " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
            " \t metadata: MetadataField (print field.metadata to see specific information). \n",
            " \t tags: SequenceLabelField of length 27 with labels:\n",
            " \t\t['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL',\n",
            "\t\t'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O']\n",
            " \t\tin namespace: 'labels'. \n",
            "\n",
            "\n",
            " Corresponding output:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(With, 'O', 'O'),\n",
              " (a, 'O', 'O'),\n",
              " (wave, 'O', 'O'),\n",
              " (of, 'O', 'O'),\n",
              " (his, 'O', 'O'),\n",
              " (hand, 'O', 'O'),\n",
              " (,, 'O', 'O'),\n",
              " (Peng, 'B-PERSON', 'B-PERSON'),\n",
              " (Dehuai, 'I-PERSON', 'I-EVENT'),\n",
              " (said, 'O', 'O'),\n",
              " (that, 'O', 'O'),\n",
              " (despite, 'O', 'O'),\n",
              " (being, 'O', 'O'),\n",
              " (over, 'O', 'O'),\n",
              " (100, 'B-CARDINAL', 'B-CARDINAL'),\n",
              " (regiments, 'O', 'O'),\n",
              " (,, 'O', 'O'),\n",
              " (let, 'O', 'O'),\n",
              " ('s, 'O', 'O'),\n",
              " (call, 'O', 'O'),\n",
              " (this, 'O', 'O'),\n",
              " (campaign, 'O', 'O'),\n",
              " (the, 'B-EVENT', 'O'),\n",
              " (Hundred, 'I-EVENT', 'I-EVENT'),\n",
              " (Regiments, 'I-EVENT', 'I-EVENT'),\n",
              " (Offensive, 'I-EVENT', 'I-EVENT'),\n",
              " (., 'O', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-eetGbF2XWZ"
      },
      "source": [
        "You will compare violations between two words in a sentence.\n",
        "Sample violations: \n",
        "\n",
        "1.   B-PER I-LOC\n",
        "2.   I-PER I-LOC\n",
        "3.   O I-PER\n",
        "4.   A sentence beginning with I-TYPE\n",
        "\n",
        "Evaluations score would be based on valid spans. All the words tagged outside of span don't contribute to scores.\n",
        "Examples of valid spans for corresponding words: \n",
        "\n",
        "1.   Peng Dehuai: [B-PERSON, I-PERSON]\n",
        "2.   100 : [B-CARDINAL]\n",
        "\n",
        "Precision = Number of spans identified correctly by model/Number of spans predicted by model\n",
        "\n",
        "Recall = Number of spans identified correctly by model/Number of labelled spans\n",
        "\n",
        "If you use seqeval, valid spans will be automatically extracted by the library for a given list of predictions and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNqhDhtg2JT1"
      },
      "source": [
        "# TODO: count the number of NER label violations,\n",
        "# such as O followed by I-TYPE or B-TYPE followed by\n",
        "# I-OTHER_TYPE\n",
        "# Take tagger output as input\n",
        "\n",
        "# Valid NERs:\n",
        "# B-TYPE: All valid occurance\n",
        "# O-TYPE: All valid occurance\n",
        "# I-TYPE:\n",
        "#       a) If first I-TYPE, previous type should be same B-TYPE\n",
        "#       b) If subsequent I-TYPE, previous type should be same I-TYPE\n",
        "def violations(tagged):\n",
        "    # Write code here\n",
        "    return violations\n",
        "\n",
        "# TODO: return the span-level precision, recall, and F1\n",
        "# Only count valid spans that start with a B tag,\n",
        "# followed by zero or more I tags of the same type.\n",
        "# This is harsher than the token-level metric that the\n",
        "# LSTM was trained to optimize, but it is the standard way\n",
        "# of evaluating NER systems.\n",
        "# Take tagger output as input\n",
        "def span_stats(tagged):\n",
        "    # Write code here\n",
        "    return {'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQelwpF7l_K"
      },
      "source": [
        "**Viterbi Decoding**\n",
        "\n",
        "You can either use a graph or dynamic programming to implement the viterbi decoder. The pseudocode given is for dynamic programming, but you are free to implement a graph procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqv7PiG77oMz"
      },
      "source": [
        "# Pseudocode as a guide for implementation\n",
        "\n",
        "# For each sentence in validation dataset:\n",
        "\n",
        "  # Create a k x k transition matrix, where k is the number of tags [O, B-PER, etc.]\n",
        "  # transition_matrix = np.zeros((n_tags, n_tags))\n",
        "  # Fill the matrix based on possible transitions from a tag to another tag\n",
        "  # for tag_index_1 in range(len(tags)):\n",
        "  #   for tag_index_2 in range(len(tags)):\n",
        "  #     if is_a_violation(tags[tag_index_1], tags[tag_index_2]): \n",
        "  #       transition_matrix[tag_index_1][tag_index_2] = np.log(0)\n",
        "  #     else:\n",
        "  #       transition_matrix[tag_index_1][tag_index_2] = np.log(1) \n",
        "\n",
        "  # Get emission scores from the model for an instance s [instance as a class has been discussed above]\n",
        "  # emission_scores = model.forward_on_instance(s)['tag_logits']\n",
        "  # Construct a 2D DP table and a 2D backtracking table and fill them with zeros initially\n",
        "  # for word_index in range(1, len(s)):\n",
        "  #   for tag_index in range(len(tags)):\n",
        "  #     dp_table[word_index][tag_index] = assign values here by adding emission_scores, transition probabilities and dp_table[word_index - 1][tags] and call max function\n",
        "  #     backtracking_table[word_index][tag_index] = argmax(values generated above before calling max)\n",
        "  # \n",
        "  # From the backtracking_table, trace back the path based on indices to get the list of tags for s."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}