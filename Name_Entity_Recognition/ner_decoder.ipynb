{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NULabTMN/hw3-aidasharif1365/blob/master/ner_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsY0LJOIT_cG"
      },
      "source": [
        "# Implementing a Viterbi Decoder and Evaluation for Sequence Labeling\n",
        "\n",
        "In this assignment, you will build a Viterbi decoder for an LSTM named-entity recognition model. As we mentioned in class, recurrent and bidirectional recurrent neural networks, of which LSTMs are the most common examples, can be used to perform sequence labeling. Although these models encode information from the surrounding words in order to make predictions, there are no \"hard\" constraints on what tags can appear where.\n",
        "\n",
        "There hard constraints are particularly important for tasks that label spans of more than one token. The most common example of a span-labeling task is named-entity recognition (NER). As described in Eisenstein, Jurafksy & Martin, and other texts, the goal of NER is to label spans of one or more words as _mentions_ of an _entity_, such as a person, location, organization, etc.\n",
        "\n",
        "The most common approach to NER is to reduce it to a sequence-labeling task, where each token in the input is labeled either with an `O`, if it is \"outside\" any named-entity span, or with `B-TYPE`, if it is the first token in an entity of type `TYPE`, or with `I-TYPE`, if it is the second or later token in an entity of type `TYPE`. Distinguishing between the first and later tokens of an entity allow us to identify distinct entity spans even when they are adjacent.\n",
        "\n",
        "Common values of `TYPE` include `PER` for person, `LOC` for location, `DATE` for date, and so on. In the dataset we load below, there are 17 distinct types.\n",
        "\n",
        "The span-labeling scheme just described implies that the labels on tokens must obey certain constraints: the tag `I-PER` must follow either `B-PER` or another `I-PER`. It cannot follow `O`, `B-LOC`, or `I-LOC`, i.e., a tag for a different entity type. By themselves, LSTMs or bidirectional LSTMs cannot directly enforce these constraints. This is one reason why conditional random fields (CRFs), which _can_ enforce these constraints, are often layered on top of these recurrent models.\n",
        "\n",
        "In this assignment, you will implement the simplest possible CRF: a CRF so simple that it does not require any training. Rather, it will assign weight 1 to any sequence of tags that obeys the constraints and weight 0 to any sequence of tags that violates them. The inputs to the CRF, which are analogous to the emission probabilities in an HMM, will come from an LSTM.\n",
        "\n",
        "But first, in order to test your decoder, you will also implement some functions to evaluate the output of an NER system according to two metrics:\n",
        "1. You will count the number of _violations_ of the NER label constraints, i.e., how many times `I-TYPE` follows `O` or a tag of a different type or occurs at the beginning of a sentence. This number will be greater than 0 in the raw LSTM output, but should be 0 for your CRF output.\n",
        "1. You will compute the _span-level_ precision, recall, and F1 of NER output. Although the baseline LSTM was trained to achieve high _token-level_ accuracy, this metric can be misleadingly high, since so many tokens are correctly labeled `O`. In other words, what proportion of spans predicted by the model line up exactly with spans in the gold standard, and what proportion of spans in the gold standard were predicted by the model? Define _span_ as a sequence of tags that starts with a `B-TYPE` followed by zero or more `I-TYPE` tags. Sequences solely of `I-TYPE` tags don't count as spans.For more, see the original task definition: https://www.aclweb.org/anthology/W03-0419/.\n",
        "\n",
        "We start with loading some code and data and the describe your tasks in more detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhnn49QEU_Ik"
      },
      "source": [
        "## Set Up Dependencies and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJINX1MwOLBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c05fb2-1cf6-4efe-965c-10be4a6b8b4a"
      },
      "source": [
        "!pip install --upgrade spacy==2.1.0 allennlp==0.9.0\n",
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/0f/ca790def675011f25bce8775cf9002b5085cd2288f85e891f70b32c18752/spacy-2.1.0-cp37-cp37m-manylinux1_x86_64.whl (27.7MB)\n",
            "\u001b[K     |████████████████████████████████| 27.7MB 117kB/s \n",
            "\u001b[?25hCollecting allennlp==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (0.8.2)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/42/d7ea7539af3852fd8c1f0b3adf4a100fb3d72b40b69cef1a764ff979a743/thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 51.7MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/2b/3ecd5d90d2d6fd39fbc520de7d80db5d74defdc2d7c2e15531d9cc3498c7/preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.6.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.0) (2.23.0)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/5f/47b7b29ad202b2210020e2f33bfb06d1db2abe0e709c2a84736e8a9d1bd5/blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.7MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/db/84/901e700de86604b1c4ef4b57110d4e947c218b9997adf5d38fa7da493bce/Flask_Cors-3.0.10-py2.py3-none-any.whl\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/98/b79af68b6800f8f46893a6f7d8ec733db53583019f2e3a5fa765dc5157a2/boto3-1.17.37.tar.gz (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hCollecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/85/df3d1fd2b60a87455475f93012861b76a411d27ba4a0859939adbe2c9dc3/gevent-21.1.2-cp37-cp37m-manylinux2010_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 42.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.5.3)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.8.0+cu101)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 56.0MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.2)\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/43/0e/2f50064e327f41a1eb811df089f813036e19a64b95e33f8e9e0b96c2447e/flaky-3.7.0-py2.py3-none-any.whl\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (4.41.1)\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.6.4)\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/40/6f16e5ac994b16fa71c24310f97174ce07d3a97b433275589265c6b94d2b/jsonnet-0.17.0.tar.gz (259kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2.10.0)\n",
            "Collecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/b1/a1/162c90162e0f4539534b6ce6d723c4c07be8ad38c1cb975d7c63128502e0/responses-0.13.1-py2.py3-none-any.whl\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 51.6MB/s \n",
            "\u001b[?25hCollecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/1a/f2db026d4d682303793559f1c2bb425ba3ec0d6fd7ac63397790443f2461/jsonpickle-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.9.0) (1.4.1)\n",
            "Collecting numpydoc>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.3MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp==0.9.0) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors>=3.0.7->allennlp==0.9.0) (1.15.0)\n",
            "Collecting botocore<1.21.0,>=1.20.37\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/a7/d363c5e245e79e789094a289acb4935bc953f9f2ce6fd8cda34cfc41cbb9/botocore-1.20.37-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 42.5MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.0MB/s \n",
            "\u001b[?25hCollecting zope.event\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/85/b45408c64f3b888976f1d5b37eed8d746b8d5729a66a49ec846fda27d371/zope.event-4.5.0-py2.py3-none-any.whl\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/57/8a68360d697cf9159cba5ee35f2d25bdcda33883e8b5a997714a191a0b11/zope.interface-5.3.0-cp37-cp37m-manylinux2010_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 56.5MB/s \n",
            "\u001b[?25hCollecting greenlet<2.0,>=0.4.17; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/25/f52f0dde4135833c2f85eae30a749d260231065b46942534df8366d7e1ec/greenlet-1.0.0-cp37-cp37m-manylinux2010_x86_64.whl (160kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 59.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.9.0) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->allennlp==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.9.0) (2.4.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.1.0->allennlp==0.9.0) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (8.7.0)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.9.0) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==0.9.0) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle->allennlp==0.9.0) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.9.0) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: sphinx>=1.6.5 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=0.8.0->allennlp==0.9.0) (1.8.5)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle->allennlp==0.9.0) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp==0.9.0) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.16)\n",
            "Requirement already satisfied, skipping upgrade: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (0.7.12)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (2.9.0)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.2.4)\n",
            "Requirement already satisfied, skipping upgrade: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.8.0->allennlp==0.9.0) (1.1.4)\n",
            "Building wheels for collected packages: ftfy, boto3, word2number, jsonnet, parsimonious, overrides\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=8bca6bcff5d84a719acfff898516b58d7d5b94d897e840ee81bc33a884737472\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.37-py2.py3-none-any.whl size=128780 sha256=85f9cf28f236dae7303be5f7dd73ff8957bda955dc78ac6d59d49acf4a5084e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/66/72/f1bb0906ee42cd7ab67802dcb37fc3163ff34413a526553b7b\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=88f39ea55f026d48f352ce66b2b8ecf7f9ecd1c797ad4e9455af8fcb506371fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388777 sha256=349f6b0cbe2a67ff7e8a321e76e582130e551fa753627bd9f33d9ad3b3aa9edc\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/7a/37/7dbcc30a6b4efd17b91ad1f0128b7bbf84813bd4e1cfb8c1e3\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp37-none-any.whl size=42711 sha256=2a00e8b4fe60d4a3d0dffa358bdef95ee73a7619c0e930fb38d04dbc4ce719b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=ff128c567163ddde80f916d9a405fd6b120b77cb5b1b38ebdba6ab5ebbc23315\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
            "Successfully built ftfy boto3 word2number jsonnet parsimonious overrides\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.37 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: responses 0.13.1 has requirement urllib3>=1.25.10, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: preshed, blis, plac, thinc, spacy, ftfy, flask-cors, jmespath, botocore, s3transfer, boto3, zope.event, zope.interface, greenlet, gevent, unidecode, word2number, flaky, conllu, sentencepiece, pytorch-transformers, jsonnet, responses, tensorboardX, parsimonious, jsonpickle, numpydoc, overrides, pytorch-pretrained-bert, allennlp\n",
            "  Found existing installation: preshed 3.0.5\n",
            "    Uninstalling preshed-3.0.5:\n",
            "      Successfully uninstalled preshed-3.0.5\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 boto3-1.17.37 botocore-1.20.37 conllu-1.3.1 flaky-3.7.0 flask-cors-3.0.10 ftfy-5.9 gevent-21.1.2 greenlet-1.0.0 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 numpydoc-1.1.0 overrides-3.1.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.13.1 s3transfer-0.3.6 sentencepiece-0.1.95 spacy-2.1.0 tensorboardX-2.1 thinc-7.0.8 unidecode-1.2.0 word2number-1.1 zope.event-4.5.0 zope.interface-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4zJfaIlJ2bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e2c37ff-24ac-4c24-9ddc-878f69398e21"
      },
      "source": [
        "from typing import Iterator, List, Dict\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from allennlp.data import Instance\n",
        "from allennlp.data.fields import TextField, SequenceLabelField\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
        "from allennlp.data.tokenizers import Token\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
        "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
        "from allennlp.training.metrics import CategoricalAccuracy\n",
        "from allennlp.data.iterators import BucketIterator\n",
        "from allennlp.training.trainer import Trainer\n",
        "from allennlp.predictors import SentenceTaggerPredictor\n",
        "from allennlp.data.dataset_readers import conll2003\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f023954f790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo16Ko0Gchxk"
      },
      "source": [
        "class LstmTagger(Model):\n",
        "  def __init__(self,\n",
        "               word_embeddings: TextFieldEmbedder,\n",
        "               encoder: Seq2SeqEncoder,\n",
        "               vocab: Vocabulary) -> None:\n",
        "    super().__init__(vocab)\n",
        "    self.word_embeddings = word_embeddings\n",
        "    self.encoder = encoder\n",
        "    self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "    self.accuracy = CategoricalAccuracy()\n",
        "\n",
        "  def forward(self,\n",
        "              tokens: Dict[str, torch.Tensor],\n",
        "              metadata,\n",
        "              tags: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "    mask = get_text_field_mask(tokens)\n",
        "    embeddings = self.word_embeddings(tokens)\n",
        "    encoder_out = self.encoder(embeddings, mask)\n",
        "    tag_logits = self.hidden2tag(encoder_out)\n",
        "    output = {\"tag_logits\": tag_logits}\n",
        "    if tags is not None:\n",
        "      self.accuracy(tag_logits, tags, mask)\n",
        "      output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, tags, mask)\n",
        "\n",
        "    return output\n",
        "\n",
        "  def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "    return {\"accuracy\": self.accuracy.get_metric(reset)}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdKvPftVVLt"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sOVVZslKm3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50d395a-8cf7-4833-bf4b-95f75c9c098f"
      },
      "source": [
        "reader = conll2003.Conll2003DatasetReader()\n",
        "train_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.train.ner.sample'))\n",
        "validation_dataset = reader.read(cached_path('http://www.ccs.neu.edu/home/dasmith/onto.development.ner.sample'))\n",
        "\n",
        "from itertools import chain\n",
        "vocab = Vocabulary.from_instances(chain(train_dataset, validation_dataset))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159377B [00:00, 48251450.02B/s]\n",
            "562it [00:00, 19876.71it/s]\n",
            "8366B [00:00, 21344006.85B/s]\n",
            "23it [00:00, 4184.12it/s]\n",
            "585it [00:00, 62146.49it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpg2Udr-Vnwm"
      },
      "source": [
        "## Define and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kDQQBMywdKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f56af74-0c21-419f-cc4d-aecf7346513e"
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, bidirectional=False, batch_first=True))\n",
        "model = LstmTagger(word_embeddings, lstm, vocab)\n",
        "if torch.cuda.is_available():\n",
        "    cuda_device = 0\n",
        "    model = model.cuda(cuda_device)\n",
        "else:\n",
        "    cuda_device = -1\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
        "iterator.index_with(vocab)\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  iterator=iterator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset,\n",
        "                  patience=10,\n",
        "                  num_epochs=100,\n",
        "                  cuda_device=cuda_device)\n",
        "trainer.train()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8442, loss: 0.9047 ||: 100%|██████████| 281/281 [00:01<00:00, 196.23it/s]\n",
            "accuracy: 0.7878, loss: 1.2029 ||: 100%|██████████| 12/12 [00:00<00:00, 380.83it/s]\n",
            "accuracy: 0.8442, loss: 0.7296 ||: 100%|██████████| 281/281 [00:01<00:00, 260.74it/s]\n",
            "accuracy: 0.7878, loss: 1.1897 ||: 100%|██████████| 12/12 [00:00<00:00, 394.88it/s]\n",
            "accuracy: 0.8442, loss: 0.7158 ||: 100%|██████████| 281/281 [00:01<00:00, 239.58it/s]\n",
            "accuracy: 0.7878, loss: 1.1900 ||: 100%|██████████| 12/12 [00:00<00:00, 351.43it/s]\n",
            "accuracy: 0.8442, loss: 0.7066 ||: 100%|██████████| 281/281 [00:01<00:00, 244.84it/s]\n",
            "accuracy: 0.7878, loss: 1.1669 ||: 100%|██████████| 12/12 [00:00<00:00, 454.91it/s]\n",
            "accuracy: 0.8442, loss: 0.6988 ||: 100%|██████████| 281/281 [00:01<00:00, 252.79it/s]\n",
            "accuracy: 0.7878, loss: 1.1730 ||: 100%|██████████| 12/12 [00:00<00:00, 368.55it/s]\n",
            "accuracy: 0.8442, loss: 0.6910 ||: 100%|██████████| 281/281 [00:01<00:00, 261.73it/s]\n",
            "accuracy: 0.7878, loss: 1.1639 ||: 100%|██████████| 12/12 [00:00<00:00, 469.23it/s]\n",
            "accuracy: 0.8442, loss: 0.6818 ||: 100%|██████████| 281/281 [00:01<00:00, 244.15it/s]\n",
            "accuracy: 0.7878, loss: 1.1545 ||: 100%|██████████| 12/12 [00:00<00:00, 481.11it/s]\n",
            "accuracy: 0.8442, loss: 0.6703 ||: 100%|██████████| 281/281 [00:01<00:00, 245.84it/s]\n",
            "accuracy: 0.7878, loss: 1.1269 ||: 100%|██████████| 12/12 [00:00<00:00, 463.42it/s]\n",
            "accuracy: 0.8442, loss: 0.6566 ||: 100%|██████████| 281/281 [00:01<00:00, 245.63it/s]\n",
            "accuracy: 0.7878, loss: 1.1081 ||: 100%|██████████| 12/12 [00:00<00:00, 417.41it/s]\n",
            "accuracy: 0.8442, loss: 0.6394 ||: 100%|██████████| 281/281 [00:01<00:00, 252.04it/s]\n",
            "accuracy: 0.7878, loss: 1.0953 ||: 100%|██████████| 12/12 [00:00<00:00, 383.43it/s]\n",
            "accuracy: 0.8442, loss: 0.6143 ||: 100%|██████████| 281/281 [00:01<00:00, 261.25it/s]\n",
            "accuracy: 0.7878, loss: 1.0471 ||: 100%|██████████| 12/12 [00:00<00:00, 455.47it/s]\n",
            "accuracy: 0.8442, loss: 0.5790 ||: 100%|██████████| 281/281 [00:01<00:00, 253.95it/s]\n",
            "accuracy: 0.7878, loss: 0.9899 ||: 100%|██████████| 12/12 [00:00<00:00, 369.97it/s]\n",
            "accuracy: 0.8463, loss: 0.5390 ||: 100%|██████████| 281/281 [00:01<00:00, 263.84it/s]\n",
            "accuracy: 0.7878, loss: 0.9517 ||: 100%|██████████| 12/12 [00:00<00:00, 422.66it/s]\n",
            "accuracy: 0.8565, loss: 0.5030 ||: 100%|██████████| 281/281 [00:01<00:00, 245.17it/s]\n",
            "accuracy: 0.7878, loss: 0.8692 ||: 100%|██████████| 12/12 [00:00<00:00, 436.02it/s]\n",
            "accuracy: 0.8594, loss: 0.4723 ||: 100%|██████████| 281/281 [00:01<00:00, 254.34it/s]\n",
            "accuracy: 0.7878, loss: 0.8448 ||: 100%|██████████| 12/12 [00:00<00:00, 476.08it/s]\n",
            "accuracy: 0.8602, loss: 0.4459 ||: 100%|██████████| 281/281 [00:01<00:00, 254.40it/s]\n",
            "accuracy: 0.7939, loss: 0.7910 ||: 100%|██████████| 12/12 [00:00<00:00, 365.65it/s]\n",
            "accuracy: 0.8615, loss: 0.4312 ||: 100%|██████████| 281/281 [00:01<00:00, 240.13it/s]\n",
            "accuracy: 0.7939, loss: 0.7810 ||: 100%|██████████| 12/12 [00:00<00:00, 405.69it/s]\n",
            "accuracy: 0.8616, loss: 0.4182 ||: 100%|██████████| 281/281 [00:01<00:00, 259.36it/s]\n",
            "accuracy: 0.7939, loss: 0.7784 ||: 100%|██████████| 12/12 [00:00<00:00, 489.09it/s]\n",
            "accuracy: 0.8627, loss: 0.4057 ||: 100%|██████████| 281/281 [00:01<00:00, 266.92it/s]\n",
            "accuracy: 0.7939, loss: 0.7479 ||: 100%|██████████| 12/12 [00:00<00:00, 433.33it/s]\n",
            "accuracy: 0.8632, loss: 0.3995 ||: 100%|██████████| 281/281 [00:01<00:00, 246.80it/s]\n",
            "accuracy: 0.7939, loss: 0.7684 ||: 100%|██████████| 12/12 [00:00<00:00, 503.82it/s]\n",
            "accuracy: 0.8632, loss: 0.3923 ||: 100%|██████████| 281/281 [00:01<00:00, 214.86it/s]\n",
            "accuracy: 0.7939, loss: 0.7359 ||: 100%|██████████| 12/12 [00:00<00:00, 381.13it/s]\n",
            "accuracy: 0.8646, loss: 0.3861 ||: 100%|██████████| 281/281 [00:01<00:00, 246.88it/s]\n",
            "accuracy: 0.7939, loss: 0.7210 ||: 100%|██████████| 12/12 [00:00<00:00, 371.01it/s]\n",
            "accuracy: 0.8644, loss: 0.3801 ||: 100%|██████████| 281/281 [00:01<00:00, 248.19it/s]\n",
            "accuracy: 0.7939, loss: 0.7121 ||: 100%|██████████| 12/12 [00:00<00:00, 449.80it/s]\n",
            "accuracy: 0.8636, loss: 0.3796 ||: 100%|██████████| 281/281 [00:01<00:00, 256.28it/s]\n",
            "accuracy: 0.7939, loss: 0.7084 ||: 100%|██████████| 12/12 [00:00<00:00, 426.61it/s]\n",
            "accuracy: 0.8653, loss: 0.3746 ||: 100%|██████████| 281/281 [00:01<00:00, 214.86it/s]\n",
            "accuracy: 0.7959, loss: 0.7103 ||: 100%|██████████| 12/12 [00:00<00:00, 456.38it/s]\n",
            "accuracy: 0.8657, loss: 0.3696 ||: 100%|██████████| 281/281 [00:01<00:00, 245.46it/s]\n",
            "accuracy: 0.7959, loss: 0.7013 ||: 100%|██████████| 12/12 [00:00<00:00, 426.50it/s]\n",
            "accuracy: 0.8680, loss: 0.3656 ||: 100%|██████████| 281/281 [00:01<00:00, 253.59it/s]\n",
            "accuracy: 0.7959, loss: 0.6834 ||: 100%|██████████| 12/12 [00:00<00:00, 496.22it/s]\n",
            "accuracy: 0.8687, loss: 0.3607 ||: 100%|██████████| 281/281 [00:01<00:00, 251.82it/s]\n",
            "accuracy: 0.8020, loss: 0.6959 ||: 100%|██████████| 12/12 [00:00<00:00, 435.00it/s]\n",
            "accuracy: 0.8710, loss: 0.3574 ||: 100%|██████████| 281/281 [00:01<00:00, 250.24it/s]\n",
            "accuracy: 0.8020, loss: 0.6819 ||: 100%|██████████| 12/12 [00:00<00:00, 445.09it/s]\n",
            "accuracy: 0.8720, loss: 0.3557 ||: 100%|██████████| 281/281 [00:01<00:00, 244.29it/s]\n",
            "accuracy: 0.8082, loss: 0.6828 ||: 100%|██████████| 12/12 [00:00<00:00, 457.42it/s]\n",
            "accuracy: 0.8730, loss: 0.3514 ||: 100%|██████████| 281/281 [00:01<00:00, 257.23it/s]\n",
            "accuracy: 0.8082, loss: 0.6657 ||: 100%|██████████| 12/12 [00:00<00:00, 504.23it/s]\n",
            "accuracy: 0.8737, loss: 0.3487 ||: 100%|██████████| 281/281 [00:01<00:00, 256.42it/s]\n",
            "accuracy: 0.8102, loss: 0.6545 ||: 100%|██████████| 12/12 [00:00<00:00, 424.58it/s]\n",
            "accuracy: 0.8752, loss: 0.3446 ||: 100%|██████████| 281/281 [00:01<00:00, 252.26it/s]\n",
            "accuracy: 0.8082, loss: 0.6673 ||: 100%|██████████| 12/12 [00:00<00:00, 409.15it/s]\n",
            "accuracy: 0.8768, loss: 0.3419 ||: 100%|██████████| 281/281 [00:01<00:00, 252.61it/s]\n",
            "accuracy: 0.8082, loss: 0.6506 ||: 100%|██████████| 12/12 [00:00<00:00, 395.13it/s]\n",
            "accuracy: 0.8778, loss: 0.3386 ||: 100%|██████████| 281/281 [00:01<00:00, 224.51it/s]\n",
            "accuracy: 0.8184, loss: 0.6450 ||: 100%|██████████| 12/12 [00:00<00:00, 335.45it/s]\n",
            "accuracy: 0.8764, loss: 0.3386 ||: 100%|██████████| 281/281 [00:01<00:00, 241.61it/s]\n",
            "accuracy: 0.8102, loss: 0.6572 ||: 100%|██████████| 12/12 [00:00<00:00, 403.39it/s]\n",
            "accuracy: 0.8777, loss: 0.3344 ||: 100%|██████████| 281/281 [00:01<00:00, 238.07it/s]\n",
            "accuracy: 0.8143, loss: 0.6569 ||: 100%|██████████| 12/12 [00:00<00:00, 419.08it/s]\n",
            "accuracy: 0.8779, loss: 0.3330 ||: 100%|██████████| 281/281 [00:01<00:00, 255.66it/s]\n",
            "accuracy: 0.8224, loss: 0.6382 ||: 100%|██████████| 12/12 [00:00<00:00, 442.49it/s]\n",
            "accuracy: 0.8801, loss: 0.3282 ||: 100%|██████████| 281/281 [00:01<00:00, 243.56it/s]\n",
            "accuracy: 0.8204, loss: 0.6399 ||: 100%|██████████| 12/12 [00:00<00:00, 409.37it/s]\n",
            "accuracy: 0.8814, loss: 0.3257 ||: 100%|██████████| 281/281 [00:01<00:00, 243.81it/s]\n",
            "accuracy: 0.8265, loss: 0.6265 ||: 100%|██████████| 12/12 [00:00<00:00, 378.51it/s]\n",
            "accuracy: 0.8831, loss: 0.3236 ||: 100%|██████████| 281/281 [00:01<00:00, 248.65it/s]\n",
            "accuracy: 0.8245, loss: 0.6564 ||: 100%|██████████| 12/12 [00:00<00:00, 404.08it/s]\n",
            "accuracy: 0.8857, loss: 0.3204 ||: 100%|██████████| 281/281 [00:01<00:00, 235.60it/s]\n",
            "accuracy: 0.8265, loss: 0.6299 ||: 100%|██████████| 12/12 [00:00<00:00, 444.62it/s]\n",
            "accuracy: 0.8840, loss: 0.3204 ||: 100%|██████████| 281/281 [00:01<00:00, 236.42it/s]\n",
            "accuracy: 0.8286, loss: 0.6141 ||: 100%|██████████| 12/12 [00:00<00:00, 448.80it/s]\n",
            "accuracy: 0.8867, loss: 0.3161 ||: 100%|██████████| 281/281 [00:01<00:00, 251.90it/s]\n",
            "accuracy: 0.8327, loss: 0.6141 ||: 100%|██████████| 12/12 [00:00<00:00, 421.09it/s]\n",
            "accuracy: 0.8870, loss: 0.3147 ||: 100%|██████████| 281/281 [00:01<00:00, 220.22it/s]\n",
            "accuracy: 0.8327, loss: 0.6045 ||: 100%|██████████| 12/12 [00:00<00:00, 457.78it/s]\n",
            "accuracy: 0.8871, loss: 0.3109 ||: 100%|██████████| 281/281 [00:01<00:00, 252.78it/s]\n",
            "accuracy: 0.8388, loss: 0.6032 ||: 100%|██████████| 12/12 [00:00<00:00, 455.16it/s]\n",
            "accuracy: 0.8875, loss: 0.3094 ||: 100%|██████████| 281/281 [00:01<00:00, 236.64it/s]\n",
            "accuracy: 0.8388, loss: 0.5938 ||: 100%|██████████| 12/12 [00:00<00:00, 420.85it/s]\n",
            "accuracy: 0.8899, loss: 0.3070 ||: 100%|██████████| 281/281 [00:01<00:00, 244.36it/s]\n",
            "accuracy: 0.8449, loss: 0.5867 ||: 100%|██████████| 12/12 [00:00<00:00, 495.68it/s]\n",
            "accuracy: 0.8913, loss: 0.3035 ||: 100%|██████████| 281/281 [00:01<00:00, 254.20it/s]\n",
            "accuracy: 0.8429, loss: 0.5847 ||: 100%|██████████| 12/12 [00:00<00:00, 313.41it/s]\n",
            "accuracy: 0.8903, loss: 0.3015 ||: 100%|██████████| 281/281 [00:01<00:00, 246.90it/s]\n",
            "accuracy: 0.8449, loss: 0.5896 ||: 100%|██████████| 12/12 [00:00<00:00, 428.86it/s]\n",
            "accuracy: 0.8932, loss: 0.2999 ||: 100%|██████████| 281/281 [00:01<00:00, 253.78it/s]\n",
            "accuracy: 0.8429, loss: 0.5774 ||: 100%|██████████| 12/12 [00:00<00:00, 403.54it/s]\n",
            "accuracy: 0.8910, loss: 0.2963 ||: 100%|██████████| 281/281 [00:01<00:00, 249.12it/s]\n",
            "accuracy: 0.8469, loss: 0.5847 ||: 100%|██████████| 12/12 [00:00<00:00, 467.16it/s]\n",
            "accuracy: 0.8951, loss: 0.2909 ||: 100%|██████████| 281/281 [00:01<00:00, 221.74it/s]\n",
            "accuracy: 0.8429, loss: 0.5656 ||: 100%|██████████| 12/12 [00:00<00:00, 369.09it/s]\n",
            "accuracy: 0.8945, loss: 0.2893 ||: 100%|██████████| 281/281 [00:01<00:00, 247.51it/s]\n",
            "accuracy: 0.8469, loss: 0.5706 ||: 100%|██████████| 12/12 [00:00<00:00, 424.10it/s]\n",
            "accuracy: 0.8976, loss: 0.2865 ||: 100%|██████████| 281/281 [00:01<00:00, 257.14it/s]\n",
            "accuracy: 0.8469, loss: 0.5654 ||: 100%|██████████| 12/12 [00:00<00:00, 433.83it/s]\n",
            "accuracy: 0.8992, loss: 0.2825 ||: 100%|██████████| 281/281 [00:01<00:00, 245.47it/s]\n",
            "accuracy: 0.8490, loss: 0.5582 ||: 100%|██████████| 12/12 [00:00<00:00, 425.79it/s]\n",
            "accuracy: 0.8976, loss: 0.2814 ||: 100%|██████████| 281/281 [00:01<00:00, 246.87it/s]\n",
            "accuracy: 0.8490, loss: 0.5457 ||: 100%|██████████| 12/12 [00:00<00:00, 336.68it/s]\n",
            "accuracy: 0.8993, loss: 0.2770 ||: 100%|██████████| 281/281 [00:01<00:00, 223.92it/s]\n",
            "accuracy: 0.8551, loss: 0.5350 ||: 100%|██████████| 12/12 [00:00<00:00, 346.88it/s]\n",
            "accuracy: 0.8998, loss: 0.2749 ||: 100%|██████████| 281/281 [00:01<00:00, 253.81it/s]\n",
            "accuracy: 0.8531, loss: 0.5489 ||: 100%|██████████| 12/12 [00:00<00:00, 429.25it/s]\n",
            "accuracy: 0.9036, loss: 0.2722 ||: 100%|██████████| 281/281 [00:01<00:00, 255.18it/s]\n",
            "accuracy: 0.8510, loss: 0.5416 ||: 100%|██████████| 12/12 [00:00<00:00, 458.72it/s]\n",
            "accuracy: 0.8999, loss: 0.2692 ||: 100%|██████████| 281/281 [00:01<00:00, 216.20it/s]\n",
            "accuracy: 0.8490, loss: 0.5213 ||: 100%|██████████| 12/12 [00:00<00:00, 394.82it/s]\n",
            "accuracy: 0.9022, loss: 0.2690 ||: 100%|██████████| 281/281 [00:01<00:00, 240.01it/s]\n",
            "accuracy: 0.8510, loss: 0.5198 ||: 100%|██████████| 12/12 [00:00<00:00, 419.94it/s]\n",
            "accuracy: 0.9038, loss: 0.2624 ||: 100%|██████████| 281/281 [00:01<00:00, 254.16it/s]\n",
            "accuracy: 0.8510, loss: 0.5069 ||: 100%|██████████| 12/12 [00:00<00:00, 363.16it/s]\n",
            "accuracy: 0.9010, loss: 0.2630 ||: 100%|██████████| 281/281 [00:01<00:00, 256.21it/s]\n",
            "accuracy: 0.8551, loss: 0.5199 ||: 100%|██████████| 12/12 [00:00<00:00, 385.24it/s]\n",
            "accuracy: 0.9046, loss: 0.2592 ||: 100%|██████████| 281/281 [00:01<00:00, 225.83it/s]\n",
            "accuracy: 0.8531, loss: 0.5131 ||: 100%|██████████| 12/12 [00:00<00:00, 403.08it/s]\n",
            "accuracy: 0.9041, loss: 0.2558 ||: 100%|██████████| 281/281 [00:01<00:00, 245.99it/s]\n",
            "accuracy: 0.8592, loss: 0.5044 ||: 100%|██████████| 12/12 [00:00<00:00, 404.07it/s]\n",
            "accuracy: 0.9060, loss: 0.2531 ||: 100%|██████████| 281/281 [00:01<00:00, 244.28it/s]\n",
            "accuracy: 0.8592, loss: 0.5160 ||: 100%|██████████| 12/12 [00:00<00:00, 369.50it/s]\n",
            "accuracy: 0.9078, loss: 0.2495 ||: 100%|██████████| 281/281 [00:01<00:00, 232.79it/s]\n",
            "accuracy: 0.8571, loss: 0.4850 ||: 100%|██████████| 12/12 [00:00<00:00, 437.38it/s]\n",
            "accuracy: 0.9081, loss: 0.2477 ||: 100%|██████████| 281/281 [00:01<00:00, 243.42it/s]\n",
            "accuracy: 0.8592, loss: 0.4878 ||: 100%|██████████| 12/12 [00:00<00:00, 375.95it/s]\n",
            "accuracy: 0.9105, loss: 0.2446 ||: 100%|██████████| 281/281 [00:01<00:00, 251.97it/s]\n",
            "accuracy: 0.8571, loss: 0.4852 ||: 100%|██████████| 12/12 [00:00<00:00, 412.86it/s]\n",
            "accuracy: 0.9107, loss: 0.2416 ||: 100%|██████████| 281/281 [00:01<00:00, 246.14it/s]\n",
            "accuracy: 0.8592, loss: 0.4738 ||: 100%|██████████| 12/12 [00:00<00:00, 372.01it/s]\n",
            "accuracy: 0.9113, loss: 0.2395 ||: 100%|██████████| 281/281 [00:01<00:00, 245.37it/s]\n",
            "accuracy: 0.8612, loss: 0.5010 ||: 100%|██████████| 12/12 [00:00<00:00, 362.94it/s]\n",
            "accuracy: 0.9121, loss: 0.2386 ||: 100%|██████████| 281/281 [00:01<00:00, 256.02it/s]\n",
            "accuracy: 0.8571, loss: 0.4654 ||: 100%|██████████| 12/12 [00:00<00:00, 497.67it/s]\n",
            "accuracy: 0.9131, loss: 0.2348 ||: 100%|██████████| 281/281 [00:01<00:00, 246.11it/s]\n",
            "accuracy: 0.8633, loss: 0.4652 ||: 100%|██████████| 12/12 [00:00<00:00, 406.85it/s]\n",
            "accuracy: 0.9124, loss: 0.2335 ||: 100%|██████████| 281/281 [00:01<00:00, 251.37it/s]\n",
            "accuracy: 0.8592, loss: 0.4625 ||: 100%|██████████| 12/12 [00:00<00:00, 363.72it/s]\n",
            "accuracy: 0.9130, loss: 0.2314 ||: 100%|██████████| 281/281 [00:01<00:00, 257.45it/s]\n",
            "accuracy: 0.8571, loss: 0.4762 ||: 100%|██████████| 12/12 [00:00<00:00, 436.89it/s]\n",
            "accuracy: 0.9113, loss: 0.2310 ||: 100%|██████████| 281/281 [00:01<00:00, 249.28it/s]\n",
            "accuracy: 0.8673, loss: 0.4446 ||: 100%|██████████| 12/12 [00:00<00:00, 456.86it/s]\n",
            "accuracy: 0.9151, loss: 0.2280 ||: 100%|██████████| 281/281 [00:01<00:00, 249.77it/s]\n",
            "accuracy: 0.8592, loss: 0.4689 ||: 100%|██████████| 12/12 [00:00<00:00, 444.18it/s]\n",
            "accuracy: 0.9144, loss: 0.2264 ||: 100%|██████████| 281/281 [00:01<00:00, 243.83it/s]\n",
            "accuracy: 0.8653, loss: 0.4535 ||: 100%|██████████| 12/12 [00:00<00:00, 443.29it/s]\n",
            "accuracy: 0.9165, loss: 0.2230 ||: 100%|██████████| 281/281 [00:01<00:00, 243.50it/s]\n",
            "accuracy: 0.8653, loss: 0.4393 ||: 100%|██████████| 12/12 [00:00<00:00, 461.67it/s]\n",
            "accuracy: 0.9159, loss: 0.2231 ||: 100%|██████████| 281/281 [00:01<00:00, 259.03it/s]\n",
            "accuracy: 0.8653, loss: 0.4462 ||: 100%|██████████| 12/12 [00:00<00:00, 467.34it/s]\n",
            "accuracy: 0.9179, loss: 0.2193 ||: 100%|██████████| 281/281 [00:01<00:00, 254.40it/s]\n",
            "accuracy: 0.8714, loss: 0.4372 ||: 100%|██████████| 12/12 [00:00<00:00, 457.12it/s]\n",
            "accuracy: 0.9175, loss: 0.2184 ||: 100%|██████████| 281/281 [00:01<00:00, 234.37it/s]\n",
            "accuracy: 0.8673, loss: 0.4320 ||: 100%|██████████| 12/12 [00:00<00:00, 426.77it/s]\n",
            "accuracy: 0.9205, loss: 0.2147 ||: 100%|██████████| 281/281 [00:01<00:00, 241.86it/s]\n",
            "accuracy: 0.8694, loss: 0.4326 ||: 100%|██████████| 12/12 [00:00<00:00, 480.10it/s]\n",
            "accuracy: 0.9191, loss: 0.2140 ||: 100%|██████████| 281/281 [00:01<00:00, 243.67it/s]\n",
            "accuracy: 0.8735, loss: 0.4154 ||: 100%|██████████| 12/12 [00:00<00:00, 473.38it/s]\n",
            "accuracy: 0.9207, loss: 0.2117 ||: 100%|██████████| 281/281 [00:01<00:00, 247.00it/s]\n",
            "accuracy: 0.8694, loss: 0.4177 ||: 100%|██████████| 12/12 [00:00<00:00, 416.58it/s]\n",
            "accuracy: 0.9214, loss: 0.2103 ||: 100%|██████████| 281/281 [00:01<00:00, 249.26it/s]\n",
            "accuracy: 0.8735, loss: 0.4128 ||: 100%|██████████| 12/12 [00:00<00:00, 480.69it/s]\n",
            "accuracy: 0.9224, loss: 0.2087 ||: 100%|██████████| 281/281 [00:01<00:00, 263.86it/s]\n",
            "accuracy: 0.8714, loss: 0.4190 ||: 100%|██████████| 12/12 [00:00<00:00, 516.82it/s]\n",
            "accuracy: 0.9210, loss: 0.2062 ||: 100%|██████████| 281/281 [00:01<00:00, 249.21it/s]\n",
            "accuracy: 0.8714, loss: 0.4153 ||: 100%|██████████| 12/12 [00:00<00:00, 437.42it/s]\n",
            "accuracy: 0.9226, loss: 0.2036 ||: 100%|██████████| 281/281 [00:01<00:00, 250.85it/s]\n",
            "accuracy: 0.8796, loss: 0.4003 ||: 100%|██████████| 12/12 [00:00<00:00, 512.87it/s]\n",
            "accuracy: 0.9229, loss: 0.2036 ||: 100%|██████████| 281/281 [00:01<00:00, 251.43it/s]\n",
            "accuracy: 0.8735, loss: 0.4064 ||: 100%|██████████| 12/12 [00:00<00:00, 354.99it/s]\n",
            "accuracy: 0.9242, loss: 0.2010 ||: 100%|██████████| 281/281 [00:01<00:00, 254.81it/s]\n",
            "accuracy: 0.8816, loss: 0.4006 ||: 100%|██████████| 12/12 [00:00<00:00, 442.92it/s]\n",
            "accuracy: 0.9235, loss: 0.2006 ||: 100%|██████████| 281/281 [00:01<00:00, 242.56it/s]\n",
            "accuracy: 0.8796, loss: 0.3929 ||: 100%|██████████| 12/12 [00:00<00:00, 370.99it/s]\n",
            "accuracy: 0.9254, loss: 0.1968 ||: 100%|██████████| 281/281 [00:01<00:00, 231.21it/s]\n",
            "accuracy: 0.8776, loss: 0.4065 ||: 100%|██████████| 12/12 [00:00<00:00, 479.09it/s]\n",
            "accuracy: 0.9254, loss: 0.1975 ||: 100%|██████████| 281/281 [00:01<00:00, 246.10it/s]\n",
            "accuracy: 0.8796, loss: 0.3887 ||: 100%|██████████| 12/12 [00:00<00:00, 427.08it/s]\n",
            "accuracy: 0.9252, loss: 0.1948 ||: 100%|██████████| 281/281 [00:01<00:00, 222.48it/s]\n",
            "accuracy: 0.8796, loss: 0.3914 ||: 100%|██████████| 12/12 [00:00<00:00, 419.41it/s]\n",
            "accuracy: 0.9274, loss: 0.1923 ||: 100%|██████████| 281/281 [00:01<00:00, 254.93it/s]\n",
            "accuracy: 0.8796, loss: 0.3794 ||: 100%|██████████| 12/12 [00:00<00:00, 491.17it/s]\n",
            "accuracy: 0.9283, loss: 0.1917 ||: 100%|██████████| 281/281 [00:01<00:00, 226.56it/s]\n",
            "accuracy: 0.8776, loss: 0.3833 ||: 100%|██████████| 12/12 [00:00<00:00, 388.51it/s]\n",
            "accuracy: 0.9282, loss: 0.1890 ||: 100%|██████████| 281/281 [00:01<00:00, 230.46it/s]\n",
            "accuracy: 0.8796, loss: 0.3838 ||: 100%|██████████| 12/12 [00:00<00:00, 337.49it/s]\n",
            "accuracy: 0.9284, loss: 0.1901 ||: 100%|██████████| 281/281 [00:01<00:00, 249.28it/s]\n",
            "accuracy: 0.8796, loss: 0.3810 ||: 100%|██████████| 12/12 [00:00<00:00, 482.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_epoch': 96,\n",
              " 'best_validation_accuracy': 0.8795918367346939,\n",
              " 'best_validation_loss': 0.37943273926309,\n",
              " 'epoch': 99,\n",
              " 'peak_cpu_memory_MB': 3224.064,\n",
              " 'peak_gpu_0_memory_MB': 1058,\n",
              " 'training_accuracy': 0.9283974155280161,\n",
              " 'training_cpu_memory_MB': 3224.064,\n",
              " 'training_duration': '0:02:05.158952',\n",
              " 'training_epochs': 99,\n",
              " 'training_gpu_0_memory_MB': 1058,\n",
              " 'training_loss': 0.1901172606272,\n",
              " 'training_start_epoch': 0,\n",
              " 'validation_accuracy': 0.8795918367346939,\n",
              " 'validation_loss': 0.3810475505888462}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwN6ctqVV0tf"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkDs_UdIeuFz"
      },
      "source": [
        "The simple code below loops over the validation set, applying the model to each exmaple and collecting out the input token, gold-standard output, and model output. You can see from these methods how to access ground truth and model outputs for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEUJDKtVx40u",
        "outputId": "857ab525-6b13-4cac-fb61-aaf800223d57"
      },
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 32.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 28.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=2028a5c68bbdcfd7b1b4cf0315d49d86576d8f4ef893e65d986ade8ee469d303\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0bE4fmLik08"
      },
      "source": [
        "def tag_sentence(s):\n",
        "  tag_ids = np.argmax(model.forward_on_instance(s)['tag_logits'], axis=-1)\n",
        "  fields = zip(s['tokens'], s['tags'], [model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])\n",
        "  return list(fields)\n",
        "\n",
        "word_gold_real = [tag_sentence(i) for i in validation_dataset]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpYMx7RVfCyT"
      },
      "source": [
        "Now, you can implement two evaluation functions: `violations` and `span_stats`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF_oc0tttOOu"
      },
      "source": [
        "def valid_output(example):\n",
        "  arr1=[]\n",
        "  for i in example:\n",
        "    arr1.append(i[2])\n",
        "  return arr1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P042A2Ofg3wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db44a1da-49c5-40aa-e7c9-12da7dd1a340"
      },
      "source": [
        "# TODO: count the number of NER label violations,\n",
        "# such as O followed by I-TYPE or B-TYPE followed by\n",
        "# I-OTHER_TYPE\n",
        "# Take tagger output as input\n",
        "import collections\n",
        "def violations(tagged):\n",
        "  count=0\n",
        "  for i in range(len(tagged)):\n",
        "    for j in range(len(tagged[i])):\n",
        "      if j==0:\n",
        "        if tagged[i][j][2]!='O' and tagged[i][j][2][0]!='B':\n",
        "          count+=1\n",
        "      if j+1<len(tagged[i]) and tagged[i][j][2]=='O' and tagged[i][j+1][2][0]=='I':\n",
        "        count+=1\n",
        "      elif j+1<len(tagged[i]) and tagged[i][j][2][0]=='B' and tagged[i][j+1][2][0]=='I' and tagged[i][j][2][1:]!=tagged[i][j+1][2][1:]:\n",
        "        count+=1\n",
        "      elif j+1<len(tagged[i]) and tagged[i][j][2][0]=='I' and tagged[i][j+1][2][0]=='I' and tagged[i][j][2][1:]!=tagged[i][j+1][2][1:]:\n",
        "        count+=1\n",
        "  return count\n",
        "\n",
        "print('Number of violations in LSTM output is: ',violations(word_gold_real))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of violations in LSTM output is:  36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay7FHKgcp0Hc",
        "outputId": "f4768037-dda6-4e42-bd30-47e16e1ae870"
      },
      "source": [
        "# TODO: return the span-level precision, recall, and F1\n",
        "# Only count valid spans that start with a B tag,\n",
        "# followed by zero or more I tags of the same type.\n",
        "# This is harsher than the token-level metric that the\n",
        "# LSTM was trained to optimize, but it is the standard way\n",
        "# of evaluating NER systems.\n",
        "# Take tagger output as input\n",
        "def span_stats(tagged):\n",
        "\n",
        "  predicted_output=[]\n",
        "  true_output=[]\n",
        "\n",
        "  #making a continuous array from all predicted outputs in validation set\n",
        "  for sample in tagged:\n",
        "    for token in sample:\n",
        "      predicted_output.append(token[2])\n",
        "\n",
        "  #making a continuous array from all true outputs in validation set\n",
        "  for sample in validation_dataset:\n",
        "      for i in sample['tags'].labels:\n",
        "        true_output.append(i)\n",
        "        \n",
        "  print('True output in validation dataset')\n",
        "  print(true_output)\n",
        "  print('')\n",
        "  print('Predicted output by LSTM')\n",
        "  print(predicted_output)\n",
        "\n",
        "  print('\\nLSTM precision: ',round(precision_score([true_output],[predicted_output]),3))\n",
        "  print('LSTM recall: ',round(recall_score([true_output],[predicted_output]),3))\n",
        "  print('LSTM F1: ',round(f1_score([true_output],[predicted_output]),3))\n",
        "\n",
        "\n",
        "\n",
        "span_stats(word_gold_real)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True output in validation dataset\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-TIME', 'I-TIME', 'O', 'B-PERSON', 'I-PERSON', 'B-ORDINAL', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'B-NORP', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Predicted output by LSTM\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-DATE', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-DATE', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-CARDINAL', 'I-EVENT', 'O', 'O', 'O', 'B-GPE', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'I-EVENT', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'B-NORP', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-EVENT', 'O', 'O', 'O', 'O']\n",
            "\n",
            "LSTM precision:  0.188\n",
            "LSTM recall:  0.279\n",
            "LSTM F1:  0.224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MJomCG1nnqu",
        "outputId": "d892143e-a49d-4707-baa6-244322ac2840"
      },
      "source": [
        "#checking a simple span for definition of precision/recall\n",
        "true_output=['O', 'O', 'B-PERSON', 'O', 'B-PERSON', 'O', 'O', 'B-PERSON', 'I-PERSON']\n",
        "predicted_output=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON']\n",
        "\n",
        "print(precision_score([true_output],[predicted_output]))\n",
        "print(recall_score([true_output],[predicted_output]))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX7-quD2hnzB"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCgW9d9ohsGv"
      },
      "source": [
        "Now you can finally implement the simple Viterbi decoder. The `model` object, when applied to an input sentence, first calculates the scores for each possible output tag for each token. See the expression `model.forward_on_instance(s)['tag_logits']` in the code above.\n",
        "\n",
        "Then, you will construct a transition matrix. You can use the code below to get a list of the tags the model knows about. For a set of K tags, construct a K-by-K matrix with a log(1)=0 when a transition between a given tag pair is valid and a log(0)=-infinity otherwise.\n",
        "\n",
        "Finally, implement a Viterbi decoder that takes the model object and a dataset object and outputs tagged data, just like the `tag_sentence` function above. It should use the Viterbi algorithm with the (max, plus) semiring. You'll be working with sums of log probabilities instead of products of probabilties.\n",
        "\n",
        "Run your `violations` function on the output of this decoder to make sure that there are no invalid tag transitions. Also, compare the span-level metrics on `baseline_output` and your new output using your `span_stats` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO3QXZjazkel"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEYH2txwzT3d"
      },
      "source": [
        "# Making the transition matrix and plotting it\n",
        "def make_transitionmatrix():\n",
        "  transition_dict={}\n",
        "  tag_types=vocab.get_index_to_token_vocabulary('labels')\n",
        "  transition_matrix=[[0 for i in range(len(tag_types))] for i in range(len(tag_types))]\n",
        "\n",
        "  for key1 in range(len(tag_types)):\n",
        "    for key2 in range(len(tag_types)):\n",
        "      if tag_types[key1]=='O' and (tag_types[key2][0]=='B' or tag_types[key2][0]=='O'):\n",
        "        transition_matrix[key1][key2]=1\n",
        "      elif tag_types[key1][0]=='B' and (tag_types[key2]=='I-'+tag_types[key1][2:] or tag_types[key2]=='O'):\n",
        "        transition_matrix[key1][key2]=1\n",
        "      elif tag_types[key1][0]=='I' and (tag_types[key2]=='I-'+tag_types[key1][2:] or tag_types[key2]=='O'):\n",
        "        transition_matrix[key1][key2]=1\n",
        "  \n",
        "  #making a dict for transition matrix\n",
        "  for i in range(len(tag_types)):\n",
        "    transition_dict[tag_types[i]]={}\n",
        "    for j in range(len(tag_types)):\n",
        "      transition_dict[tag_types[i]][tag_types[j]]=np.log10(transition_matrix[i][j]+10**(-80))\n",
        "\n",
        "  plt.figure(figsize = (12,12))\n",
        "  x_axis_labels = ['O','B-GPE', 'I-ORG', 'I-DATE','B-CARDINAL', 'I-EVENT',  'B-PERSON', 'B-NORP', 'B-DATE', 'B-ORG', 'B-LOC', 'I-LOC', 'I-FAC',  'I-PERSON','I-GPE', 'I-CARDINAL','B-EVENT', 'I-TIME','I-WORK_OF_ART','B-ORDINAL', 'B-FAC', 'B-TIME','I-LAW', 'I-QUANTITY', 'I-NORP', 'I-MONEY', 'B-MONEY', 'B-WORK_OF_ART','B-QUANTITY','B-LAW', 'B-PRODUCT','I-PRODUCT','B-PERCENT','I-PERCENT']\n",
        "  ax = sns.heatmap(transition_matrix, linewidth=0.5,xticklabels=x_axis_labels,yticklabels=x_axis_labels, cbar=False)\n",
        "  ax.xaxis.tick_top() # x axis on top\n",
        "  ax.xaxis.set_label_position('top')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  plt.show()\n",
        "\n",
        "  return transition_dict"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "gVHqGrAAy86g",
        "outputId": "bd64b83d-ca89-4916-920c-13fdb37000da"
      },
      "source": [
        "transition_dict=make_transitionmatrix()\n",
        "#print(transition_dict)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwYAAAL1CAYAAAB9t/PXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdebhcVZn2/+9NIBKByBCcIopREBkkiAMC2giK2ICCiJiggKKAbduK4Bh934gNqIgIaqtRW8SfCL4CAtrdags0iqgESBACEiaHdLcMKoNGEHh+f6xdyU6lqs7Zu1ad2ufU/bmuXJzaw1NrU9Neaz3P3ooIzMzMzMxstK0z7AaYmZmZmdnwuWNgZmZmZmbuGJiZmZmZmTsGZmZmZmaGOwZmZmZmZoY7BmZmZmZmhjsGZmZmZmaGOwZmZmZmZoY7BmZmZmZTmqSTht0GmxzcMTAzMzOb2vYZdgNsclh32A0wMzMzs4GaJmkTQJ1WRsQfJrg91lCKiGG3wczMzMwGRNKDwAo6dwwiIuZMcJOsoTxjYGZm1oOkkyLig8Nuh1kflkXETsNuhDWfawzMzMx6c362mY0EdwzMzMx6myZpE0mbdvo37MaZjcPpkqZJmtVaIGm6pKMk3TjMhlmzOJXIzMyst22Aq+mSnw04P9ua7q/AH4A/S1oOnAj8K3AVcOgwG2bN4uJjMzOzHiRd6/xsm8wkXQ8cEBG3SHoucCXw2oi4eMhNs4ZxKpGZmZnZ1PZQRNwCEBHXAMvdKbBOnEpkZmbW2+mSpgGbRMTdkPKzgSOAYyPi2cNsnNk4PF7Su0uPNy4/johPDaFN1kDuGJiZmfXm/Gyb7L4EbNTlsXPKbRXXGJiZmfXg/GybyiQ9PyKuGnY7rBncMTAzM+tB0jUR8dzS4+sjYvthtsmsH5K2BeYV//4UEc8bcpOsIZxKZGZm1pvzs23Sk7QlqzsDfwOeBjwvIu4YXqusadwxMDMz68352TapSboSmAmcAxwUEcsl3e5OgbVzx8DMzKyHiPhIt3WSnj+RbTGr6ffAbOAJwObActyptQ5cY2BmZlaB87NtMpL0OOA1pPftVsDGwCsi4hdDbZg1ijsGZmZmY3B+tk0lkh4PvI70fn5qRGwx5CZZQ7hjYGZm1kNbfvY5pfzspw+5aWZ9k/S0iPh18fdnIuIdw26TDc86w26AmZlZw/2eVGzcys8G52fbFNHqFBR2G1pDrBHcMTAzM+shIg4AdgCuBhZKuh3YRNILhtsyM7O8nEpkI0XSJyPi+GG3w8wmL+dn21TVfjO/LtvMjIj7JqpNNrE8Y2Cj5nXDboCZTW4RcWdEfDYidgN2by2X9JkhNsssB41jm2slvX7gLbGhcMfARs14vvTMzMbF+dk2GUga732rTh/HNnsCh0j6oaRn9tGs7CQ9ddhtmOycSmRTjqRNu60ClkbEUyayPWY2GsaThmE2DOX3Zq4rD0l6JXAmcBXwaGt5RLyq39h9tMmfwT75zsc2FV1NumJIp9mBv01wW8xGiut4bFCc296X8u9h3zNbkp4FHA/8GPgcpY7BkDkroE/uGNiU42uLmw3V60gnDKPIJyWDda2kBRFxzrAbMgllSw+R9DHg1cCxEfEfueJmMlvSGd1WRsQ/TWRjJiN3DGwkSHoGMB94fURsN+z2mE1hU+7kWNK6EfHwODYdT352r+e5oihots72BD4t6UjgbRFxy7AbNIlsI+k60ufzGcXfFI8jIp5TIdbzgZ0i4q+5G5nBSlLWQBaSnhoRv8kY78yIOCJXvEFwx8CmLElPBg4hdQh2AE4GfCUFsz6NUccz5ToGwC+AMfOzI+LMPp/HhZM9FIXeBxa57VdIakxu+yTw7IyxNmlopwDgnoj4WsZ436H47GdSpQM2FO4Y2JQj6SjS9cVnA98CjgQujIiPDLVhZlPHqNXxZM3P7sFXAxlDg3PbG6119SxJTwdas+bLIuK24bVqIB7KHC/3QMdjJe3ULW5EXJP5+Spzx8Cmos8CVwLzI2IxgCT/4JplMoJ1PDnzs1/TbRUwI9fzTEUNz21vNEkzgS8DzwOWFIvnSroaOLJiUfccSRd1WznkmZuFkl4bEd8uL5T0WuDeiPhhxXi5axZmA6fSuWMQpHS5oXLHwKaiJwEHA6dKeiJp1mC94TbJbGqb4nU8OfOz9++x7rt1Gzgimpzb3nRnAMtIn89HASQJ+DBpMO2wCrHuIp3cNtGHgQM6LL8MuBio2jHIWrMA3BIR2U7+B1Gz4I6BTTkRcQ/wBeALkp5CqjP4vaQbgQsi4oPDbN9YN2DJWehkNkgjVMeTLT87It7UbZ2kJ+R6nimqybntTbdb+wlkpBtZnSBpecVYD0TEf2VrWV6PiYi72hdGxN2SNqgRL3fNQm7ZaxbcMbApLSJ+RxrZOFXS1jTjpOV7rJ2fHcDmwOOBacNolNl4jVodzyDzsyVtDBxE6lw9G3hyvzHNKqqaR3/7QFqRx8xOVxGTtB71UvVy1yy8N3O87DULvvOxTTmSXtJrfURcPlFtGQ9JWwLvA14GnBERnxlqg8zGIOkhUh3PcaU6ntsiYs5wWzYY3fKzSSkGVfOzkTSDlCs/H9gJ2IiU/nB5K83D1ibpT0DX729flag7SV8DbgU+GqUTP0kfBraOiDdWiHUQPepuIuL8ftraj6IO5QnAP0bEn4tlG5IuJXx3RLyvYrx9gA1z1SxIupTu/+8iIvaqGO9+0p2nO9Ys1ElbcsfAphxJF3dYHKQpty0iohEj8pK2AhYALyTNanwtIqbiFV1sipG0GamOZx7QquM5IiK2GGrDBkTSmcAdwAkd8rOfGRHjzs+WdDbwYuAHwDnAJaS841Er6K6sSHl5S7f1DU5vGbqic/sV0qU3y53ba0md23srxPpqj9UREW+u3dA+SVoX+GfS++TXpBPmLUjH/uGqv7GSrgAOaE9PkjQLuDgiXlQx3s4dFu9Cmkm4MyKeXzHetRGxU5V9xozpjoFNdZJ2Az4EbAKcGBGdOg4T2Z7tSR2C7YBPAN+MiEeG2SZrnslSi1Kq45kHbEAD6nhyk7Q8Iraquq7L9kuAdYCzgHMi4ndTebYlp0GcBDWVpEMi4twBxH0GsG3xcFlE3Nq2fruIuGGMGK8Z5qzAeBSzcs8sHt4SEStrxlkcEc/rsu66ihceaN//70iDC+uTzk3+vUYMdwzMxkvSXqQPXQAn1bhM2UBIegT4LanWYK0OgW/ZbgCSfkmPWpSmzHyVtep4IuKEYbclpzE6BrdExDM7resRbxtSR+oQ4G7gWcD2EfH7vhs7hUk6PyK6Xe51SpH0XVId6D9M5L0GJF0TET1v6DWebYalw+WAg/QZWxIR99eIdzOwbZeahWVVBgVK+76CNFj5IKlDcGnVGKVYL899buOOgU05kvYljcjfS/rQ/WTITVqDpMN7rW/4FRBsSJpUizLZ6nj6lTM/u0PsnUm1BgcDv4uIXftt71TV5Nz2QZB0AOlKX2cDn2fNuzz/YUDPOeYIdMM7Bp3SnDYlpRIfGRGXVIyXu2bhKtLgzimkOq01VC0Wzl2zAO4Y2BQk6VHgd8BSOnxgmlSgVnzBEBEPDLst1kxNrEXJXcfT9NSEnPnZPZ5DwIunWqcqpybntg+KpB1JBdd/ZPXvWQwq9WycMwZ/AW7ptIrq9/WYEJKeBnwrIl5Ycb/cNQuX0ftEvlKxcO6aBXDHwKagIm+vqyYUqEl6G/ABUk42wAPAxyPiX4bXKmuSyVSL0m8dT5NHIMsy5Wf/n17rp1oaVk5N7kDmrgmQ9BjSZ+q1wHsiYkJufjfOjsENwN93W9+6vG/T9PM9k6tmYZBy1CyA72NgU1ATTvx7kfQhYFdgj1buqKQ5wOmSNo2Ifx5qA60plrK6FuUFwAvSoHLShFqUptbxDErREbi1xyZfJ80q9PLnDss2IN0LYjPAHYPuPgQ0smMAvFHSm8hXE3AdcB7w3EGfhEraJCL+WDwcz3X7H2rqyX83kp5Fyumvul+nmoWNJdWtWXhvRHyi+PvgiPh/pXUn1blwQ86aBfCMgU1BpaLNjoY9zSnpV8CO0XYHz2JEYmlEbD2cllmTNLkWJXcdz2RMTeik6hVCJG0EvJPUKfgWcGpE3Dmo9k12TZ9ZylkTIGnbiFjWZd1aN/AaR7wvR8Ral3qVtAXw7xGxfYVYn42If6zy/BOlSHNs//3fFHgS8IaIWCuvf4x4uWsWVr2H29/Pdd7fuWsWwB0Dm4KKXMKuqo505J6+lnRTRGxTdZ2NrqbVouSu45msqQntxvvDLmlT4N3AocDXgNNLI7bWxWToQOaqCZD0k4jYvfj76+UC95onkGeSskQOi9X34ng2aUbyhIg4s0Ksw+k9+HZWlbbl1CGVOIB7gOURke0uxn3ULKwaPGgfSKhz6dHcNQvgVCKbglonEZI2BlqXEru5jwLB3NPXKyTtFRE/Ki+UtCfwP3UCStoceBop9/FPGdpoDdBeiyKpKbUoL80cb9KlJtQl6RTgNcAiYIemdPYmiduB/YfdiE7aagIOzVATsEHp7+3an65GvDcBXwTOlfR60sUMzgXeVqOtHa/rD7wKmE26R8dQtFKJ237/f5ezU1A8z6+LS5ZW3rXL350ej6cde9RoQ0/uGNiUU3xBfxE4gPRDIuBpki4Ajsn9BVHDPwEXSvoJcHWx7HnAbsCrqwaT9BbgJFLu89MlHRURF+VqrA1Hk2tRBlDHc0XmeBOmRn72caRc4A8BC0p1I61R75n5WzllNLkDmbsmoNdJYp0TyACOknQGcBlpIOngiPhZjVjvaP1dXE3rUNKllH8GnFg1Xk6l3/9Xk37/12EAv/91axaAHSXdR/q8zyj+pni8fo125K9ZcCqRTTWSPgrMIX0J3F8s2wj4HPDriPhwxXjZp68lrU+6dnlrJGgZ8I32uoNxxroeeGlE3FWcOH4jKt6m3ZqnybUouet4mpyaAHnzs62+hue2564JuI3UiVyHlD9+fGsV8ImIeEbFeJ9h9Q0T5wPXADe21le9mIHSZTyPKNr1M+DkiPhVlRiDMIDf/6w1C7nlrlkAdwxsCipOlF8QEX9pW74h8LOqP+KDzn+WtBnwEuA3EXH1WNt32D/Ll0Gx77ci4nXF3x+P0s1bJP0gIvauE9eqa3ItygDqeLrdrO1VwOyIGOrsdub87E17ra9apDpKmtyBHEBNQK97NhARb6oYL9vFDCS9nVQ0/yNSauMdVdoySAP4/c9asyDp+cCsaLuUqKRXku47UOkcIHfNAjiVyKamR9u/FCAVbkqq0xPOOn2tdKv790fE9ZKeRBq5WQw8Q9KiiPh0xZBPKaaHOz6uOBJUvr37y0nTwy2bV2yX9Sd7LUouuet4mpyaUMiZn301q0du2wVptNM6y5rbnrk2K2tNQNUT/3HE63riL+mpFcN9BrgT2B3YrUM63DCLwLP+/g+gZuHjpO+TdsuArwJVi4Wz1iyAOwY2NYWkTej8Zfxoh2VjyZ3//PSIuL74+03ADyPisGK68wqgasfgPW2PK886lGTNa7W+ZK1FyWkQdTwdUhNe24TUBMien/30zM0bGTk7kAOozcr63SnpsF7xIuLrNWK+iNSBujwi7pT0HOD9wItJd/Mdrya/h7P+/g+gZmGjTgONRTHzrKrtI3PNArhjYFPT40gnUnWu3NDJVb2+pGtMX5dvob4X8KUizv1Kl4GsJPNI0GMl7UT68ptR/K3i34yqbZM0DZjRuvKKpF2A6cXqa6PGDWJGRUTcoHT343ItyuXA0XVqUTL7ELAesEWHPN4PF//GrS01YZ8mpSbAWvnZ25Jm+eZLmg+187NfCbTSwZYB36+ah56bpE9HxLuKv98ZEaeX1p0ZEUcMrXGr25GrA/kuYLtybRbQT8dgY0kHkr47N9bqG2OJ9JtU1fO7LG/NjlTqGChdDWs/YAnwPknfB95Cuu/Cm6vE6jaDLml3YB7w9optKxfw96vX73+dwa3Wd91Tc3zXke4O381jqzYuIqZV3WcsrjGwRmhybnvu/OeimOkHpOvA/ytpBuFPRVHp4ohon4YeT8yuI0ERMe6RIEk975gYEZUuUynpk6S8ydZVE24HrieNZFxTfp2tt35rUTK3JXce76Ok1IS7WPPHuwmpCbnzs2cDl5DSwa4lHeNOwBNJFxH47z6a2pdBFDLmlDO3Pffx5a4JaItdnh1ZRrqp4HUVYywjXTHpr8WI+m+B7fvthBeDR/OBg0kj6udHRLffzG4x7gTuJs2Y/xS4IiJu7qdduQzgu+4LpBqFDxUzka3X9yPAEyPiqIrxstYsgGcMRoakbSLipuLvx0TEg6V1u9SZEs9s4LntkrYG3hMRb62y3wDyn48ETgBeBhxSym3dhZRjWEnOkSDgA5nfC3ux5sjXnyJi/+L/448zPs+UM4BalJxy1/E0OTUh96zcicDn218/Sf9E+sz27IQMmLr83RQ5c9tz1mZlrwmA7Ol1f23NNEbEHyUtr9spKH5L5xX/7ibV26jqwFFLRDy+iLlr8e+4ov7jZ6ROwicqtm866Xe6Nch2A3B2+byngtzfdccBXwZukbSkWLYj6bt9rSufjUPumgV3DEbI2UBrNOTK0t8A/9L2eBiy5WcWo+WfBJ4MfIc05fdZUsHgqXUal/MLOiLuBI7psPxSSbfWCLkvsFOmkaDc74V12tIj3gfpF7wYcalE0htIP0Bfb1v+RuCRiDi7r9Y2S+5alJxy1/HM6DVwAQz92vUZ87N36ZSSExFnKF2idpjWKV7XdUp/t17j7CkLNeTsQOaszcpeEzCA9Lo5klqpUiLVVaxKnYpqdyu/iTSws19E3FK099h+GlfMENwMnCnpGaQrAb4T2BsYd8dA0raklLArWP2a7kG6Z8irI+KGqk3L+V0XEX8G5hXpa6s6LhFxWzHIUPWGh7lrFtwxGCG9RoIqjwwNIHc8Z277l4DPkzpA+5BG0r9GuhtlnfsEZM9/zniSARlHgsg/Sjhd0kat90NE/ABA0uOoVxj1DtIsRLvzSfn3U6ljkLUWJbPcdTzZBi4GkSefeVau182v1hqZHEfbuqUS/D3w+4qpBO2v6zWldXUKaHPmjmfNbc88CwSZawLIf+Wf9gsWfLLi/mWvAV4PXCrpP4Bz6OO7QFJrpuBFpN+/20gDcG9gzffgeHyGdLWwH7Y9x8tIA4RVZzVy1yx0/P2XdDb1fv+z1iyAawxGRu7c0dy54zlz2yUtiYi5pce3RUTtSwDmzn9uO8l4JlA+yfhi1c6LpD+RTopbbXpx6XGlkaC2WGupOKqEpHeTUqaOiYjfFMueRuq4XRIRlX6cer1XJV037Fz0nDSAWpSmUsZrcQ8iTz5nfrbSjauO77SKejeuugR4U/tJc/E5+2pEVE4lyGWQueOZctuz1GZ1iJujJiDrvUJKcdcn/e5Aukxr7QsZSNqA1OGYR0pZOQu4oDUAVCHOo6QOwGnF/pU7yKVYve7/cmNEPLtu7BwG8PuftWYBPGMwSlr5k2LNXEqRvhiryp07njO3ff3SjAPAg+XHEVF1BCJ3/nPO1B/IOxJ0FzXTrTqJiE8p3Tn6J8WPCKSp0o9FxOdrhJwhaYNiOnaVIr1mepd9JqtstSiaoBoj1azjIe+1uAeRJ59zVu6/gP27rOvaKe8hWyqBpJ6dpqrfnQPIHc+W2555FqgVM2fKaeteIU9ndcrJsoi4rY+2nUQ6tl+TPhtbKBVNL4iIv/Xav0sb/0ya7Tu7+C07mNQZqtQxIKX9tt4jRxdtvYY0e3hlxWNep/07DlZ1iGqd8ypvzULu3//cNQueMRgVynhVjSLe0ojYsfR471KayBoj9uOMl+2KF2PMPkTVEbTcJ1UdRjFr3Z2wQ9y+R4Jyvg4dYm8EKRWmjxjHkzqlx5R+OLck1ZFcFhGnVIg1aesVJD21NQMzzu1zzxj2rOOJiNMqxruT1ekIhxR/Uzx+XUQ8oUKspaSc4nVIV//Zg9UdhEvL31sVYmablctN0i0R8cyq67ps/yhp5vfu1qLS6srfnR3il3PHZ0dEpTTRon0/Bo6M1bnttWaEc84CFfGy3g1Y0kzSCd/zSJ0XgLmktJYjI+K+bvt2iXcasBFwbKy+7OZM0ud4ZUS8s0Ks9rt3B2lwMMsJpaTHkjow7yLNlI67vkXSh0iDJ29v+404gzTTekLFtnSqWdiZ4n4yUbFmYYC//x1rFqr8TqyK5Y7B6FDGuzxKupF0Ca/725Y/Dvh5t6m8HvGyfDjG8TzrVR0ZGcBJVXu6zkvo4ySj20gQaVS50kiQpPMj4jVtyzagyCmNiH2rtK3YfxqwSUTcXTyeThpVO7bOtK6kY4APABuSjvV+asxASPo5sFcUdTKl5RuQUgt2rhgvZ653a99cl6HNlqpT7PNz1qzj+SCpjuf/1OyQ5rwc6B2kosCOOcE1TyL/rtf6KO6OWiFets9EzlQCSe8CXgvcS+qcXdD++ajYtm654z8jpZxWujmUpANIue27Aa3c9i9HjZvG5T5BU/6U0zOBO4ATIuLRYplI181/ZkT0KnbuFG85sHX7yXvxXrwpIrbqvGfHWLez9t27NyJ1YI7sNIM1RrzHkd4jrffLTsBy0vfLFRHx7Yrx/hF4L6tz7P8MfDIqppoVsX5E+n3pVLOwoOpsVe7f/yJm1pQ4dwxGhNru8gj0dZdH5c8dz5rb3hZbpPzH+aSrKIx79LHYP/dJVe6TjGwjQaWY00lTnvOBVwDnkXJ4L64Y5/Wku0b+mfRFfyIpX/4q4KNRPa2rHLuvGYhenTrVqFdQ5lxvZcxFHUDnNncdz8HAxXU6FRMp06xc1s9E0ZH9MvAC1hxZvgp4a53PRzH6+HpSmuKvgZMiYknvvTrGyZY73ha379z23LNAylwToJSu1vFkvde6HvFujoitq66r+ByvIZ1b7FNxv7so0oZII/NXRUSvIv3xxs0xS521ZmEAv/9ZaxbANQajJOtdHiN/7njW3HYApSslzQcOADYlXbGiU9HfWHLmP0O6alPHaWDVuxrGfrSNBEXEfZLeRrqsXJUp4r1JP7Z7A5eSfnCfH/Wv0f0hYOeIuEUpf/lKUt5tpQ5GWxu3J11qcLvi8Q2k0aBfVgyVu14h92Xjcuai5q4xyl3HMx/4nFKe9zdJdwF+pEa7UOY8+SJmzvzsrJ+J6H35w/VqxrxN0oWkK8K9Edia1Z2OKnLmjpfblyO3PWdtVvaagDHUqZ1ZJumwiDhrjUAppfKmHI2KiPOVUnmq7rfWvYqU0s7mk2aqK11oIfMsde6ahdy//7lrFjxjMCpyjBL2iJ2jV56zPSeRfih+QzrJuICUW1iriFgZ85+LeOXR2x9FxF6d1lWIl20kSKtzeI+IiNuLZbVHgzu8766PineKbIv3atIP+Mmk4ipIObgfAI6PiAsrxMpWr1Dsmy3Xu9gnW6qD8tcYZa3jKWLOBA4kjVTPBS4EvlljBC17nnzOWbncn4kO8WvPkLbNFPyW9F33vRyjt0X82rnjxf7Zc9tzzAIVcXLXBHyNNMP/0fLxSfowaSDojRXjzSZd1nklq3Pln0fq/B0YESuqxOvyHBsCP4mKNYal/Z9M+o2dD+xA+p4/v8qgzwBm5HLXLOT+/c9es+COwYgondy2vL78OCre5bGImTNPNltue3GsN5NuAHVxRDzY58lt7pOq3KlJ3yF9eXYaCXpdlSlxSXNJ742DSfnA55DyxntOk/eI9zvgU6VF7y4/johPrbVT73hLSQVfd7Qt3xK4MCoWlipTvUIRK/et7nPXomSrMRrjeSrX8XSIsRkp1/0fgE2jWj1F1jz5ImbO/Oysn4lS3E4zpBdFhfsIFJ2q60idsvtomxGt8XnNnTueLbe92ywQNWqzinhnkrcmYCbwFdI9PModjWuBt9T9DEvakzVnNH5UI8a7OyzehHTPhs9GxJcqxjuKNFM9G/hW8e/COoN5kq4HDsg8S52zZiH373/+mgV3DEbDAE5uB5I7rgy57cWP9ctJXzR7kVJiXgZsEWvehXe88bLmPyt/vvdARoKUCgfnAQcBS0knWIsqxvi/vdZHxEcqxruh27SypGURsW2VeKV9c8x6Zc31VsZcVGWuMeoQv686nrZYm5BO7OcBWwHfjojKd1VVpjz5IlbOWbncn4lsM6SSFtIjPbJG2waSO97heSrntuecBSr2zVoTUNr3GUDre21ZRNxaM86nWX0vib5mBzq8h4M0KHJ5ldH9UryHSO+R4yJicbGs7tWmBjYjl+l3Ivfvf9aaBXDHwGrK3SvX2rnt5wKfiYgt+2znY0g5+PNIxWU/ioj5FWNcQLoKRt/5z0W81oihgGNZPVoo4F1VRkfb4vY9EtQl7jqkjtXrI6LW9b1zKWYM9o+2S7ApFf5dHNULhteoVyBdn7pOvUI5Zsdc7xojkDO7pR+o+uVKrwdeGqUao4h4UZX2dInb9yh1EWdDUhrRPNKo8kWk0f7L2kfpK8bdjtQ5eCPw3oj4Vs042Wblcss9Qzpo6iN3fIy4lU6qcs4CteL16BjUSSXMWitTjHq3Zm4gdRJ+SuqwLW3NclRVdKbo9l01zhibkTq384AnkmYMjqjzWziIGbnM2RFZf/9z/k6s2s8dg9GgdMv4Oa0fNknfJv2QA/xzRFxSMV7u3PGsue1dnmMj0gj6WWNuvPa+WfKfi1hZRwxzK76k5wOtKzHcSDrWe2rGeyUpXac16nUD6Trf/1Yj1gHAJ0ij3+XZkfcD74+ICyrEylav0CV+v1fDypaLmmNkqi1e7jqeu1l9+cnvV+1EtcXKniefe1Yu82ci2wyppM/Qe8agcsppEbfv3PEx4lfObc85C1Tsk7smYGD3lChej1Yn4VXA4yNiZsUY7ySl16xfLLqHlHZ6jqQtIuK3Ndv2FNJ7ZR6wAWmm+oMV9s89I5e7ZiF3+7LWLICvSjRKPgK8o/T4WaQe7waka5BX6hgAj9eaeYYblx/X6JU/l/Rj/p+SWrntlQrTWop23BsRX2lb9TrS1HFlRY/8a8DXtDr/+QxJlfKfi1gfKdo5qzUC0Q9J99P5x3xdYHpEjPtzLunZpPfC90m5rCLd4fqDkl4aFe/iKemtwNGkH5DyyYRMpQwAACAASURBVPfHJD0lKqYmRcR3lPKMj2P1+/kG0qjt0iqxSHcVfnmsWa9wndJlRy8s/lXWZRS9ztWwyicC7UWXVa9KUr4S0VqPa5zwvYU0Sv15Vo9S9zPKtENE/E+nFTVGvW5hzTz5pwJvS/20eiOGxYn/C9tm5f6tzqzcAD4Tj5A6Vf9RmiGdAawoThSqzJAuHnuT8dPaueNHknLHaw1+aIzc9orhcl+l5x2kmoDyHWhX1QTUiPdu0u/MSvLVyojUMduVNAu+Lenz8vVe+3WIs5CUMvniKK66VHTITy9mb9/K6oLuSiLid6QrFJ6qdKfrQyrun/X3lfxXEcvdvpy/E2knzxiMBklXRcTzS49XFftKuiIidqsYb2Cj3uozt13S1cAu7aOOxfTf4qrpJm0x+s5/lrQfqcDtb6QbMb0uIn5at00d4m9IOhk9mvT/7rgK+34b+FZ7yoWkg4D5EXFQxbYsA3aPiD+0Ld+MNMJX+QZnPZ7rNxEx7su9KXO9wgBG0bPloip/jVHuOp6csyMLyZgnX8TMmZ+d9TMh6aTWiKqkl0dxI6ZilvOAKjOk5Vg5KGPueLFvttz23LNApbhZagJK8XLdU+KHwExS/dPPgJ9FxI0127Sc1Jn/a9vyGaRLj8+PTDVMNb7Xs/6+DiA7YmDtyzUz7BmD0bFx+UGseQWgyoWCg0x3KT4kPy2mKl9G+lKsMoq2bqdUhIh4SK1hwwrUOf/5o9TPfz6JNNJyk6QXklJjehYQjbOdG5MuA3gY6Rrfz4/q6T87RMRr2xdGxHnFiW/lZrWfABXx7qnxUoz5XBW3f7jTaHQx4lX55Jb8o+itWTmx5gydgLWu+91L1RP/ccTLOUoNGUe9ImJhxecej1tIM0CfKN63/eRn5/5MtO48DfBx4IdFvPtI9yGpGyuHJ5E6y6dKauWO17q3Aqz5u6M+c9tzzgIV7SmfgLU6FY9rLa+aclJqZ657StwGPIc0oHUPcLeku2qOWj/S3iko2rpS0opcnYJC1Q9F7t/X3NkRg2pf378TLe4YjI6bJO0bEd8rLyx6r5XSQ0r7ZsuTLeJ1y22vctMaSDckeUJE/L4tft0rpdxBOgn6F/rMfy48HBE3AUTEz1Vc6aAupZtnHUeacv1X0s1O7q0Z7s8113Vzn6Qd29N8JO1IujRoTlVPwv8vKXWtY71Cjed/EqtH0T+tdK3/GZLWrTOKDnyJ1alv5b8hXf1o3JS5xqgs0o1/zgPOK97LB9YJ0+XvTo970gDy5CPisxSpKlozP/tdwONJI7HjNZGfiaqmFbOiHU/GOnVoeikGJr4AfEGrc8d/L+lGKuaOt6gtt11SX7ntxXu/9vu/ZDE9agJI9Ubjps61MidFzVqZiDi6iDuTdF3+XYG3K13G+PqI6Dmr2GaFpL3aO1FFJ6vv+yG0qfq9nvX3lbW/e9sfVzXI9vX1O9HijsHoOBb4nqTXku48CbAz6cthv6rBlDlPVnlz208hHetxrHmsp1Dv7pY5859h7RGINR7XGIH4NWn69qvAX4AjyyOPFeO1t62l7ujDccBFSneILZ98Hw68oWqwLm1rtW/DKrEib71C9lH0yJuLmrXGSPnreHKOemXNk28pZhv7zs8m82eC7v/vgMqf/22KNnXqGARQ+2IQ0WfuOOTNbVfG2qxC7pqA7LUyhQdJvxMri7+fQvU7vf8TcKGkn7Dme3g3Ur1HJTm/18n8+5r5e3gytM81BqOkOFk5lDUvzXh2pynBccTKnSebO7f9laRR3+1JX/43kG5c9e9V4hSxct+pMPdVCRaSKac6d9uKmE8g1TysupQq8LmI+N8asSbkik5V81qLfbLlehf7ZctFVf4ao6x1PDlfV2XOky9iZsvPLuI18jOhDHdNrfBcdT5jA8ttVx+1WW1xctUELCTvPSVOI3VqtyINvrXuLXFl1LhZmtIdo+ez5nv4GzXPJ3K+h3P/vuauCWh0+8AdA6tJ0o3dTv57resR71cR8ayq6yaCMt+p0JpP0m+j+vWkuxaB1WzDdaQv+VW5qBFRKxdV+a+xvjS63GVa0i8jYoc67cwhx//7DjG/SMrPXknqGFxJOqHKMkLXFBPcMajzGbspIrbpsq7W74TWrs06LWpemrkUM8v9M3KS9E+kjsCS6OM+PKMm5/fwIAyifevkaZpNRpJqFUMV7ityYttj1s2TzZ3bvoY+jzVb/nM3fbZvoPGa3LZBxCs0YcRkjVxU+strvUnSvu0LVb/GaB11qNnptKyuPl7XaZI2kbRpp391AkbE0ZFuCHcAcBkpNfH/k3S10vXr+9Kgz8TpGWONpc5nbIWkvdoXqkZuu6RZkk4mpZs+TKrN+lDdToGkOZI+KOnnpNS9pcCzc3YK+nktIuKMiLi63CkYld+JPmPl/B7uqGntc41BwxXTda3RvFvqTNP1Ct/HvoPKk21Xu7K+Q5y6slf9d5D7Ej054zW5bbXjZc5rhby53uV4HR9XjJe1xoj8dTyd1H2fDCxPnjz52Z004jMREWfmigUD+YzlzG3PWZsFg6sJKGvE+2QCYuWOl+P3v+PjBryu2dvnjkFDSVqXdFmrN5O+wARsUZyIL2jP7a3pe2Nv0llE/ETSC0g5mUcUi5eR8o4r58nSu9K/VmV9m9rHygCq/jvop32DjtfktvUTr9fIylojp+PQ631SR7arYUS6Oc9zWLPG6HLgmDqDDRFxlqS7SDeJK9fx/J86dTxd1H1dl+VOh+mSn/0F4PA6+dkdNOUzkTtW1s9YRNwgaXvWzG2/HDi6xvv4FFbPWuQYBT6hFK9Op2c8pur7ZNDxcv3+d3qcQ6Pa5xqDhip+iDYCjo2I+4tlM0mjcSsj4p0ZnmMWcE+MwJug6ceau3054zW5bYOIZ81Q93UdRJ78oPOzm/yZ8OerOUbpfdLktuXWtPa5xqC59gPe2uoUwKobubwN+PuqwSTtIukySedL2knS9aRrLv9e0j45GtyUHMOmH2vu9uWM1+S2DSJeh/iNeA9PRLwmtS3z65o9Tz5nfnaTPxNN/+4cdLwmtW2U3idNbluX52jM65q7fQBEhP818B9wc511PfZZDOxNugvlH0kpP5Dyca/N1OYscfqN1/Rjzd2+nPGa3LaJeG2b8h6eiHhNattke137idnkz0TTvzsHHa9JbRul90mT29b01zV3+yLCMwYNtkzSYe0LJb0BuKlGvHUj4gcR8f+A/42InwFEUc2eSVNyDJt+rLnblzNek9s2iHjtmvIenoh4TWrbZHtd+4nZ5M9E0787Bx2vSW0bpfdJk9vWSZNe1076eh+7+Li53g6cL+nNrHn1hRnAgTXiPVr6u/2W6lny2iLiQzniZIjX9GPN3b6c8ZrctkHEWzNAc97DA4/XsLZNqte1z5hN/kw0/btzoPEa1rZRep80uW1radjrupZ+38cuPm44peszr7qzYET8qGacR0j3AxCpc/GX1ipg/YhYr2K8breTFxARMXNY8SbBseZuX7Z4TW5b7nhNfg/njtfkthXxGvu65o7Z8M9E0787/ZmYmu+TJret6a9r/u87dwzMzMzMzMw1BmZmZmZm5o7BZCPpqCbGanq8Jret6fGa3Lbc8Zrcttzxmty23PGa3Lamx2ty23LHa3Lbcsdrcttyx2ty25oYzx2DySfnGyjrm7Hh8ZrctqbHa3Lbcsdrcttyx2ty23LHa3Lbmh6vyW3LHa/Jbcsdr8ltyx2vyW1rXDx3DMzMzMzMzJcrbZBxVYF/8YtfHPe2Exmr6fGa3Lamx2ty23LHa3Lbcsdrcttyx2ty25oer8ltyx2vyW3LHa/Jbcsdr8ltG2I8dV3hqxI1Rvzt7tuyBFpv1hxyxgJYd/rsLPEefmhF9ni5jhXS8eY+1ia+rrnjtd4nTY7X5Pdw7nhNbhs083WFwfy/G6XPf5Pfd01uG4zW+6TJx9rk1zXzd13XjoFTiczMzMzMzB0DMzMzMzNzx8DMzMzMzHDHwMzMzMzMcMdgICQ9RdKFkpZLulXS6ZKmD7tdZmZmZmbduGOQmSQB5wPfiYitgK2BDYETh9owMzMzM7Me3DHIb0/grxHxVYCIeAQ4FnizpMcOtWVmZmZmZl24Y5DfdsDV5QURcR/wG+CZQ2mRmZmZmdkY3DEYIklHSVosafGiRYuG3RwzMzMzG2HrDrsBU9Ay4LXlBZJmAk8Fbikvj4hFQKtHkO3Ox2ZmZmZmVXnGIL8fAY+VdBiApGnAqcCZEfGXobbMzMzMzKwLdwwyi4gADgQOlrQcuBn4K/DBoTbMzMzMzKwHpxINQET8Fth/2O0wMzMzMxsvzxiYmZmZmZk7BmZmZmZm5o6BmZmZmZnhjoGZmZmZmQFKF9GxBvALYWZmZmaDpm4rfFWiBsl1g7P1Zs1h3emzs8R6+KEVAI2OlytW7ng+1mbFa3LbcsfL+V0CzT7WUfpMjNKx5o7X5LbljjdK7xMfa/143TiVyMzMzMzM3DEwMzMzMzN3DMzMzMzMDHcMzMzMzMwMdwzMzMzMzIwR7RhIekTSEklLJV0jadce224l6buSbpV0taRLJb2kWHeEpLuKWMskvbXD8ta/bSfq+MzMzMzMqhrVy5WujIi5AJJeAZwM/F37RpLWB74HHB8RFxXLtgeeB1xebHZuRPyjpMcDN0i6qLx8wMdhZmZmZpbFqHYMymYCf+yy7lDgylanACAirgeub98wIu6UdCvwtIG00szMzMxsgEYylQiYUaT33AR8Gfhol+22A64ZT0BJc4A5wC3FokPaUolmdNjnKEmLJS1etGhRjcMwMzMzM8tjVGcMyqlELwLOkrR9RESvnSRdAGwF3BwRrykWHyJpd+BB4OiI+IMkGEcqUUQsAlo9gsh1t1IzMzMzs6pGdcZglYi4EpgFbC7pxNYIf7H6BuC5pW0PBI4ANi2FODci5kbECyPigolqt5mZmZlZTiPfMZC0DTANuCciFhQn+XOL1WcDu0l6VWmXx054I83MzMzMBmxUU4lmlGYFBBweEY+0bxQRKyXtB3xK0qeB3wP3A/88judopRi1/ENE/LTfhpuZmZmZDcJIdgwiYlqFbW8C/r7LujOBM8e73MzMzMysqUY+lcjMzMzMzNwxMDMzMzMz3DEwMzMzMzNAY1y63yaOXwgzMzMzGzR1W+EZAzMzMzMzG82rEjVVrjsfrzdrDutOn50l1sMPrQDytg3I2r5csXLHa/2/87E2I16T25Y7XpPbljveKH0mRulYc8drcttyxxul94mPtX68bjxjYGZmZmZm7hiYmZmZmZk7BmZmZmZmhjsGZmZmZmbGiHcMJD3QY93ukn4h6abi31GldQslrZC0RNIySfPa9n13sc8vJS2V9ClJ6w3yWMzMzMzM+jHSHYNuJD0ROBs4JiK2AXYHjpa0b2mz0yJiLvBq4IutE39JxwB7A7tExA7A84E7gRkTeQxmZmZmZlW4Y9DZ24EzI+IagIi4G3gv8P72DSNiOfAXYJNi0QLgbRHxp2L9QxHxsYi4b0JabmZmZmZWgzsGnW0HXN22bHGxfA2Sngssj4g7Jc0ENoyI28fzJJKOkrRY0uJFixb13WgzMzMzs7p8g7P6jpX0JmBrYP9OG0h6BfBxYGNgfkT8tLw+IhYBrR5B5LqJmJmZmZlZVZ4xACSdWBQSLykWLQN2bttsZ+CG0uPTImI74CDgK5LWL9KFHpD0dICI+H5Rh3A9MH2wR2FmZmZmVp87BkBELIiIucVJPMDngCMkzQWQtBlp5P8THfa9iJRmdHix6GTg85I2LvYVsP6AD8HMzMzMrC9OJeogIv5H0huAL0naCBDw6Yi4uMsuJwBnS/oS8HlgA+Dnkh4EHgCuAK6dgKabmZmZmdUy0h2DiNiwx7rLSZca7bRuYdvjq4FnlRadUvwzMzMzM5sUnEpkZmZmZmbuGJiZmZmZmTsGZmZmZmYGKCKG3QZL/EKYmZmZ2aCp2wrPGJiZmZmZ2Whflahpct35eL1Zc1h3+uwssR5+aAVA9nhNPFZI7Wvy/7tROtbc8Zrcttzxmty23PFG6TMxSseaO16T25Y73ii9T3ys9eN14xkDMzMzMzNzx8DMzMzMzNwxMDMzMzMz3DEwMzMzMzNGrGMg6YEuyxdKWiFpiaTlks6XtG3bNnMlhaR9iscXFNvfIune4u8lknaVdJmkX5WWfXsijs/MzMzMrC5flWi10yLikwCSDgEukbRDRNxVrJ8H/KT4739ExIHFtnsAx0fEfq1AkgAOjYjFE9h+MzMzM7PaRmrGYLwi4lzgB8B8AKUz/YOBI4CXS1p/eK0zMzMzM8vPHYPurgG2Kf7eFbg9Im4FLgP2Hcf+3yilEp0yoDaamZmZmWXhjkF35dtFzwPOKf4+p3g8lkMjYm7x7z0dn0A6StJiSYsXLVrUZ3PNzMzMzOobyRoDSSdSjPpHxNwum+0ELJY0DTgIeLWkBaQOw2aSNoqI+/tpR0QsAlo9gsh1N2AzMzMzs6pGcsYgIha0RvM7rZd0ELA38E1gL+C6iNgiIraMiKcB5wEHTlyLzczMzMwGayQ7Bl0c27pcKfAGYM/iikTzgAvatj2PsdOJyjUG/zmA9pqZmZmZZTNSqUQRsWGX5QuBhV3WvanDsouAi4q/LyMVJJfX79FPO83MzMzMJppnDMzMzMzMzB0DMzMzMzNzx8DMzMzMzABFxLDbYIlfCDMzMzMbNHVb4RkDMzMzMzMbrasSNV2uG5ytN2sO606fnSXWww+tAGh0vFyxcsfzsTYrXpPbljte7rbl/G6CZh5r7nhNfl1zxxvFz0ST443S+8THWj9eN54xMDMzMzMzdwzMzMzMzMwdAzMzMzMzwx0DMzMzMzNjiB0DSY9IWiJpqaRrJO3aY9vDJF0v6ZeSrpV0fGndupLukvSxtn0uk/SrIv5VkuaW1t1RxPqlpGWS/lnS+sW6LSVdX/y9h6SQtH9p3+9K2qP0eJakv0k6pu3575A0q4//RWZmZmZmE2aYMwYrI2JuROwIfAA4udNGkl4JvAvYOyJ2AHYB7i1t8nLgZuBgSe3XZT20iP8vwClt615axHsBMAf4Ypd2/g5Y0OM4DgZ+BszrsY2ZmZmZWaM1JZVoJvDHLus+ABwfEf8NEBEPRsSXSuvnAacDvwFe1CXGlUDH6zxFxAPAMcABkjbtsMlS4F5JL+8Sex5wHDBb0lO6bGNmZmZm1mjD7BjMKFKJbgK+DHy0y3bbA1d3WlGk/7wMuBj4Jt1H7fcBvtOtIRFxH3A7sFWXTU4EPtTh+bcAnhQRvwC+BRzS7TnMzMzMzJqsCalE25BO3M/qkAo0lv2ASyNiJXAeadR/Wmn9NyTdTkoF+twYsbo+d0RcDiBp97ZVh5A6BADnUDGdSNJRkhZLWrxo0aIqu5qZmZmZZdWIOx9HxJVFoe7mkt4J7FssnwvcAOwMXNJh13nA7pLuKB5vBuwJ/LB4fChptuEU4DPAazo9v6SNgC1JtQqP69LM1qzBw23P/0RJhxaPnyxpq4hY3ut4WyJiEdDqEUSuu4uamZmZmVXViBoDSdsA04B7ImJBMZPQuorQycApkp5YbDtd0lskzQReDDw1IraMiC2Bt9M2ah8RAXwY2KV4nvbn3pBUnPydiOhW50BE/ADYBHhOsd/WwIYRMbv0/Ce3P7+ZmZmZ2WTQhBqDJcC5wOER8Uj7RhHxb8Bngf+UdANwDalY+UDgkoh4sLT5hcD+kh7TFmMlcCrwntLiS4vLkv6CVLh89DjafCKwRfH3POCCtvXnsWbH4DpJvyv+fWoc8c3MzMzMhmJoqUQRMW3srVZt+1Xgqx1Wfa1tuz8AmxcP92hbd2rp7y17PNcdpIJnIuIy4LLSuotYXYtwGW0i4jrg2WM9h5mZmZlZ0zQilcjMzMzMzIbLHQMzMzMzM3PHwMzMzMzM3DEwMzMzMzNA6Wqe1gB+IczMzMxs0Lre1LcRNzizJNcNztabNYd1p8/OEuvhh1YANDperliteDlfB2j2sTb5dc0dr8ltyx2vyW3LHW+UPhOjdKy54zW5bbnjjdL7xMdaP143TiUyMzMzMzN3DMzMzMzMzB0DMzMzMzPDHQMzMzMzM2MKdgwkPdBl+UJJKyQtKf17sqR7JM1s2/Y7kg6RdISku9r22VbSlpJC0jtK+3y22P5zxXbLJK0s7ffaQR+7mZmZmVldU65jMIbTImJu6d9/A98HDmxtIOlxwO7AxcWic9v2WVYsvxN4p6Tp5SeIiLdHxFzg74FbS/t9e+BHZ2ZmZmZW06h1DDr5JvD60uMDge9HxF/G2O8u4EfA4YNqmJmZmZnZRBm1jsGxpdSeS4tl3weeK2mz4vHrSZ2FlkPaUolmlNZ9HDhe0rQJaLuZmZmZ2cCMWsegnEr0UoCIeAi4CHitpFnATqTOQkt7KtHK1oqIuA34OTC/TmMkHSVpsaTFixYtqn1QZmZmZmb9mrJ3PpZ0IrAvQJHz38s3gQ+TbhF9YUT8rcJTnQR8G/ivqm2MiEVAq0cQue64a2ZmZmZW1ZSdMYiIBa1R/nFsfhmwFfB21kwjGs/z3AQsA/av3EgzMzMzs4aYsh2DLo5tqxfYEiAiHiWN+m/G2iP/7TUGu3aIeyLwlEE23MzMzMxskKZcKlFEbNhl+UJgYY/93gW8q23ZmcCZXXbZvrTdUto6WRFxR3kbMzMzM7MmG7UZAzMzMzMz68AdAzMzMzMzc8fAzMzMzMzcMTAzMzMzM0ARMew2WOIXwszMzMwGTd1WTLmrEk1muW5wtt6sOaw7fXaWWA8/tAKg0fFyxcodz8farHhNbhv481833ih9JkbpWHPHa3LbcscbpfeJj7V+vG6cSmRmZmZmZu4YmJmZmZmZOwZmZmZmZoY7BmZmZmZmhjsGZmZmZmbGJOkYSHpE0hJJSyVdI2nXLtstlLSi2PZ6Sa/qsLz1b2NJe0i6t3h8k6RPlmI9QdJ3i+dcJunfSuu2k3SJpF9JWi7pw5JUrDtC0qOSnlPa/npJWw7q/4+ZmZmZWb8mRccAWBkRcyNiR+ADwMk9tj0tIuYCBwP/Kmmd8vLSvz8Vy39cbL8TsJ+k3YrlJwA/jIgdI2Jb4P0AkmYAFwEfi4hnATsCuwL/UGrD74AFfR+1mZmZmdkEmSwdg7KZwB/H2igibgQeBmaNJ2hErASWAK0LxT6JdILfWn9d8ed84IqI+EGx/C/AP1J0HArfBbaT9KzxPLeZmZmZ2bBNlo7BjFa6D/Bl4KNj7SDphcCjwF3FomNLaUSXdth+E2Ar4PJi0eeAr0i6VNICSU8ulm8HXF3eNyJuBTaUNLNY9CjwCeCDY7TxKEmLJS1etGjRWIdkZmZmZjYwk+XOxyuLdB8kvQg4S9L2EREdtj1W0huA+4FDIiKK9P/TIuKTHbZ/saSlpE7BpyPifwEi4vuS5gD7AK8ErpW0fYU2nw0skPT0bhtExCKg1SOIXHc+NTMzMzOrarLMGKwSEVeS0oM2l3RiaxagtEmrluDFEfHjcYT8cVG7sB1wpKS5pef6Q0ScHRFvBK4CXgIsA3YuByg6EA9ExH2lfR8GTgXeV/NQzczMzMwmzKTrGEjaBpgG3BMRC1rFxP3GjYjbgY9RnMhL2lPSY4u/NwKeAfwG+Aawu6SXFetmAGeQUofanQm8DNi83/aZmZmZmQ3SZEklmlGaFRBweEQ8UjFGK8Wo5YAO23wBOL64tOjOwGclPUzqQH05Iq4CkPRq4DOSPkfqpHwd+Gx7sIh4SNIZwOkV22pmZmZmNqEmRccgIqaNc7uFPZZ3WncHcFlpu5WsvirRKcW/TvF+CezRZd2ZpJmC1uMzSDMKZmZmZmaNNelSiczMzMzMLD93DMzMzMzMzB0DMzMzMzMDdb4VgA2BXwgzMzMzGzR1W+EZAzMzMzMzmxxXJRoVue58vN6sOaw7ffbYG47Dww+tAGh0vFyxcsfzsTYrXpPbljtek9uWO94ofSZG6Vhzx8v5+wrNPtZRep/4WOvH68YzBmZmZmZm5o6BmZmZmZm5Y2BmZmZmZrhjYGZmZmZmTNGOgaRHJC2RtFTSNZJ27bLdQkl/kfT40rIHSn8/RdKFkpZLulXS6ZKmF+v2kHRv8Tw3Sfpkab8jJN1VrFsm6a2DPF4zMzMzs35NyY4BsDIi5kbEjsAHgJN7bHs3cFz7QkkCzge+ExFbAVsDGwInljb7cUTMBXYC9pO0W2nducW6PYCTJD2hnwMyMzMzMxukqdoxKJsJ/LHH+n8FDpG0advyPYG/RsRXASLiEeBY4M2SHlveMCJWAkuAta4lFRF3ArcCT6t9BGZmZmZmAzZVOwYzWik+wJeBj/bY9gFS5+Cdbcu3A64uL4iI+4DfAM8sL5e0CbAVcHl7cElzgDnALR3WHSVpsaTFixYtGvOgzMzMzMwGZare4GxlkcaDpBcBZ0naPiKiy/ZnAEvKdQLj9GJJS0mdgk9HxP+W1h0iaXfgQeDoiPhD+84RsQho9Qgi1w1YzMzMzMyqmqozBqtExJXALGBzSScWMwlL2rb5E3A28PbS4mXAzuXtJM0Ensrq0f8fF3UM2wFHSppb2vzcos7hhRFxQd6jMjMzMzPLa8p3DCRtA0wD7omIBcXJ+twOm34KOJrVsyg/Ah4r6bAizjTgVODMiPhLeceIuB34GPC+AR2GmZmZmdlATdWOwYzSzMC5wOFF8XBXEXE3cAHwmOJxAAcCB0taDtwM/BX4YJcQXwBeImnLLEdgZmZmZjaBpmSNQURMG+d2C9sevxt4d+nxb4H9u+x7GXBZ6fFKVl+V6Mzin5mZmZnZpDBVZwzMzMzMzKwCdwzMzMzMzMwdAzMzMzMzA3W/tL9NML8QZmZmZjZo6rbCMwZmZmZmZjY1r0o0WeW68/F6s+aw7vTZY284Dg8/tAIge7wmHiuk9jX5/90oHWvueE1uW+54TW5b7nij9JkYtMnSNQAAIABJREFUpWPNHa/Jbcsdb5TeJz7W+vG68YyBmZmZmZm5Y2BmZmZmZu4YmJmZmZkZ7hiYmZmZmRlTvGMg6RFJSyQtlXSNpF27bLdQ0opi2+WSzpe0bds2cyWFpH2KxxcU298i6d7i7yWSdpV0maRflZZ9eyKO18zMzMysrql+VaKVETEXQNIrgJOBv+uy7WkR8cli20OASyTtEBF3FevnAT8p/vsfEXFgse0ewPERsV8rkCSAQyNicf5DMjMzMzPLb0rPGLSZCfxxPBtGxLnAD4D5AEpn+gcDRwAvl7T+gNpoZmZmZjYUU33GYIakJcD6wJOAPSvsew2wTfH3rsDtEXGrpMuAfYHzxtj/G5JWFn//MCLeU+G5zczMzMwm1FSfMVgZEXMjYhtgH+CsYvR/PMrbzQPOKf4+p3g8lkOL557brVMg6ShJiyUtXrRo0TibZWZmZmaW31SfMVglIq6UNAvYXNI7SaP+tGoQOtgJWCxpGnAQ8GpJC0gdhs0kbRQR9/fZpkVAq0cQue4GbGZmZmZW1VSfMVhF0jbANOCeiFjQGs3vsu1BwN7AN4G9gOsiYouI2DIinkZKIzpwotpuZmZmZjZoU33GoFVjAGmk//CIeKTLtsdKegOwAXA9sGdE3CVpHnBB27bnAW8Dzurx3OUag7sj4mX1DsHMzMzMbPCmdMcgIqaNc7uFwMIu697UYdlFwEXF35cBl7Wt36NKO83MzMzMhm1kUonMzMzMzKw7dwzMzMzMzMwdAzMzMzMzA0XEsNtgiV8IMzMzMxu0rvf08oyBmZmZmZlN7asSTTa5bnC23qw5rDt9dpZYDz+0AsjbNiBr+3LFyh2v9f/Ox9qMeE1uW+54TW5b7nij9JkYpWPNHa/Jbcsdb5TeJz7W+vG68YyBmZmZmZm5Y2BmZmZmZu4YmJmZmZkZ7hiYmZmZmRkj1jGQ9IikJZKWSrpG0q49tt1d0i8k3VT8O6q0bqGkFUWsZZLmte377mKfXxbP9SlJ6w3y2MzMzMzM+jFSHQNgZUTMjYgdgQ8AJ3faSNITgbOBYyJiG2B34GhJ+5Y2Oy0i5gKvBr7YOvGXdAywN7BLROwAPB+4E5gxqIMyMzMzM+vXqHUMymYCf+yy7u3AmRFxDUBE3A28F3h/+4YRsRz4C7BJsWgB8LaI+FOx/qGI+FhE3Je5/WZmZmZm2YzafQxmSFoCrA88Cdizy3bbAV9rW7a4WL4GSc8FlkfEnZJmAhtGxO0Z22xmZmZmNnCjNmPQSiXaBtgHOEtS19tCj+FYSTcAPwdO7LSBpFcUdQh3dKpnkHSUpMWSFi9atKhmM8zMzMzM+jdqHYNVIuJKYBawuaQTixP4JcXqZcDObbvsDNxQenxaRGwHHAR8RdL6RbrQA5KeXjzH94s6hOuB6R3asCginhcRzzvqqKPaV5uZmZmZTZiR7RhI2gaYBtwTEQuKmYS5xerPAUdImltsuxnwceAT7XEi4iJSmtHhxaKTgc9L2rjYV6TUJTMzMzOzxhrVGgMAAYdHxCPtG0XE/0h6A/AlSRsV2346Ii7uEvcE4GxJXwI+D2wA/FzSg8ADwBXAtZmPxczMzMwsm5HqGETEtArbXk661GindQvbHl8NPKu06JTin5mZmZnZpDCyqURmZmZmZraaOwZmZmZmZuaOgZmZmZmZuWNgZmZmZmaAImLYbbDEL4SZmZmZDVrXm/uO1FWJmu5vd9+WJc56s+aw7vTZWWI9/NAKgEbHyxUrdzwfa7Pi5fx8QbOPtcltyx1vlD4To3SsueM1uW25443S+8THWj9eN04lMjMzMzMzdwzMzMzMzMwdAzMzMzMzwx0DMzMzMzNjhDsGkh6RtETSUknXSNq1y3YLJR3fYfkBkq6TdKOkX0o6oG398ZJuKp7jKkmHDepYzMzMzMz6NcpXJVoZEXMBJL0COBn4u/HsKGlH4JPAyyPidklPB34o6baIuE7SMcDLgRdExH2SZgIHDuYwzMzMzMz6N7IzBm1mAn+ssP3xwEkRcTtA8d+TgfcU6z8IvC0i7ivW3xcRX8vYXjMzs/+fvfsPs6us773//hgSjY2gNVgpqGkApQYPo1B/ULTgo0+t+AOsiFErWC3aogdpsVZDr5NqI1ilKAd6ZI4K0qNCHxX80T61rYKooHXAiQgiEOBo06flhwiHi2hM+D5/7DWw3M6eZCZrkp3s9+u69jVr3fe9vuu+Z89O5jv3vdaSpE6N8ozB4iSTwMOAvYDnzuLYFfRmDNomgBOb2YFHVFU3N02XJEmStoNRnjHYUFVjVXUA8ALggiQDnwQ3H5KckGQiycT4+Pj2PLUkSZL0c0Z5xuABVXVlkqXAnklOAo5syscGHHIdcDCwtlV2MHBtc03BvUmWb2nWoKrGgamMoLp6MqskSZI0W6M8Y/CAJAcAC4A7q2pVM5MwKCmA3jKidyRZ1hy/jN51BWc09acB5zTLikiyxLsSSZIkaZiN8ozB1DUGAAGOq6rNA9qemuStUztVtU+StwOfT7IQ+Bnwp1U1Fe9/AEuAbyX5WVN/Rn9QSZIkaViMbGJQVQu2st1qYPU05Z8BPjPgmAL+qnlJkiRJQ8+lRJIkSZJMDCRJkiSZGEiSJEnCxECSJEkSkN51shoCvhGSJEmabwMf6DuydyUaRl094Gzh0uXstmjvTmJt2rgeYKjjdRWr63iOdbjidfn5guEe6zD3ret4o/SZGKWxdh1vmPvWdbxR+jlxrHOPN4hLiSRJkiSZGEiSJEkyMZAkSZKEiYEkSZIkTAwekOTeAeWrk5wyTflRSb6T5HtJrklyVF/9KUmuTzKZ5FtJXjtffZckSZK2lXclmoMkBwHvB55fVbck+TXgn5PcXFXfSfIm4PnA06vqniS7A0fvyD5LkiRJM3HGYG5OAd5TVbcANF9PA97W1L8T+MOquqepv6eqPrZDeipJkiRtBRODuVkBXNVXNgGsaGYHHlFV3dw0XZIkSdoOTAx2oCQnJJlIMjE+Pr6juyNJkqQR5jUGfZKsAY4EqKqxAc2uAw4G1rbKDgauba4puDfJ8i3NGlTVODCVEVRXT2aVJEmSZssZgz5VtaqqxmZICqB34fE7kiwDaL6+EzijqT8NOKdZVkSSJd6VSJIkScPMGYOtc2qSt07tVNU+Sd4OfD7JQuBnwJ9W1WTT5H8AS4BvJflZU39Gf1BJkiRpWJgYNKpqyYDy1cDqaco/A3xmwDEF/FXzkiRJkoaeS4kkSZIkmRhIkiRJMjGQJEmShImBJEmSJCC962Q1BHwjJEmSNN8yqMK7Eg2Rrh5wtnDpcnZbtHcnsTZtXA8w1PG6itV1PMc6XPG67luXn1cY7rEOc7xR+kyM0li7jjfMfes63ij9nDjWuccbxKVEkiRJkkwMJEmSJJkYSJIkScLEQJIkSRImBpIkSZIwMfg5Se4dUL46yfokk83r9KZ8tyS3T+232i9Jcm6SdUmuSnJZkmdsjzFIkiRJc+HtSrfemVX1/r6y5wM3AMckeUc9+FCIDwO3APtX1f1Jfg148nbsqyRJkjQrzhhsm5XAB4EfAM8CSLIv8Azg1Kq6H6Cqbqmqv99hvZQkSZK2wMRg653cWkr020keBjwP+DzwSXpJAsAKYLKqNm8pYJITkkwkmRgfH5+/nkuSJElb4FKirfdzS4mSvBy4tKo2JPk08OdJ3jqbgFU1DkxlBNXVk1QlSZKk2TIxmEaSNcCRAFU1NqDZSuCwJLc2+48GngtcCxyUZMHWzBpIkiRJw8ClRNOoqlVVNTYoKUiyO/Bs4PFVtayqlgEnAiurah0wAfxFkjTtlyU5cjt1X5IkSZo1E4O5ORr4clX9tFX2WeDFSR4KvAH4FeCmJN8Fzgdu2+69lCRJkraSS4laqmrJgPLVffsfAz7WV/YjYM9m96fAH8xDFyVJkqR54YyBJEmSJBMDSZIkSSYGkiRJkoBU1Y7ug3p8IyRJkjTfMqjCGQNJkiRJ3pVomHT15OOFS5ez26K9O4m1aeN6gKGO11WsruM51uGKN8x9Az//c403Sp+JURpr1/GGuW9dxxulnxPHOvd4gzhjIEmSJMnEQJIkSZKJgSRJkiRMDCRJkiSxkyYGSe4dUL46yfokk0m+m+Ql05RPvR6Z5PAkdzf71yd5fyvWryT5QpK1Sa5L8g+tuhVJvpzk+0luTPLnSdLUHZ/k/iT/pdX+u0mWzdf3Q5IkSdpWO2VisAVnVtUYcAzw0SQPaZe3Xj9uyr/atH8q8KIkv9mUvwv456o6qKqeDPwZQJLFwOeA06vqScBBwKHAH7X68G/AqvkcpCRJktSlXTExAKCqvgdsApZuZfsNwCQwdT+ovej9gj9V/51m81XA16vqn5ry+4A30yQOjS8AK5I8aVvGIEmSJG0vu2xikOQZwP3A7U3Rya1lRJdO0/5RwP7A5U3ROcBHklyaZFWSX23KVwBXtY+tqnXAkiS7N0X3A38FvHMLfTwhyUSSifHx8TmMUpIkSerGrviAs5OTvAb4P8CxVVXN8v8zq+r907R/dpK19JKCD1TVfwBU1ReTLAdeAPwO8O0kB86iH58AViX5tUENqmocmMoIqqsHHEmSJEmztVPPGCRZMzUL0Cqeupbg2VX11a0I89WqOojeTMDrk4xNVVTVj6rqE1X1e8C3gOcA1wEH9/VjOXBvVd3TOnYTcAbw9jkPUJIkSdpOdurEoKpWTV1M3EGsW4DTaX6RT/LcJA9vth8B7Av8APg4cFiS5zV1i4Gz6C0d6nc+8Dxgz23tnyRJkjSfdurEYJba1xhMDrh96IeA5zR1BwMTSb4DXAl8uKq+1Vyk/FLg1CTfB66hN5twdn+wqtpIL2l4zDyMR5IkSerMTnmNQVUtGVC+eoby6epuBS5rtdvAg3clel/zmi7eNcDhA+rOpzdTMLV/Fr3kQJIkSRpaozRjIEmSJGkAEwNJkiRJJgaSJEmSIFW1o/ugHt8ISZIkzbcMqnDGQJIkSdLOeVeiXVVXTz5euHQ5uy3ae8sNt8KmjesBhjpeV7G6judYhyveMPet63hd/lsCwz3WUfpMjNJYu443zH3rOt4o/Zw41rnHG8QZA0mSJEkmBpIkSZJMDCRJkiRhYiBJkiQJEwMAktw7Q93+Sb6QZF2Sq5JcmuQ5Td3xSW5PMpnkuiR/ME351OvJ22s8kiRJ0mx5V6IZJHkY8PfAKVX1uabsQOAQ4PKm2UVV9eYkjwGuTfK5dvl277QkSZI0ByYGM3s1cOVUUgBQVd8FvtvfsKpuS7IOeMJ27J8kSZLUCZcSzWwFcPXWNEyyHFgO3NQUHdu3lGjxfHVSkiRJ2lYmBrOQ5OIk303ymVbxsUkmgU8Cb6yqHzXlF1XVWOu1YZp4JySZSDIxPj6+PYYgSZIkTculRC1J1gBHAlTVGHAt8Jyp+qo6OskhwPtbh835WoKqGgemMoLq6mmlkiRJ0mw5Y9BSVaum/sLfFH0C+M0kL2k1e/gO6JokSZI0r5wxmEFVbUjyIuCvk3wA+E/g/wB/uRWHH5vksNb+H1XVFfPRT0mSJGlbmRgAVbVkhrrrgRcOqDsfOH9ryyVJkqRh5VIiSZIkSSYGkiRJkkwMJEmSJGFiIEmSJAlIVe3oPqjHN0KSJEnzLYMqvCvREOnqAWcLly5nt0V7dxJr08b1AEMdr6tYXcdzrMMVb5j71nW8rvvW5b9NMJxj7TreML+vXccbxc/EMMcbpZ8Txzr3eIO4lEiSJEmSiYEkSZIkEwNJkiRJmBhIkiRJYogSgyT3zlD32iTfTXJNkm8nOaVVt1uS25Oc3nfMZUm+n2Rtkm8lGWvV3drEuibJdUn+MsnDmrplSb7bbB+epJK8uHXsF5Ic3tpfmuRnSd7Ud/5bkyzdhm+JJEmStN0MTWIwSJLfAd4K/N9V9RTgmcDdrSbPB24AjknSf/ulV1fVQcDfAO/rqzuiifd0YDlw7oAu/BuwaoYuHgN8A1i5FcORJEmShtLQJwbAO4BTqurfAarqp1X1P1v1K4EPAj8AnjUgxpXAtPd5qqp7gTcBRyX55WmarAXuTvL8AbFXAn8C7J1kny0NRpIkSRpGO0NicCBw1XQVzfKf5wGfBz7J4L/avwC4ZNAJquoe4BZg/wFN1gCnTnP+xwF7VdW/An8HHDvoHJIkSdIw2xkSg5m8CLi0qjYAn6b3V/8FrfqPJ7mF3lKgc7YQa+BT4KrqcoAkh/VVHUsvIQC4kFkuJ0pyQpKJJBPj4+OzOVSSJEnq1NA9+TjJGuBIgKoaA64FDga+PE3zlcBhSW5t9h8NPBf452b/1fRmG94H/HfgZQPO+QhgGb1rFfYY0LWpWYNNfed/bJJXN/u/mmT/qrpxxkE2qmocmMoIqquni0qSJEmzNXQzBlW1qqrGmqQA4DTgfUkeC5BkUZI3JNkdeDbw+KpaVlXLgBPp+6t9VRXw58AzkxzQf74kS+hdnHxJVd01Q7/+CXgU8F+a454ILKmqvVvnP63//JIkSdLOYOgSg35V9Q/A2cC/JLkWuBrYHTga+HJV/bTV/LPAi5M8tC/GBuAM4G2t4kub25L+K70Ll9+4Fd1ZAzyu2V4JXNxX/2l+PjH4TpJ/a15/vRXxJUmSpB1iaJYSVdWSGerOA86bpupjfe1+BOzZ7B7eV3dGa3vZDOe6ld4Fz1TVZcBlrbrP8eC1CJfRp6q+A/z6ls4hSZIkDZuhnzGQJEmSNP9MDCRJkiSZGEiSJEkyMZAkSZIEpHc3Tw0B3whJkiTNt4EP9R2auxIJunrA2cKly9lt0d6dxNq0cT3AUMfrKtZUvC7fBxjusQ7z+9p1vGHuW9fxhrlvXccbpc/EKI2163jD3Leu443Sz4ljnXu8QVxKJEmSJMnEQJIkSZKJgSRJkiRMDCRJkiSxCyQGSTYnmUyyNsnVSQ4d0G51kvVN26nXrya5M8nufW0vSXJskuOT3N53zJOTLEtSSd7SOubspv05TbvrkmxoHffy+f5eSJIkSXO1K9yVaENVjQEk+W3gNOC3BrQ9s6re3y5I8kXgaOBjzf4ewGHAq4BXABdV1Zv7jlkG3AaclOTcqto4VVdVJ7bafGGqb5IkSdIw2+lnDPrsDtw1y2M+CbyytX808MWqum8Lx90OfAk4bpbnkyRJkobOrjBjsDjJJPAwYC/guTO0PTnJa5rtu6rqCOCLwIeTPLqq7qSXJJzdOubYJIe19p/V2n4v8P8m+eg2j0KSJEnagXaFGYMNVTVWVQcALwAuSDLoiW5nNm3HmqSAZhnQ54CXJ1kKPJVesjDlotYxY1W1Yaqiqm4Gvklv2dGsJTkhyUSSifHx8bmEkCRJkjqxK8wYPKCqrmx+ud8zyUnAkU35ltb5fxL4c3qPiP5sVf1sFqd9D/Ap4Ctz6O84MJURVFdP3JUkSZJma1eYMXhAkgOABcCdVbVq6q/8W3HoZcD+wIn0koStVlXXA9cBL55ldyVJkqShsSvMGExdYwC9v/gfV1WbB7RtX2MAcFRV3VpV9yf5FL27EPX/5b//GoM/Av69r80a4Ntz7L8kSZK0w+30iUFVLdjKdquB1TPUvxV4a1/Z+cD5Aw45sNVuLX2zL1V1a7uNJEmSNMx2qaVEkiRJkubGxECSJEmSiYEkSZIkEwNJkiRJQKpqR/dBPb4RkiRJmm+DHgS889+VaFfS1QPOFi5dzm6L9u4k1qaN6wGGOl5XsbqO51iHK94w963reMPct67jdflgyGH/t9PP/9zjDXPfuo43Sj8njnXu8QZxKZEkSZIkEwNJkiRJJgaSJEmSMDGQJEmShImBJEmSJEY4MUhyb9/+65JMNq+NSa5ptk9PcnySs5t2q5NUkv1ax761KTuk2b+1dfxkkrO27+gkSZKk2fF2pY2qOg84D3q/2ANHVNUdzf7xfc2vAV4J/GWzfwxwbV+bB46XJEmSht3Izhhso0uAlwIk2Re4GzAJkCRJ0k7LxGBu7gF+mORAejMHF03T5tLWUqKTpwuS5IQkE0kmxsfH57O/kiRJ0oxcSjR3F9JLCn4b+L+A1/XVb3EpUVWNA1MZQXX5BE9JkiRpNkZ+xiDJmqm/7M/y0C8Avwf8oKrumYeuSZIkSdvNyM8YVNUqYNUcjrsvyduBG7rvlSRJkrR9jXxisC2q6sIZqi9NsrnZ/k5VvXZ79EmSJEmai5FNDKpqyQx1y/r2zwfOb7ZXDzjm8EHHS5IkScNu5K8xkCRJkmRiIEmSJAkTA0mSJElAqmpH90E9vhGSJEmabxlU4YyBJEmSpNG9K9Ew6urJxwuXLme3RXt3EmvTxvUAQx2vq1hdx3OswxVvmPvWdbxh7lvX8br+THT57zAM91iH+X3tOt4w963reKP0c+JY5x5vEGcMJEmSJJkYSJIkSTIxkCRJkoSJgSRJkiRMDCRJkiQxh8Qgyb3TlJ2U5AOt/XOT/Etr/y1Jzmq290ny2SQ3JlmX5INJFjV1hye5O8lkkuuTvL8V4/gkZzfbD0nysSQfTTLtvViT7JHkgiQ3Nee5IMkeTd2yJBua80y9Fm1h3Jck+UZf2eok65vjr0uyMsnrWjE3Jrmm2T59a76/kiRJ0o7Q1YzB14FDW/sHAXskWdDsHwpc0fwS/xngkqraH3gisARY0zr2q1U1BjwVeFGS32yfqInxIWAh8IYa/IS2jwA3V9V+VbUvcAvw4Vb9uqoaa702DhpckkcCBzdjWt5XfWbT35cC5wL/ayom8O/AEc3+nw2KL0mSJO1oXSUGk8ATkyxu/iq/oSl7SlN/KL3k4bnAT6rqPICq2gycDPx+koe3A1bVVIz+G7eeBTwaeG1V3T9dZ5LsR+8X+Xe3it8FHJJk3zmM72XA54ELgVdO16CqbgTuAx61tUGTnJBkIsnE+Pj4HLolSZIkdaOTB5xV1aYk3wZ+A1gMfBO4ETg0ye1AquqHSY4Gruo79p4kPwD2a5cneRSwP3B5q/hVwPeAw6tq0wxdejIw2SQeU+fZnGQSWAF8B9i32Qf4elWdOEO8lfQSi/8EPg28p79BkqcBN1bVbTPE+TlVNQ5MZQTV1YN1JEmSpNnq8snHV9CbGVgMXEkvMXgncHtTt7WenWQtvaTgA1X1H626q4EDgKfTm4HYFuua5T4zSvIrTV++VlWV5GdJDqyq7zZNTk7yOnrLol68jX2SJEmSdog5LyVKsmbqItumaOo6g2fRSwy+R+8v94fyYGJwHb0lPu04uwOPB25qir5aVQfR+8v+65O0f3m/HngFcFGSFTN07zpgLMkD42u2x5q62XgFveVBtyS5FVhGbwZhyplVtQL4XeAjSR42y/iSJEnSDjfnxKCqVrUusoVeMvBMYM+quq25KPh2ehflTv11/0vAw5O8FqC5OPkM4Pyquq8v/i3A6cDb+8qvAP4Q+EKSxw/o203At4FTW8WnAlc3dbOxEnhBVS2rqmX0EptfuM6gqj4HTADHzTK+JEmStMN19hyDqrqLXiJwbav4SuAxwNqmTQFHA8ckuRG4AfgJvSVH0/kQ8Jwky/rO9Xl6a/7/McmjBxz7enoXRK9Lso7eUp/Xz2ZMzXmfADxwm9ImYbk7yTOmOeRdwB+3ZyokSZKkncGsrzGoqiUz1K3o218NrO4r+yED1uJX1WXAZa39DTx4V6Lzm9dU3XnAeTP05S7gNQPqbgUOHHRsX7v+uyJRVU9rNr/ZV34V8KTW/rItnUOSJEkaBv5lW5IkSVKndyXaIZJ8E3hoX/HvVdU1s4zzOuCkvuIt3cZUkiRJ2iVk8IODtZ35RkiSJGm+ZVCFS4kkSZIk7fxLiXYlXT35eOHS5ey26BeumZ6TTRvXAwx1vK5idR3PsQ5XvGHuW9fxhrlvXccbpc/EKI2163jD3Leu443Sz4ljnXu8QZwxkCRJkmRiIEmSJMnEQJIkSRImBpIkSZIYwsQgyeYkk0nWJrk6yaEztD0syb8mub55ndCqW51kfRPruiQrW3XnJ7mlOccNSS5Isk+r/tYkS5vtSnJGq+6UJKv7+jGZ5MK+svOTvHybvhmSJEnSdjJ0iQGwoarGquog4B3AadM1SvJY4BPAm6rqAOAw4I1Jjmw1O7OqxoCXAucmWdiqe1tzjicB3wa+nGTRNKf6KfCyqURhmn78OrAAeHaSX5rVSCVJkqQhMYyJQdvuwF0D6k4Ezq+qqwGq6g7gT4E/629YVTcC9wGPmqauqupM4D+A35nmPJuAceDkAf1YCfwt8E/0EhBJkiRppzOMicHiZmnO9cCHgXcPaLcCuKqvbKIp/zlJngbcWFW3zXDeq4EDBtSdA7w6yR7T1B0LXAh8kl6SIEmSJO10hjExmFpKdADwAuCCJAMf3bwFJye5FvgmsGYLbQeeo6ruAS4A/uvPHZAcAtxRVT8AvgQ8Nckvb23nkpyQZCLJxPj4+NYeJkmSJHVuGBODB1TVlcBSYM8ka5qZhMmm+jrg4L5DDgaube2fWVUrgN8FPpLkYTOc7qnA92ao/wDweqB9HcFK4IAktwLr6C19+t2ZR/WgqhqvqkOq6pATTjhhywdIkiRJ82SoE4MkB9C7sPfOqlrVzCSMNdXnAMcnGWvaPhp4L/BX/XGq6nP0lhkdN805kuS/AnsB/zioL1X1I+Dv6CUHJHkI8ArgKVW1rKqW0bvGwOVEkiRJ2ukMY2KwuDUzcBFwXFVt7m9UVf8f8BrgfzbXI1wBfLSqPj8g7ruAP25+oQd4X5K1wA3AbwBHVNXGLfTtDHozGADPBtZX1b+36i8Hnpxkr2b/3CT/1ryu3EJsSZIkaYfZbUd3oF9VLZhF28vp/VI/Xd3qvv2r6N2aFOD4LcRd1tpe0tr+T+DhrabP7DtuM/DYrTmHJEmSNEyGccZAkiRJ0nZmYiBJkiTJxECSJEkSpKp2dB/U4xshSZKk+Tbw2V3OGEiSJEkavrsSjbKf3XFzJ3EWLl3Obov27iR1yI5+AAAgAElEQVTWpo3rAYY6Xlexuo7nWIcrXtd96/LzCsM91mGON0qfiVEaa9fxhrlvXccbpZ8Txzr3eIM4YyBJkiTJxECSJEmSiYEkSZIkTAwkSZIkMeKJQZLNSSaTrE1ydZJDB7RbnWR903YyyelN+W5Jbp/ab7VfkuTcJOuSXJXksiTP2B5jkiRJkuZi1O9KtKGqxgCS/DZwGvBbA9qeWVXv7yt7PnADcEySd9SDD4X4MHALsH9V3Z/k14And999SZIkqRsjPWPQZ3fgrlkesxL4IPAD4FkASfYFngGcWlX3A1TVLVX19x32VZIkSerUqM8YLE4yCTwM2At47gxtT07ymmb77cBXgOcBbwQeSS9JuAJYAUxW1eZ567UkSZLUsVGfMdhQVWNVdQDwAuCCJIMeE31m03asqr4IvAi4tKo2AJ8GjkqyYDYnT3JCkokkE+Pj49s0EEmSJGlbjPqMwQOq6sokS4E9k5wEHNmUjw04ZCVwWJJbm/1H05txuBY4KMmCLc0aVNU4MJURVFdPUpUkSZJma9RnDB6Q5ABgAXBnVa2amh0Y0HZ34NnA46tqWVUtA04EVlbVOmAC+Iup2Ycky5IcuV0GIkmSJM3BqM8YTF1jABDguK28NuBo4MtV9dNW2WeBv0ryUOANwBnATUk2AHcAb+uw35IkSVKnRjoxqKqtuiagqlb37X8M+Fhf2Y+APZvdnwJ/0EEXJUmSpO3CpUSSJEmSTAwkSZIkmRhIkiRJwsRAkiRJEpCq2tF9UI9vhCRJkubboIf5jvZdiYZNVw84W7h0Obst2ruTWJs2rgcY6nhdxeo6nmMdrnjD3Leu4w1z37qO1+WDIYf9304//3OPN8x96zreKP2cONa5xxvEpUSSJEmSTAwkSZIkmRhIkiRJwsRAkiRJEiOUGCTZnGQyydokVyc5tK/+dU39ZJKNSa5ptk9PcnySs5t2q5NUkv1ax761KTuk2b+1dfxkkrO272glSZKk2RmluxJtqKoxgCS/DZwG/NZUZVWdB5zX1N8KHFFVdzT7x/fFugZ4JfCXzf4xwLV9bR44XpIkSRp2IzNj0Gd34K5tOP4S4KUASfYF7gZMAiRJkrTTGqUZg8VJJoGHAXsBz92GWPcAP0xyIL0E4SLgdX1tLk2yudn+WFWduQ3nkyRJkubVKM0YbKiqsao6AHgBcEGSgU9+2woX0ltOdBRw8TT1RzTnGxuUFCQ5IclEkonx8fFt6IokSZK0bUZpxuABVXVlkqXAnklOAo5sysdmEeYLwPuAiaq6Zy45RlWNA1MZQXX5BE9JkiRpNkYyMUhyALAAuLOqVgGrZhujqu5L8nbghq77J0mSJG1vo5QYTF1jABDguKraPNMBW1JVF85Q3b7G4DtV9dptOZckSZI0n0YmMaiqBbNou6xv/3zg/GZ79YBjDh90vCRJkjTsRuniY0mSJEkDmBhIkiRJMjGQJEmSZGIgSZIkCUhV7eg+qMc3QpIkSfNt4MO3RuauRDuDrh5wtnDpcnZbtHcnsTZtXA8w1PG6itV1PMc6XPGGuW9dxxvmvnUdb5Q+E8M+1i7/D4Ph/N75mdi2WOBY5xqv67EO4lIiSZIkSSYGkiRJkkwMJEmSJGFiIEmSJAkTA0mSJEmMeGKQ5N4B5auTnDKgbmmSnyV5U7OfJHckeVSzv1eSSnJY65jbkzx6PsYgSZIkdWGkE4M5Ogb4BrASoHoPgvgG8Kym/lDg281XkjwJuLOq7tz+XZUkSZK2jonB7K0E/gTYO8k+TdkVNIlA8/VMfj5R+Pp27aEkSZI0SyYGs5DkccBeVfWvwN8BxzZVX+fBxODpwMXA45r9Q+klDtPFOyHJRJKJ8fHx+eu4JEmStAU++Xh2jqWXEABcCHwUOAP4FvDUJL8ELKyqe5PcnGQ/eonBGdMFq6pxYCojqK6eGilJkiTNlokBkGQNcCRAVY3N0HQl8Ngkr272fzXJ/lV1Y5Ibgd8Hrm7qvgG8EHgM8P356bkkSZLUDZcSAVW1qqrGZkoKkjwRWFJVe1fVsqpaBpxGcxEyveVCbwWubPavBE4CvtFcoCxJkiQNLRODwU5N8m9TL3oJwMV9bT7Ng4nB14HlPJgYXA3sw4DrCyRJkqRhMtJLiapqyYDy1cDqrTj+O8CvN9v/D5BW3U+Bh3bRT0mSJGm+OWMgSZIkycRAkiRJkomBJEmSJCDeMGdo+EZIkiRpvmVQhTMGkiRJkkb7rkTDpqsnHy9cupzdFu3dSaxNG9cDDHW8rmJ1Hc+xDle8Ye5b1/GGuW9dxxulz8QojRX8P3Gu8Ubp58Sxzj3eIM4YSJIkSTIxkCRJkmRiIEmSJAkTA0mSJEkMWWKQ5N4Z6lYk+XKS7ydZl+QvkjykqVud5JS+9rcmWdraPypJJTmgVbasKXtLq+zsJMcnOSfJZJLrkmxotieTvDzJ+c3Xi5uym5Lc3WpzaZL3tmI+IcnNSR7Z1fdKkiRJ6tJQJQaDJFkMfA44vaqeBDwFeDpw0izCrAS+1nxtuw04KcmidmFVnVhVY8ALgXVVNda8PtVqc3TT5g3AV6faNMccleTXm6YfBP68qn48i/5KkiRJ281OkRgArwK+XlX/BFBV9wFvBt62NQcnWQIcBrweeGVf9e3Al4DjuupsVW0ATgbOSfJC4BFV9fGu4kuSJEld21kSgxXAVe2CqloHLN7K5TkvBf6xqm4A7kxycF/9e4FTkizopLe9/v0DcBfwMeCPpmuT5IQkE0kmxsfHuzq1JEmSNGu7ygPOagvlK+kt5wG4sNl/INGoqpuTfJPezESXzgEWV9X3p+1c1TgwlRFUVw9zkSRJkmZrKBODJGuAIwGaNfvXAc/pa7McuLOqfpzkTmCvvjCPAH6c5JeB5wJPSVLAAqCS9C9Deg/wKeArHQ7l/uYlSZIkDbWhXEpUVataF/ICfBw4LMnz4IGLkc8C/ltTfznwkiSPaOpfBqytqs3Ay4G/raonVNWyqnoccAvw7L5zXk8vAXnxPA9PkiRJGjpDmRj0ay7mfQmwKskNwB30Lkb+eFP/HeBs4GtJJoE30btTEPSWDV3cF/LT/OLdiQDWAPt0PwJJkiRpuA3VUqKqWjJD3XeBI6D3TALgr5N8oqr+d1N/LnDuNMcdMU3ZWa3dA1vla+lLlqrq1nabpuz4vv3LgMumOc+05ZIkSdKw2SlmDPpV1SVVtXwqKZAkSZK0bXbKxECSJElSt0wMJEmSJJGqQY8A0HbmGyFJkqT5lkEVzhhIkiRJGq67Eo26rp58vHDpcnZbtHcnsTZtXA8w1PG6itV1PMc6XPGGuW9dxxvmvnUdb5Q+E6M01q7jdfn/Kwz3WEfp58Sxzj3eIM4YSJIkSTIxkCRJkmRiIEmSJAkTA0mSJEmMSGKQ5N4B5auT3JfkMdO1TbJPks8muTHJuiQfTLKoqTs8yd1JJpNcn+T9reOOT3J7U3ddkj+Yz/FJkiRJ22okEoMtuAP4k/7CJAE+A1xSVfsDTwSWAGtazb5aVWPAU4EXJfnNVt1FTd3hwHuS/Mo89V+SJEnaZiYG8FHg2CS/3Ff+XOAnVXUeQFVtBk4Gfj/Jw9sNq2oDMAn8wr2kquo2YB3whHnouyRJktQJEwO4l15ycFJf+QrgqnZBVd0D/ADYr12e5FHA/sDl/cGTLAeWAzd112VJkiSpWyYGPWcBxyV5xCyPe3aStcB64ItV9R+tumOTTAKfBN5YVT/qPzjJCUkmkkyMj4/PufOSJEnSthqpJx8nWQMcCdCs/6fZ/nGSTwAntppfB7y87/jdgcfT++v/0+ldY/CiJL8GfCPJ31XVZNP8oqp680z9qapxYCojqK6ezChJkiTN1kjNGFTVqqoaaycFLX8NvJEHk6UvAQ9P8lqAJAuAM4Dzq+q+vri3AKcDb5+3zkuSJEnzaKQSg5lU1R3AxcBDm/0CjgaOSXIjcAPwE+CdA0J8CHhOkmXz3llJkiSpYyOxlKiqlgwoX923/8fAH7f2fwi8eMCxlwGXtfY38OBdic5vXpIkSdJOwRkDSZIkSSYGkiRJkkwMJEmSJAHpXWOrIeAbIUmSpPmWQRXOGEiSJEkajbsS7Sy6esDZwqXL2W3R3ltuuBU2bVwPMNTxuorVdTzHOlzxhrlvXccb5r51HW+UPhOjNNau43Xdty7/v4bhHGvX8Yb5fe063s4w1kGcMZAkSZJkYiBJkiTJxECSJEkSJgaSJEmS2EUSgyT3DihfnaSS7Ncqe2tTdkizv0eSC5LclGRds71HU7esafuW1vFnJzm+2T4/yS1JJpvXFUlWJLkhyeLWMX+fZOU8DV+SJEnaZrtEYrAF1wCvbO0fA1zb2v8IcHNV7VdV+wK3AB9u1d8GnJRk0YD4b6uqseZ1aFVdC3wGWAWQ5ChgYVV9sqPxSJIkSZ0bhcTgEuClAEn2Be4G7mj29wMOBt7dav8u4JCmLcDtwJeA42ZxzncBxyQZA04HTtyWAUiSJEnzbRQSg3uAHyY5kN7MwUWtuicDk1W1eaqg2Z4EVrTavRc4JcmCaeK/r7WU6ONNjPuAU4DLgQur6sZORyRJkiR1bBQSA4AL6SUFRwEXz/bgqroZ+Cbwqmmq20uJXt065vPAj4G/GRQ3yQlJJpJMjI+Pz7ZbkiRJUmd2qScfJ1kDHAlQVWOtqi8A7wMmquqeJFPl1wFjSR5SVfc3MR4CjDV1be8BPgV8ZRZdur95TauqxoGpjKC6epKiJEmSNFu71IxBVa2a+ut9X/l9wNuBNX3lNwHfBk5tFZ8KXN3UtdteTy9ZePF89F2SJEnakXapxGAmVXVhVV09TdXrgSc2typdBzyxKZvOGmCfvrL2NQaTM9y9SJIkSRpau8RSoqpaMqB89YDyw1vbdwGvGdDuVuDA1v5aWslUVR2/hX4tm6lekiRJGhYjM2MgSZIkaTATA0mSJEkmBpIkSZJMDCRJkiQBqaod3Qf1+EZIkiRpvmVQxS5xV6JdRVcPOFu4dDm7Ldq7k1ibNq4HGOp4XcXqOp5jHa54w9y3ruMNc9+6jjdKn4lRGmvX8bruW5f/X8NwjrXreMP8vnYdb2cY6yAuJZIkSZJkYiBJkiTJxECSJEkSJgaSJEmS2EkTgySbk0wmWZvk6iSHDmi3Okkl2a9V9tam7JBmf48kFyS5Kcm6ZnuPpm5Z0/YtrePPTnJ8s31+kluavkwmuSLJiiQ3JFncOubvk6ycp2+HJEmStM12ysQA2FBVY1V1EPAO4LQZ2l4DvLK1fwxwbWv/I8DNVbVfVe0L3AJ8uFV/G3BSkkUD4r+t6ctYVR1aVdcCnwFWASQ5ClhYVZ+czQAlSZKk7WlnTQzadgfumqH+EuClAEn2Be4G7mj29wMOBt7dav8u4JCmLcDtwJeA42bRp3cBxyQZA04HTpzFsZIkSdJ2t7MmBoubpTvX0/vr/rtnaHsP8MMkB9KbObioVfdkYLKqNk8VNNuTwIpWu/cCpyRZME3897WWEn28iXEfcApwOXBhVd04+yFKkiRJ28/OmhhMLSU6AHgBcEGSgU9xAy6klxQcBVw825NV1c3AN4FXTVPdXkr06tYxnwd+DPzNoLhJTkgykWRifHx8tt2SJEmSOrPTP/m4qq5MshTYM8lJwJFN+Vir2ReA9wETVXVPK4e4DhhL8pCquh8gyUOAsaau7T3Ap4CvzKJ79zevQX0fB6YygurqSYqSJEnSbO2sMwYPSHIAsAC4s6pWTf31vt2mWdrzdmBNX/lNwLeBU1vFpwJXN3XtttfTSxZe3P0oJEmSpB1rZ50xWJxkstkOcFz7OoHpVNWFA6peD/z3JOua/SubsumsoZdItL0vSTuxeHpVbZypL5IkSdKw2SkTg6qa7iLg6dqtHlB+eGv7LuA1A9rdChzY2l9La5alqo7fwvmXbU0/JUmSpB1tp19KJEmSJGnbmRhIkiRJMjGQJEmSZGIgSZIkCUhV7eg+qMc3QpIkSfNt4EOBd8q7Eu2qunrA2cKly9lt0d6dxNq0cT3AUMfrKlbX8RzrcMUb5r51HW+Y+9Z1vK4/E13+OwzDPdZhfl+7jjfMfes63ij9nDjWuccbxKVEkiRJkkwMJEmSJJkYSJIkScLEQJIkSRImBpIkSZLYisQgyeYkk0nWJrk6yaHTtDkpyQda++cm+ZfW/luSnNVs75Pks0luTLIuyQeTLGrqDk9yd3O+65O8vxXj+CRnN9sPSfKxJB9NMu0tl5LskeSCJDc157kgyR5N3bIkG5rzTL0WbeH7cEmSb/SVrU6yvjn+uiQrk7yuFXNjkmua7dO39L2WJEmSdpStmTHYUFVjVXUQ8A7gtGnafB1oJwwHAXskWdDsHwpc0fwS/xngkqraH3gisARY0zr2q1U1BjwVeFGS32yfqInxIWAh8IYa/CCGjwA3V9V+VbUvcAvw4Vb9umZcU6+Ng74BSR4JHNyMaXlf9ZlNf18KnAv8r6mYwL8DRzT7fzYoviRJkrSjzXYp0e7AXdOUTwJPTLK4+av8hqbsKU39ofSSh+cCP6mq8wCqajNwMvD7SR7eDlhVUzH6b9x6FvBo4LVVdf90nUyyH71f5N/dKn4XcEiSfbdyrG0vAz4PXAi8croGVXUjcB/wqDnElyRJknaorUkMFk8t7aH3F/d39zeoqk3At4HfAJ4JfBP4BnBokr3pPWH5h8AK4Kq+Y+8BfgDs1y5P8ihgf+DyVvGrgKcBr2zOOciTgckm8Zg6z2Z6icaKpmjf1pKfc7bwPVgJfLJ5rZyuQZKnATdW1W1biNU+5oQkE0kmxsfHt/YwSZIkqXNb8+TjDc2yGJI8C7ggyYHTLOG5gt7MwGLgSuBG4J3A7U3d1np2krX0koIPVNV/tOquBg4Ank5vBmJbrJsa10yS/ErTl69VVSX5WTP+7zZNTk7yOnrLol48mw5U1TgwlRFUV0/clCRJkmZrVkuJqupKYCmwZ5I1U39xb6qnrjN4Fr3E4Hv0/nJ/KA8mBtfRW+LzgCS7A48HbmqKvtpcz7ACeH2S9i/v1wOvAC5KsoLBrgPGkjwwvmZ7rKmbjVfQWx50S5JbgWX8/KzBmVW1Avhd4CNJHjbL+JIkSdION6vEIMkBwALgzqpa1brIFnrJwDOBPavqtmZG4XZ6F+VO/XX/S8DDk7y2ibcAOAM4v6rua5+rqm4BTgfe3ld+BfCHwBeSPH66flbVTfSWNp3aKj4VuLqpm42VwAuqallVLaOX2PzCdQZV9TlgAjhulvElSZKkHW421xhMAhcBx7XX7k+pqrvoJQLXtoqvBB4DrG3aFHA0cEySG4EbgJ/QW3I0nQ8Bz0myrO9cn6d3MfE/Jnn0gGNfT++C6HVJ1tFb6vP6LQ/3Qc15n0Dveompc98C3J3kGdMc8i7gj9szFZIkSdLOYIvXGFTVgi21abVd0be/GljdV/ZDBqzFr6rLgMta+xt48K5E5zevqbrzgPNm6MtdwGsG1N0KHDjo2L52/XdFoqqe1mx+s6/8KuBJrf1lWzqHJEmSNAz8y7YkSZKkrbor0VBL8k3goX3Fv1dV18wyzuuAk/qKv15VJ25L/yRJkqSdwU6fGFTVdGv95xJnxqVJkiRJ0q4sv/g4Au0gvhGSJEmabxlUsdPPGOxKunrA2cKly9lt0S9cMz0nmzauBxjqeF3F6jqeYx2ueMPct67jDXPfuo43Sp+JURor+H/iXOON0s+JY517vEG8+FiSJEmSiYEkSZIkEwNJkiRJmBhIkiRJwsRAkiRJEjs4MUiyOclkkrVJrk5y6AxtVyT5cpLvJ1mX5C+SPKSpW53klL72tyZZ2to/KkklOaBVtqwpe0ur7Owkxyc5p+nbdUk2NNuTSV6e5Pzm68VN2U1J7m61uTTJe1sxn5Dk5iSP7Op7J0mSJHVpR88YbKiqsao6CHgHcNp0jZIsBj4HnF5VTwKeAjydX3xS8UxWAl9rvrbdBpyUZFG7sKpOrKox4IXAuqafY1X1qVabo5s2bwC+OtWmOeaoJL/eNP0g8OdV9eNZ9FeSJEnabnZ0YtC2O3DXgLpXAV+vqn8CqKr7gDcDb9uawEmWAIcBrwde2Vd9O/Al4Lg59HlaVbUBOBk4J8kLgUdU1ce7ii9JkiR1bUcnBoubpTfXAx8G3j2g3QrgqnZBVa1rjt+a5TkvBf6xqm4A7kxycF/9e4FTkiyYXfcHq6p/oJfofAz4o+naJDkhyUSSifHx8a5OLUmSJM3ajn7y8YZm6Q1JngVckOTAqqpZxhnUfqp8Jb3lPAAXNvsPJBpVdXOSb9KbmejSOcDiqvr+tJ2rGgemMoLq6imPkiRJ0mzt6MTgAVV1ZXOx8J5JTgKObMrHgOuA57TbJ1kO3FlVP05yJ7BXX8hHAD9O8svAc4GnJClgAVBJ+pchvQf4FPCVDod1f/OSJEmShtqOXkr0gOZuQQvo/bK/qnUhL8DHgcOSPK9puxg4C/hvTf3lwEuSPKKpfxmwtqo2Ay8H/raqnlBVy6rqccAtwLPb56+q6+klIC+e14FKkiRJQ2hHJwZT1xhMAhcBxzW/zP+c5mLelwCrktwA3EHvYuSPN/XfAc4GvtbEehO9OwVBb9nQxX0hP80v3p0IYA2wz7YPS5IkSdq57NClRFW11Rf7VtV3gSOg90wC4K+TfKKq/ndTfy5w7jTHHTFN2Vmt3QNb5WvpS5aq6tZ2m6bs+L79y4DLpjnPtOWSJEnSsNnRMwZzUlWXVNXyqaRAkiRJ0rbZKRMDSZIkSd0yMZAkSZJEZv/IAM0T3whJkiTNtwyqcMZAkiRJ0vA84EzQ1ZOPFy5dzm6L9u4k1qaN6wGGOl5XsbqO51iHK94w963reMPct67jjdJnYtjH2uX/YTCc3zs/E9sWCxzrXON1PdZBnDGQJEmSZGIgSZIkycRAkiRJEiYGkiRJkhixxCDJ5iSTSdYmuTrJoQParU5yyoC6pUl+luRNzX6S3JHkUc3+XkkqyWGtY25P8uj5GJMkSZLUhZFKDIANVTVWVQcB7wBOm0OMY4BvACsBqvcgiG8Az2rqDwW+3XwlyZOAO6vqzm3suyRJkjRvRi0xaNsduGsOx60E/gTYO8k+TdkVNIlA8/VMfj5R+Po29FOSJEmad6OWGCxulhJdD3wYePdsDk7yOGCvqvpX4O+AY5uqr/NgYvB04GLgcc3+ofQSh+ninZBkIsnE+Pj47EYiSZIkdWjUHnC2oarGAJI8C7ggyYHNcqCtcSy9hADgQuCjwBnAt4CnJvklYGFV3Zvk5iT70UsMzpguWFWNA1MZQXX1cBhJkiRptkYtMXhAVV2ZZCmwZ5KTgCOb8rEZDlsJPDbJq5v9X02yf1XdmORG4PeBq5u6bwAvBB4DfH9eBiFJkiR1ZNSWEj0gyQHAAnoXBq9qLkoemBQkeSKwpKr2rqplVbWM3sXLK5smVwBv5f9v7+5D7arOPI5/fxOTMW0aqsSIzdDG2FBtCglVJiJ1jKXUiiWdgRYpFSyMSqEFCTgqJJTYRhTMkGkLpd42NPiHoNPBIn2zf9S0wabCjb2OGlujROwErLGINiYakzzzx93XHE7vyX3JTrLT+/3AgXPXy7PXui9wn7PW2hu2N19vB24GfjeFFQlJkiTplJhpicHYGYMR4AHg+qo6PKDtuiT/N/ZiNAF4qK/N/3A0MXgMWMLRxOAJ4J8YcL5AkiRJ6pIZtZWoqmZNst16YP0k2v0vcFHz/r+B9NS9DfzjdMYpSZIknWwzbcVAkiRJ0jhMDCRJkiSZGEiSJEmCeMOczvAHIUmSpBMtgypcMZAkSZI0s+5K1HVtPfl49oIlnDFnUSuxDh3cA9DpeG3Fajuec+1WvC6Pre14XR5b2/Fm0t/ETJpr2/G6PLa2482k35O259rm/2HQ7bkO4oqBJEmSJBMDSZIkSSYGkiRJkjAxkCRJkkRHEoMkh5OMJHkyyRNJLhvQbn2SPU3bp5OsHqd8Z5Iv9vRJknVJdiV5LsmjSZb11L+Y5KnmtTPJhiRnNnWrkvykbwxbkny+eT87yd1N7CeSbE9ydZLHm7G8lGRv834kyeL2v3uSJEnS8evKXYkOVNUKgCRXAXcBVwxou6mqNia5CNiWZGFf+VJgR5IfVdU7wFeBy4DlVbU/yaeBh5Msq6q3mr5XVtWrSeYBQ8C9wPWTGPc3gfOAj1XV20nOBa6oqpXNXL4MXFJVX5vat0OSJEk6ubqSGPSaD7w2UaOqejbJIWBBX/muJPuBs4BXgNsY/Wd9f1P/yyS/Bb4EbO7ruy/JV4A/JTn7WNdP8h7gRuD8qnq76f9n4MHJTVOSJEnqjq4kBnOTjABnMvoJ/Ccn6pBkJXAE2NtX/nFgV1W9kmQ+8N6q6r8x7TCwjHFU1RtJdgNLJxjCh4GXquqNicYqSZIkdV0nzhjQbCWqqguBzwD3JRn0uOY1TRKxEbi2qqqn/BngceDO4xzP2LVrQP2g8qldJLkpyXCS4aGhoTZCSpIkSdPSlRWDd1XV9iQLgHOS3Axc05SvaJpsqqqN43QdO2OwGtic5ILm0/83kyzpWzW4GPj1eNdP8j5gMfAcsIjRLUm9zgZeBZ4HPphk/nRXDapqiNEzDQDV1hP3JEmSpKnqyorBu5JcCMwC/lJVa5uVhBUT9RtTVQ8zulVo7PDwPcC3k8xt4n8K+ARw/zjXngd8F/hxVb0G7AI+0Bx0JsmHgOXASHNmYTPwrSRzmvpzknxhOvOWJEmSTqWurBiMnTGA0W0811fV4eOI9w3g/iTfB77D6Kf+TyU5DLwMfK6qDvS0f7TZuvQPwEOM3m2I5k5D1wE/bG5h+g5wQ1W93vRbB2wAdiZ5C3gT+PpxjFuSJEk6JTqRGFTVrEm2Wz+Z8qraAXykp+iO5jVe38UTXPMx4NIBdQeBW5vXePVbgC3Hii9JkiR1Qee2EkmSJEk6+UwMJEmSJJkYSJIkSYIcfd8Y1lYAAAkWSURBVAyATjF/EJIkSTrRBj0rzBUDSZIkSR25K5FGtfWAs9kLlnDGnEWtxDp0cA9Ap+O1FavteM61W/G6PLa243V5bG3Hm0l/EzNprm3H6/LY2o43k35P2p5rm/+HQbfnOogrBpIkSZJMDCRJkiSZGEiSJEnCxECSJEkSHU0MkuwbUL4+yZ4kI0meTrJ6nPKdSb7Y0ydJ1iXZleS5JI8mWdZT/2KSp5rXziQbkpzZ1K1K8pO+MWxJ8vnm/ewkdzexn0iyPcnVSR5vxvJSkr3N+5Eki9v/bkmSJEnH73S8K9GmqtqY5CJgW5KFfeVLgR1JflRV7wBfBS4DllfV/iSfBh5Osqyq3mr6XllVryaZBwwB9wLXT2Is3wTOAz5WVW8nORe4oqpWAiT5MnBJVX2tpblLkiRJJ8TpmBgAUFXPJjkELOgr35VkP3AW8ApwG6P/rO9v6n+Z5LfAl4DNfX33JfkK8KckZx/r+kneA9wInF9Vbzf9/ww82MoEJUmSpJOok1uJJiPJSuAIsLev/OPArqp6Jcl84L1V1X9j2mFgGeOoqjeA3cDSCYbwYeClpr0kSZJ0WjsdE4M1SUaAjcC1VVU95c8AjwN3Huc1xh4VXQPqB5VP7SLJTUmGkwwPDQ21EVKSJEmalk5vJUpyJ3ANQFWtaIo3VdXGcZqPnTFYDWxOckFVvZHkzSRL+lYNLgZ+PeCa7wMWA88BixjdktTrbOBV4Hngg0nmT3fVoKqGGD3TAFBtPXFPkiRJmqpOrxhU1dqqWtGTFEymz8OMbhUaOzx8D/DtJHMBknwK+ARwf3/f5vDxd4EfV9VrwC7gA81BZ5J8CFgOjDRnFjYD30oyp6k/J8kXpjdbSZIk6dTp9IrBcfgGcH+S7wPfYfRT/6eSHAZeBj5XVQd62j+aJIwmSg8xerchmjsNXQf8sLmF6TvADVX1etNvHbAB2JnkLeBN4OsnfnqSJElSuzqZGFTVvAHl6ydTXlU7gI/0FN3RvMbru3iCsTwGXDqg7iBwa/Mar34LsOVY8SVJkqQu6PRWIkmSJEknh4mBJEmSJBMDSZIkSSYGkiRJkoAcfT6YTjF/EJIkSTrRMqiik3clmqnaesDZ7AVLOGPOolZiHTq4B6DT8dqK1XY859qteF0eW9vxujy2tuPNpL+JmTTXtuN1eWxtx5tJvyczaa5tPgR39oIlA+vcSiRJkiTJxECSJEmSiYEkSZIkTAwkSZIk0cHEIMnhJCNJnkzyRJLLBrRbn2RP0/bpJKvHKR97vT/JqiSvN1//IcnGnljzktyb5IUkO5JsTbKybzxjr9ub8q1JhntiXNKUXdXTdl+SPzbv7zux3zlJkiRp+rp4V6IDVbUCIMlVwF3AFQPabqqqjUkuArYlWdhb3tswCcC2qvpskrnA75M8VFWPAT8AdgNLq+pIkvOBj/aPZxwLk1xdVT8fK6iqR4BHmmtuBW6pquEB/SVJkqRO6NyKQZ/5wGsTNaqqZ4FDwILJBK2qA8AIsCjJBcBKYF1VHWnqd1fVTycR6h5g7WSuKUmSJHVZF1cM5iYZAc4EzgM+OVGHZtvPEWBvU7QmyXXN+9eq6sq+9mcBS4HfAP8MjFTV4QnGM+auqnqgeb8d+LckVwJ/nXhqkiRJUjd1ccXgQFWtqKoLgc8A96XZBzSONc0/7RuBa+voY5w3NTFW9CUFlyd5EtgDPFJVL09hPGOvB/rqNwDrJj+9o5LclGQ4yfDQ0NB0QkiSJEmt6OKKwbuqanuSBcA5SW4GrmnKx/b8/81ZggmMnTE4H/hdkgeBZ4DlSWYdY9XgWGP8VZINwKXT6DsEjGUE1eZT7SRJkqSp6OKKwbuSXAjMAv5SVWvHPrU/3rhVtRu4G7itql4AhoE7xlYmkixOcs0UQm4Abj3ecUmSJEmnShdXDHr39Ae4fhqf5PeeMQD413HafA+4Jcli4AbgP4HnkxwAXgX+Y5zxAPyiqm7vDVRVP0uyF0mSJOk01bnEoKpmTbLd+mOUj1f3IrC1p90BYFFP/Y1TGU9Vrer7+uKJ2kiSJEld1emtRJIkSZJODhMDSZIkSSYGkiRJkkwMJEmSJAE5+kwwnWL+ICRJknSiDXpwcPfuSjSTtfWAs9kLlnDGnEUTN5yEQwf3AHQ6Xlux2o7nXLsVr8tjaztel8fWdryZ9Dcxk+badrwuj63teDPp92QmzbXNh+DOXrBkYJ1biSRJkiSZGEiSJEkyMZAkSZKEiYEkSZIkTAwkSZIkcRokBkn2DShfn2RPkpEkTydZPU752Ov9SVYleb35+g9JNvbEmpfk3iQvJNmRZGuSlU3d4b5YtzflW5MM98S4pCm7qqftviR/bN7fd2K/U5IkSdL0ne63K91UVRuTXARsS7Kwt7y3YRKAbVX12SRzgd8neaiqHgN+AOwGllbVkSTnAx9tuh6oqhUDrr8wydVV9fOxgqp6BHikueZW4JaqGh7QX5IkSeqEzq8YTEZVPQscAhZMsv0BYARYlOQCYCWwrqqONPW7q+qnkwh1D7B2eqOWJEmSuuPvIjFotv0cAfY2RWt6tvM8Ok77s4ClwG+AZcBIVR0eEH5u31aia3vqtgMHk1w5zXHflGQ4yfDQ0NB0QkiSJEmtON23Eq1Jch3wV+Daqqpmy9DfbCVqXJ7kSUaTgv+qqpeb9sdyrK1EABuAdcBtUx18VQ0BYxlBtflUO0mSJGkqTpsVgyR3jn1q31O8qapWVNXlVbVtEmG2VdVyRlcJ/j3JCuAZYHmSWdMZV1X9CpgLXDqd/pIkSVIXnDaJQVWtbZKAY316P9lYu4G7gduq6gVgGLgjzfJBksVJrplCyA3Arcc7LkmSJOlUOW0Sgyla03cuYPE4bb4H/EtTdwNwLvB8kqeBLcArTbv+MwZ39weqqp9x9HyDJEmSdNrp/BmDqpo3oHz9McrHq3sR2NrT7gCwqKf+xgHxxt1iVFWr+r6+eKI2kiRJUlf9va4YSJIkSZoCEwNJkiRJJgaSJEmSIFV1qsegUf4gJEmSdKINfIhX5w8fzyATPmlNkiRJOlHcSiRJkiTJxECSJEmSiYEkSZIkTAwkSZIkYWIgSZIkCRMDSZIkScD/A3pPYF+1jZbXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWrBlPucW6S5"
      },
      "source": [
        "def find_logits(s):\n",
        "  tag_ids = model.forward_on_instance(s)['tag_logits']\n",
        "  return tag_ids\n",
        "logit_output = [find_logits(i) for i in validation_dataset]\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnyBtQBdeCSQ"
      },
      "source": [
        "def viterbi(true_tags, tag_names, start_logit, transition_prob, emission_logits):\n",
        "    V = [{}]\n",
        "    for st in tag_names:\n",
        "        V[0][st] = {\"sum_log\": np.log(start_logit[st] + 10**(-80)) + emission_logits[st][true_tags[0]], \"prev\": None}\n",
        "    # Run Viterbi when t > 0\n",
        "    for t in range(1, len(true_tags)):\n",
        "        V.append({})\n",
        "        for st in tag_names:\n",
        "            max_tr_prob = V[t - 1][tag_names[0]][\"sum_log\"] + transition_prob[tag_names[0]][st]\n",
        "            prev_st_selected = tag_names[0]\n",
        "            for prev_st in tag_names[1:]:\n",
        "                tr_prob = V[t - 1][prev_st][\"sum_log\"] + transition_prob[prev_st][st]\n",
        "                if tr_prob > max_tr_prob:\n",
        "                    max_tr_prob = tr_prob\n",
        "                    prev_st_selected = prev_st\n",
        "\n",
        "            max_logsum = max_tr_prob + emission_logits[st][true_tags[t]]\n",
        "            V[t][st] = {\"sum_log\": max_logsum, \"prev\": prev_st_selected}\n",
        "\n",
        "    #for line in dptable(V):\n",
        "        #print(line)\n",
        "\n",
        "    opt = []\n",
        "    max_logsum = 0.0\n",
        "    best_st = None\n",
        "    # Get most probable state and its backtrack\n",
        "    for st, data in V[-1].items():\n",
        "        if data[\"sum_log\"] > max_logsum:\n",
        "            max_logsum = data[\"sum_log\"]\n",
        "            best_st = st\n",
        "    opt.append(best_st)\n",
        "    previous = best_st\n",
        "\n",
        "    # Follow the backtrack till the first observation\n",
        "    for t in range(len(V) - 2, -1, -1):\n",
        "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
        "        previous = V[t + 1][previous][\"prev\"]\n",
        "\n",
        "    #print (\"The steps of tag_names are \" + \" \".join(opt) + \" with sum log of %s\" % max_logsum)\n",
        "    return opt\n",
        "\n",
        "def dptable(V):\n",
        "    # Print a table of steps from dictionary\n",
        "    yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
        "    for state in V[0]:\n",
        "        yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"sum_log\"]) for v in V)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtweYYOM5a5Z"
      },
      "source": [
        "def find_observations(example):\n",
        "  true_tags=[]\n",
        "  for i in word_gold_real[example]:\n",
        "    true_tags.append(str(i[0]))\n",
        "  return true_tags\n",
        "\n",
        "def find_emission(example):\n",
        "  dict_em={}\n",
        "  for i in range(34):\n",
        "    dict_em[tag_names[i]]={}\n",
        "    for j in range(len(logit_output[example])):\n",
        "      dict_em[tag_names[i]][str(word_gold_real[example][j][0])]=logit_output[example][j][i]\n",
        "  return dict_em\n",
        "\n",
        "#making the start probability, it's either 1 for valid start of \"B\" or \"O\" and it's 0 otherwise\n",
        "start_logit={'O': 1, 'B-GPE': 1, 'I-ORG': 0, 'I-DATE': 0, 'B-CARDINAL': 1, 'I-EVENT': 0, 'B-PERSON': 1, 'B-NORP': 1, 'B-DATE': 1, 'B-ORG': 1, 'B-LOC': 1, 'I-LOC': 0, 'I-FAC': 0, 'I-PERSON': 0, 'I-GPE': 0, 'I-CARDINAL': 0, 'B-EVENT': 1, 'I-TIME': 0, 'I-WORK_OF_ART': 0, 'B-ORDINAL': 1, 'B-FAC': 1, 'B-TIME': 1, 'I-LAW': 0, 'I-QUANTITY': 0, 'I-NORP': 0, 'I-MONEY': 0, 'B-MONEY': 1, 'B-WORK_OF_ART': 1, 'B-QUANTITY': 1, 'B-LAW': 1, 'B-PRODUCT': 1, 'I-PRODUCT': 0, 'B-PERCENT': 1, 'I-PERCENT': 0}\n",
        "#making the state array based on tag names\n",
        "tag_names=[]\n",
        "for key in start_logit:\n",
        "  tag_names.append(key)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vBZG_1Wglrk"
      },
      "source": [
        "#running viterbi function on all validation examples and saving the answers in viterbi_answers\n",
        "viterbi_answers=[]\n",
        "for i in range(len(logit_output)):\n",
        "  true_tags=find_observations(i)\n",
        "  emission_logits=find_emission(i)\n",
        "  viterbi_answers.append(viterbi(true_tags,tag_names,start_logit,transition_dict,emission_logits))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h3zwoot-Lhj",
        "outputId": "e1063773-4a52-4a16-9046-6080717d42d4"
      },
      "source": [
        "# Count the number of NER label violations on Viterbi output\n",
        "def violations(tagged):\n",
        "  count=0\n",
        "  for i in range(len(tagged)):\n",
        "    for j in range(len(tagged[i])):\n",
        "      if j==0:\n",
        "        if tagged[i][j]!='O' and tagged[i][j][0]!='B':\n",
        "          count+=1\n",
        "      if j+1<len(tagged[i]) and tagged[i][j]=='O' and tagged[i][j+1][0]=='I':\n",
        "        count+=1\n",
        "      elif j+1<len(tagged[i]) and tagged[i][j][0]=='B' and tagged[i][j+1][0]=='I' and tagged[i][j][1:]!=tagged[i][j+1][1:]:\n",
        "        count+=1\n",
        "      elif j+1<len(tagged[i]) and tagged[i][j][0]=='I' and tagged[i][j+1][0]=='I' and tagged[i][j][1:]!=tagged[i][j+1][1:]:\n",
        "        count+=1\n",
        "  return count\n",
        "\n",
        "print('Number of violations in viterbi output is: ',violations(viterbi_answers))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of violations in viterbi output is:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-tE1YMT7v8N",
        "outputId": "7ee8985c-847a-4069-f0e0-38edd8ec365f"
      },
      "source": [
        "#span-level precision, recall, and F1 for viterbi output and comparing by LSTM output\n",
        "def span_stats(tagged,viterbi_output):\n",
        "  predicted_output=[]\n",
        "  true_output=[]\n",
        "  viterbi_output=[]\n",
        "\n",
        "  #making a continuous array from all predicted outputs in validation set\n",
        "  for sample in tagged:\n",
        "    for token in sample:\n",
        "      predicted_output.append(token[2])\n",
        "  \n",
        "  for sample in viterbi_answers:\n",
        "    for token in sample:\n",
        "      viterbi_output.append(token)\n",
        "\n",
        "  #making a continuous array from all true outputs in validation set\n",
        "  for sample in validation_dataset:\n",
        "      for i in sample['tags'].labels:\n",
        "        true_output.append(i)\n",
        "\n",
        "  print('True output in validation dataset')\n",
        "  print(true_output)\n",
        "  print('')\n",
        "  print('Viterbi output')\n",
        "  print(viterbi_output)\n",
        "  print('')\n",
        "  print('Predicted output by LSTM')\n",
        "  print(predicted_output)\n",
        "\n",
        "  print('\\nLSTM precision: ',round(precision_score([true_output],[predicted_output]),3))\n",
        "  print('LSTM recall: ',round(recall_score([true_output],[predicted_output]),3))\n",
        "  print('LSTM F1: ',round(f1_score([true_output],[predicted_output]),3))\n",
        "\n",
        "\n",
        "  print('\\nViterbi precision: ',round(precision_score([true_output],[viterbi_output]),3))\n",
        "  print('Viterbi recall: ',round(recall_score([true_output],[viterbi_output]),3))\n",
        "  print('Viterbi F1: ',round(f1_score([true_output],[viterbi_output]),3))\n",
        "\n",
        "\n",
        "span_stats(word_gold_real,viterbi_answers)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True output in validation dataset\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'B-TIME', 'I-TIME', 'O', 'B-PERSON', 'I-PERSON', 'B-ORDINAL', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'B-GPE', 'I-GPE', 'I-GPE', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'I-FAC', 'I-FAC', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'I-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'B-NORP', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Viterbi output\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'B-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'B-EVENT', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'B-GPE', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-CARDINAL', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Predicted output by LSTM\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'I-DATE', 'O', 'B-PERSON', 'I-PERSON', 'I-DATE', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-DATE', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'I-EVENT', 'I-EVENT', 'I-EVENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'O', 'O', 'O', 'B-CARDINAL', 'I-EVENT', 'O', 'O', 'O', 'B-GPE', 'O', 'B-PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'I-EVENT', 'I-ORG', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-PERSON', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NORP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PERSON', 'I-DATE', 'O', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DATE', 'O', 'O', 'B-NORP', 'O', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'I-EVENT', 'O', 'O', 'O', 'O']\n",
            "\n",
            "LSTM precision:  0.188\n",
            "LSTM recall:  0.279\n",
            "LSTM F1:  0.224\n",
            "\n",
            "Viterbi precision:  0.405\n",
            "Viterbi recall:  0.395\n",
            "Viterbi F1:  0.4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}