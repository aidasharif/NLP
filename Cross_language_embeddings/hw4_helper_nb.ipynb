{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4_helper_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDsn_Motja8Q"
      },
      "source": [
        "# Tutorial: Cross-Language Word Embeddings\n",
        "\n",
        "We have been previosuly representing individual words as features. This assignment focuses on representation of words in continuous domain w.r.t context. Word2Vec has 2 different methods for learning representation: \n",
        "\n",
        "\n",
        "1.   Bag of words: The objective is to predict middle word based on the surrounding words. Thus a word is represented in terms of the context of its occurence.\n",
        "2.   Skip-gram model: The objective is to predict the surrounding words (context) based on the input word.\n",
        "\n",
        "Genism provides efficient implmenetation of models to generate vector embeddings.\n",
        "\n",
        "We're also going to look at embeddings generated for different languages with Facebooks's library FastText.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKm5cPMQ2xHU"
      },
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfKjYFDklB4c"
      },
      "source": [
        "We'll start by downloading a plain-text version of the Shakespeare plays we used for the first assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw3bvl1yf5FB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "659e7aa0-7b21-48f1-be5f-a0988b00b849"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/shakespeare_plays.txt\n",
        "lines = [s.split() for s in open('shakespeare_plays.txt')]\n",
        "# Sample sentence from Shakespearean play\n",
        "' '.join(lines[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 20:50:08--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/shakespeare_plays.txt\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4746840 (4.5M) [text/plain]\n",
            "Saving to: ‘shakespeare_plays.txt.2’\n",
            "\n",
            "shakespeare_plays.t 100%[===================>]   4.53M  3.11MB/s    in 1.5s    \n",
            "\n",
            "2021-04-08 20:50:10 (3.11 MB/s) - ‘shakespeare_plays.txt.2’ saved [4746840/4746840]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'alls well that ends well by william shakespeare dramatis personae king of france the duke of florence bertram count of rousillon lafeu an old lord parolles a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cZ52pEflKKM"
      },
      "source": [
        "Then, we'll estimate a simple word2vec model on the Shakespeare texts. You can try passing the vector size and skip window as well here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXT5BNPs_zjM"
      },
      "source": [
        "model = Word2Vec(lines)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzt3lG1-lw33"
      },
      "source": [
        "Even with such a small training set size, you can perform some standard analogy tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ruAqhKC3-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be67dd1d-f023-4833-e750-6f984c13e42f"
      },
      "source": [
        "model.wv.most_similar(positive=['king','woman'], negative=['man'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.8456141948699951),\n",
              " ('prince', 0.7579331994056702),\n",
              " ('york', 0.7577690482139587),\n",
              " ('duke', 0.7525829076766968),\n",
              " ('clarence', 0.7353566288948059),\n",
              " ('son', 0.7147427797317505),\n",
              " ('warwick', 0.7145901918411255),\n",
              " ('gloucester', 0.7142559289932251),\n",
              " ('princess', 0.7119390368461609),\n",
              " ('suffolk', 0.7051541805267334)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJL45y5emjA9"
      },
      "source": [
        "For the rest of this assignment, we will focus on finding words with similar embeddings, both within and across languages. For example, what words are similar to the name of the title character of *Othello*?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EZGroU0KPyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e314a0-bdba-491a-9d91-e3e13a12603c"
      },
      "source": [
        "model.wv.most_similar(positive=['othello'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('desdemona', 0.961658775806427),\n",
              " ('cleopatra', 0.9296777844429016),\n",
              " ('troilus', 0.9229164719581604),\n",
              " ('iago', 0.9221736192703247),\n",
              " ('ham', 0.9135774970054626),\n",
              " ('glou', 0.9131096005439758),\n",
              " ('emilia', 0.9123107194900513),\n",
              " ('lear', 0.9111637473106384),\n",
              " ('valentine', 0.9066769480705261),\n",
              " ('pisanio', 0.9043502807617188)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM2BT_7zZle3"
      },
      "source": [
        "This search uses cosine similarity. In the default API, you should see the same similarity between the words `othello` and `desdemona` as in the search results above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e32-u4zYFda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cf2f2b-f7c2-4cab-a02b-35292ed6ba5c"
      },
      "source": [
        "model.wv.similarity('othello', 'desdemona')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9616587"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49DwfAmZ6PU"
      },
      "source": [
        "**TODO**: Your **first task**, therefore, is to implement your own cosine similarity function so that you can reuse it outside of the context of the gensim model object. Cosine similarity will tell you cosine of the angle between two vectors. With a high cosine distance between 2 vectors [ranging between 0 and 1], it can be said that there's a similarity. Since, the dimension of the vectors is 100 for the Word2Vec model and 300 for FastText, you'll use numpy to calculate cosine distance between the two Word2Vec vectors or 2 FastText vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEj2PqpuZ5xs"
      },
      "source": [
        "## TODO: Implement cosim\n",
        "def cosim(v1, v2):\n",
        "  ## return cosine similarity between v1 and v2\n",
        "  return 0\n",
        "\n",
        "## This should give a result similar to model.wv.similarity:\n",
        "cosim(model.wv['othello'], model.wv['desdemona'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbDqBIHbHfB"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We could collect a lot of human judgments about how similar pairs of words, or pairs of Shakespearean characters, are. Then we could compare different word-embedding models by their ability to replicate these human judgments.\n",
        "\n",
        "If we extend our ambition to multiple languages, however, we can use a word translation task to evaluate word embeddings.\n",
        "\n",
        "We will use a subset of [Facebook AI's FastText cross-language embeddings](https://fasttext.cc/docs/en/aligned-vectors.html) for several languages. Your task will be to compare English both to French, and to *one more language* from the following set: Arabic, German, Portuguese, Russian, Spanish, Vietnamese, and Chinese."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC_FXRnfq1BO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17cfc2f3-4bb3-44b3-82fb-7ae281f3883a"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.en.vec\n",
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.fr.vec\n",
        "\n",
        "# TODO: uncomment at least one of these to work with another language\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ar.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.de.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.pt.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.ru.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.es.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.vi.vec\n",
        "# !wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.zh.vec"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-08 21:05:37--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.en.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67681172 (65M)\n",
            "Saving to: ‘30k.en.vec’\n",
            "\n",
            "30k.en.vec          100%[===================>]  64.54M  15.5MB/s    in 5.4s    \n",
            "\n",
            "2021-04-08 21:05:43 (12.0 MB/s) - ‘30k.en.vec’ saved [67681172/67681172]\n",
            "\n",
            "--2021-04-08 21:05:43--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/30k.fr.vec\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67802327 (65M)\n",
            "Saving to: ‘30k.fr.vec’\n",
            "\n",
            "30k.fr.vec          100%[===================>]  64.66M  15.6MB/s    in 5.4s    \n",
            "\n",
            "2021-04-08 21:05:49 (12.0 MB/s) - ‘30k.fr.vec’ saved [67802327/67802327]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmuIvGpNrJPe"
      },
      "source": [
        "We'll start by loading the word vectors from their textual file format to a dictionary mapping words to numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbWORXkP2Vvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250f644a-e965-420c-88d7-2b1f9f7ce1ff"
      },
      "source": [
        "def vecref(s):\n",
        "  (word, srec) = s.split(' ', 1)\n",
        "  return (word, np.fromstring(srec, sep=' '))\n",
        "\n",
        "def ftvectors(fname):\n",
        "  return { k:v for (k, v) in [vecref(s) for s in open(fname)] if len(v) > 1} \n",
        "\n",
        "envec = ftvectors('30k.en.vec')\n",
        "frvec = ftvectors('30k.fr.vec')\n",
        "# Sample English word 'on' and its vector embedding\n",
        "print(list(envec.items())[15])\n",
        "# Sample French word 'du' and its vector embedding\n",
        "print(list(frvec.items())[15])\n",
        "\n",
        "# TODO: load vectors for one more language, such as zhvec (Chinese)\n",
        "# arvec = ftvectors('30k.ar.vec')\n",
        "# devec = ftvectors('30k.de.vec')\n",
        "# ptvec = ftvectors('30k.pt.vec')\n",
        "# ruvec = ftvectors('30k.ru.vec')\n",
        "# esvec = ftvectors('30k.es.vec')\n",
        "# vivec = ftvectors('30k.vi.vec')\n",
        "# zhvec = ftvectors('30k.zh.vec')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('on', array([-0.0108,  0.0298, -0.0147,  0.0224, -0.0068, -0.0253,  0.0375,\n",
            "       -0.0396, -0.019 , -0.0771, -0.0037,  0.004 ,  0.0159, -0.0456,\n",
            "        0.0498, -0.0383, -0.0111, -0.0132,  0.0137,  0.0515, -0.0573,\n",
            "       -0.0037, -0.11  ,  0.0006, -0.0783, -0.0704,  0.0562, -0.0938,\n",
            "        0.0244,  0.002 ,  0.0066,  0.0976, -0.0229,  0.0182, -0.0141,\n",
            "       -0.0808, -0.0776,  0.0101,  0.0992, -0.069 ,  0.0298,  0.0239,\n",
            "       -0.0178,  0.0447, -0.0796, -0.0088,  0.0033,  0.0143,  0.0109,\n",
            "       -0.1093,  0.0214, -0.067 ,  0.0189,  0.0015,  0.0077, -0.0294,\n",
            "       -0.0046,  0.0186, -0.0374,  0.0908, -0.003 , -0.0764,  0.0695,\n",
            "       -0.0898, -0.0616, -0.0906, -0.0102,  0.0035,  0.0037,  0.0754,\n",
            "        0.0338,  0.0165, -0.0389, -0.0462,  0.0064,  0.0059,  0.1263,\n",
            "        0.085 ,  0.0313,  0.0005,  0.0274,  0.0103,  0.0673,  0.035 ,\n",
            "        0.0257, -0.072 , -0.0545, -0.0663,  0.0887, -0.0877, -0.0359,\n",
            "        0.0791,  0.102 , -0.0959,  0.0649,  0.0506,  0.0088, -0.0386,\n",
            "       -0.0453, -0.01  , -0.0513, -0.0348, -0.0464, -0.0113, -0.0455,\n",
            "       -0.0132, -0.022 ,  0.106 , -0.0134,  0.088 ,  0.0371, -0.0491,\n",
            "        0.0335, -0.0193, -0.0201,  0.0382,  0.0305,  0.0095, -0.0435,\n",
            "        0.0074,  0.1297,  0.0487, -0.04  ,  0.0988, -0.0079, -0.012 ,\n",
            "        0.0938,  0.1177,  0.0594,  0.0954, -0.0411, -0.0387,  0.0099,\n",
            "        0.0739, -0.0934, -0.064 , -0.0252,  0.0011,  0.011 ,  0.0133,\n",
            "       -0.0408,  0.094 , -0.0719, -0.0312, -0.0154,  0.047 ,  0.0937,\n",
            "        0.0024, -0.103 ,  0.0974, -0.0906, -0.0925, -0.1428,  0.0062,\n",
            "        0.0539, -0.0641, -0.0222, -0.0283,  0.0484, -0.0428, -0.0522,\n",
            "        0.0394, -0.0902,  0.0465,  0.0856,  0.0624, -0.0167, -0.0298,\n",
            "        0.0452, -0.0507, -0.0979,  0.0706, -0.0954, -0.0641, -0.0676,\n",
            "       -0.0159, -0.0186,  0.0238,  0.051 , -0.0171, -0.0645,  0.0088,\n",
            "        0.0322,  0.0085,  0.0441,  0.1255,  0.0522, -0.061 ,  0.0466,\n",
            "       -0.013 ,  0.0049, -0.1054,  0.0076,  0.0098, -0.0129,  0.0109,\n",
            "        0.0133,  0.0375, -0.1225, -0.0822, -0.0393,  0.0679, -0.0238,\n",
            "       -0.0169,  0.0137, -0.05  ,  0.0234, -0.0226,  0.1125, -0.0324,\n",
            "        0.1588,  0.0316, -0.0307,  0.0114,  0.001 ,  0.0786,  0.002 ,\n",
            "       -0.1624, -0.0429, -0.0455,  0.0846, -0.0321, -0.0111,  0.0505,\n",
            "       -0.0489,  0.0092, -0.0841,  0.015 ,  0.0095,  0.0535,  0.1269,\n",
            "        0.0575, -0.0308,  0.0362,  0.043 ,  0.0302, -0.0031, -0.0364,\n",
            "        0.0093,  0.0007,  0.0427,  0.0326,  0.1117,  0.0334,  0.096 ,\n",
            "        0.1059,  0.0114, -0.0221,  0.093 ,  0.0917,  0.0759, -0.0132,\n",
            "        0.0456,  0.0144, -0.0546,  0.0463, -0.1788, -0.0011, -0.0226,\n",
            "        0.0323,  0.1232,  0.0644, -0.0407,  0.0506,  0.0264, -0.0549,\n",
            "       -0.0165,  0.0521, -0.0922, -0.0467, -0.0371, -0.0191,  0.0153,\n",
            "        0.0864, -0.0655, -0.0531,  0.0046,  0.0951,  0.0138, -0.015 ,\n",
            "        0.0808,  0.0298, -0.0773,  0.0362,  0.0399, -0.0347,  0.0272,\n",
            "       -0.028 ,  0.0555, -0.0689,  0.0538, -0.016 , -0.0419, -0.0644,\n",
            "        0.1302, -0.0597,  0.0495,  0.0131, -0.0173,  0.0005]))\n",
            "('du', array([ 0.0279, -0.0752,  0.0388, -0.0507,  0.1026,  0.0252, -0.1245,\n",
            "       -0.083 ,  0.0622, -0.0439,  0.1206, -0.0784,  0.0581, -0.0817,\n",
            "       -0.0744,  0.0851, -0.1073, -0.0395, -0.0151,  0.0453,  0.0085,\n",
            "       -0.0687, -0.0255, -0.027 , -0.0114,  0.0144, -0.0222, -0.0484,\n",
            "        0.0237, -0.1098, -0.0635, -0.0056,  0.049 ,  0.0797,  0.0483,\n",
            "       -0.0391, -0.0808, -0.0298, -0.0136,  0.0342,  0.0457, -0.0159,\n",
            "        0.0057,  0.0442, -0.073 ,  0.0125, -0.0622, -0.0167, -0.0154,\n",
            "        0.0145,  0.0704,  0.0874, -0.0334, -0.033 , -0.0544, -0.0243,\n",
            "       -0.0505, -0.0071,  0.0505, -0.0408, -0.0759, -0.1019, -0.0085,\n",
            "       -0.034 , -0.0375,  0.0253, -0.0158, -0.1454, -0.1004, -0.03  ,\n",
            "       -0.1598, -0.1204, -0.0366, -0.1343, -0.0311, -0.0549, -0.0998,\n",
            "        0.0478,  0.0445,  0.0739,  0.0576,  0.0384, -0.0425,  0.0297,\n",
            "       -0.0174,  0.0337, -0.0682, -0.0389, -0.0034, -0.0264, -0.0327,\n",
            "        0.1079,  0.0562, -0.033 , -0.0002, -0.0121,  0.0613,  0.0003,\n",
            "        0.0663,  0.1061, -0.0699, -0.0508, -0.0198, -0.0014, -0.1091,\n",
            "       -0.0213, -0.0125,  0.0662,  0.0544,  0.0165,  0.0695, -0.0335,\n",
            "        0.0139, -0.0645,  0.0986,  0.0207, -0.0693, -0.0494,  0.1   ,\n",
            "       -0.0018,  0.0868,  0.0772,  0.0762, -0.0034,  0.0267,  0.0031,\n",
            "        0.1033,  0.0126,  0.0654,  0.007 , -0.0161,  0.136 ,  0.0614,\n",
            "       -0.0407, -0.0452,  0.0266,  0.0474,  0.0155,  0.0578, -0.0034,\n",
            "        0.0572,  0.0156, -0.0068,  0.0136, -0.0085,  0.1578,  0.0294,\n",
            "        0.0251,  0.0299,  0.0326, -0.0546, -0.01  , -0.0104, -0.0059,\n",
            "        0.0591, -0.0978, -0.0175, -0.0214,  0.0008, -0.017 , -0.0993,\n",
            "       -0.0119,  0.0229, -0.0225,  0.0808, -0.049 ,  0.1249,  0.0345,\n",
            "        0.0042, -0.0818, -0.0049,  0.0344, -0.0002, -0.0311, -0.0847,\n",
            "        0.0539,  0.0803,  0.0647, -0.0204, -0.0414, -0.024 ,  0.0665,\n",
            "       -0.0206,  0.0701, -0.1214,  0.04  ,  0.0658,  0.0107, -0.0541,\n",
            "        0.0139, -0.0137, -0.01  ,  0.0517,  0.049 , -0.0776,  0.006 ,\n",
            "       -0.0061,  0.0222, -0.0522, -0.0175, -0.0034, -0.0615, -0.1728,\n",
            "        0.0629,  0.0292,  0.0389,  0.0127,  0.0859, -0.0412,  0.0065,\n",
            "       -0.0817,  0.091 , -0.1089,  0.0663, -0.0904, -0.0482, -0.0245,\n",
            "       -0.0644,  0.0815, -0.0219,  0.0985,  0.0114, -0.037 ,  0.0332,\n",
            "       -0.0016, -0.0481, -0.0786,  0.1   , -0.024 ,  0.0195, -0.0133,\n",
            "        0.0834,  0.0118,  0.0105, -0.0916,  0.0332, -0.0303, -0.01  ,\n",
            "       -0.0025,  0.0207, -0.0753,  0.0298, -0.033 ,  0.0281,  0.0919,\n",
            "       -0.003 , -0.0095,  0.0641,  0.1596,  0.1179, -0.0477,  0.0573,\n",
            "        0.043 ,  0.0322, -0.0848, -0.0046, -0.0402, -0.0594,  0.0899,\n",
            "        0.0292,  0.0264, -0.0579,  0.0172, -0.0621,  0.0258,  0.028 ,\n",
            "       -0.0073, -0.0166,  0.034 , -0.0158, -0.0594, -0.0073, -0.0139,\n",
            "        0.0582,  0.0531,  0.0129, -0.0271, -0.0055,  0.0456,  0.0232,\n",
            "        0.0009, -0.0817, -0.0582, -0.0301,  0.0423, -0.0907,  0.0122,\n",
            "        0.027 , -0.021 ,  0.0702, -0.001 , -0.0232,  0.0175,  0.0693,\n",
            "       -0.0738, -0.0252, -0.0608,  0.0037, -0.0731,  0.0102]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88E1JdueZHc"
      },
      "source": [
        "**TODO**: Your next task is to write a simple function that takes a vector and a dictionary of vectors and finds the most similar item in the dictionary. For this assignment, a linear scan through the dictionary using your `cosim` function from above is acceptible. For computational efficiency, you can calculate cosine for all vectors w.r.t a given vector by casting the first vector as a 1D numpy array and the 2nd vector as a 2D numpy array. You can then call `np.max` and `np.argmax` to get the most similar word and corresponding similarity. You can do this by adding some parameters to your cosim function to check if the computation is supposed to be for 2 vectors or 2 arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdirYOjoSWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e0da5b0-fbad-4195-9e4c-03b02ff775ec"
      },
      "source": [
        "## TODO: implement this search function\n",
        "def mostSimilar(vec, vecDict):\n",
        "  ## Use your cosim function from above\n",
        "  mostSimilar = ''\n",
        "  similarity = 0\n",
        "  return (mostSimilar, similarity)\n",
        "\n",
        "## some example searches\n",
        "[mostSimilar(envec[e], frvec) for e in ['computer', 'germany', 'matrix', 'physics', 'yeast']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 0), ('', 0), ('', 0), ('', 0), ('', 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIKUD5qxpUMB"
      },
      "source": [
        "Some matches make more sense than others. Note that `computer` most closely matches `informatique`, the French term for *computer science*. If you looked further down the list, you would see `ordinateur`, the term for *computer*. This is one weakness of a focus only on embeddings for word *types* independent of context.\n",
        "\n",
        "To evalute cross-language embeddings more broadly, we'll look at a dataset of links between Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az10sIFwsEUP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5add3925-c7a7-4a34-9980-a70d74520777"
      },
      "source": [
        "!wget http://www.ccs.neu.edu/home/dasmith/courses/cs6120/links.tab\n",
        "links = [s.split() for s in open('links.tab')]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-14 03:22:37--  http://www.ccs.neu.edu/home/dasmith/courses/cs6120/links.tab\n",
            "Resolving www.ccs.neu.edu (www.ccs.neu.edu)... 52.70.229.197\n",
            "Connecting to www.ccs.neu.edu (www.ccs.neu.edu)|52.70.229.197|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1408915 (1.3M)\n",
            "Saving to: ‘links.tab’\n",
            "\n",
            "links.tab           100%[===================>]   1.34M  3.66MB/s    in 0.4s    \n",
            "\n",
            "2020-04-14 03:22:37 (3.66 MB/s) - ‘links.tab’ saved [1408915/1408915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqHq0hFCv8NY"
      },
      "source": [
        "This `links` variable consists of triples of `(English term, language, term in that language)`. For example, here is the link between English `academy` and French `académie`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ7eusdxtdsq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5f0bfbe-6e37-4480-9618-bd539ca27ffe"
      },
      "source": [
        "links[302]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['academy', 'fr', 'académie']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYEdOQbmwql3"
      },
      "source": [
        "**TODO**: Evaluate the English and French embeddings by computing the proportion of English Wikipedia articles whose corresponding French article is also the closest word in embedding space. Skip English articles not covered by the word embedding dictionary. Since many articles, e.g., about named entities have the same title in English and French, compute the baseline accuracy achieved by simply echoing the English title as if it were French. Remember to iterate only over English Wikipedia articles, not the entire embedding dictionary.\n",
        "\n",
        "First select tuples from links for the language you're evaluating. For French, `words_lang = all words that satisfy condition: links[i][1] == 'fr'`.\n",
        "\n",
        "**Baseline Accuracy** = `sum(words_lang[i][Index for English term] == words_lang[i][Index for term in that language])/len(words_lang)`\n",
        "\n",
        "For word i in words, get the most similar word based on cosine similarity for FastText embeddings.\n",
        "`similar_word, similarity = mostsimlar(envec(words_lang[i][Index for English term], frvec)`\n",
        "\n",
        "**Accuracy** = `sum(similar_word == words_lang[i][Index for term in that language]))/len(words_lang)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJrTJ3ja91Z4"
      },
      "source": [
        "## TODO: Compute English-French Wikipedia retrieval accuracy.\n",
        "accuracy = 0\n",
        "baselineAccuracy = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hqd1buq-OEo"
      },
      "source": [
        "**TODO**: Compute accuracy and baseline (identity function) acccuracy for Englsih and another language besides French. Although the baseline will be lower for languages not written in the Roman alphabet (i.e., Arabic or Chinese), there are still many articles in those languages with headwords written in Roman characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjnKtHya-jmj"
      },
      "source": [
        "## TODO: Compute English-X Wikipedia retrieval accuracy.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6z01sufFPJh"
      },
      "source": [
        "Further evaluation, if you are interested, could involve looking at the $k$ nearest neighbors of each English term to compute \"recall at 10\" or \"mean reciprocal rank at 10\"."
      ]
    }
  ]
}